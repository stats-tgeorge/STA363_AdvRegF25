{
  "hash": "bd452012e0ef4505e395272343809a92",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Distribution Theory\"\nsubtitle: \"BMLR Chapter 3\"\nformat: \n  revealjs:\n    output-file: \"03_distribution_ch3.html\"\n    slide-number: true\n  html:\n    output-file: \"03_distribution_ch3_o.html\"\nlogo: \"../img/favicon.png\"\n---\n\n\n\n \n## Learning Objectives {.smaller}\n\n\n\\newcommand{\\lik}{\\mathrm{Lik}}\n\\newcommand{\\Lik}{\\mathrm{Lik}} \n\\newcommand{\\bbias}{p_{B|\\textrm{BBias}}}\n\\newcommand{\\neutral}{p_{B|N}}\n\\newcommand{\\gbias}{p_{B|\\textrm{GBias}}}\n\\newcommand{\\bstop}{p_{S|B1}}\n\\newcommand{\\nstop}{p_{S|N}}\n\\newcommand{\\E}{\\operatorname{E}}\n\\newcommand{\\SD}{\\operatorname{SD}}\n\n\n- Write definitions of non-normal random variables in the context of an application.\n- Identify possible values for each random variable.\n- Identify how changing values for a parameter affects the characteristics of the distribution.\n- Recognize a form of the probability density function for each distribution.\n- Identify the mean and variance for each distribution.\n- Match the response for a study to a plausible random variable and provide reasons for ruling out other random variables.\n- Match a histogram of sample data to plausible distributions.\n- Create a mixture of distributions and evaluate the shape, mean, and variance.\n\n\n## Libraries Needed (none new?)\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Packages required for Chapter 3\nlibrary(gridExtra)  \nlibrary(knitr) \nlibrary(kableExtra)\nlibrary(tidyverse)\n```\n:::\n\n\n\n## Introduction {.smaller}\n\n- What if it  is not plausible that a response is normally distributed? \n- You may want to construct a model to predict whether a prospective student will enroll at a school or  model the lifetimes of patients following a particular surgery.  \n- In the first case you have a binary response (enrolls (1) or does not enroll (0)), and in the second case you are likely to  have very skewed data with many similar values and a few hardy souls with extremely long survival. \n- These responses are not expected to be normally distributed; other distributions will be needed to describe and  model binary or lifetime data. \n- Non-normal responses are encountered in a large number of situations. Luckily, there are quite a few possibilities for models. \n\n# Discrete Random Variables\n\n## Discrete Random Variables {.smaller}\n\n- A discrete random variable has a countable number of possible values\n  - Ex: we may want to measure the number of people in a household\n  - Ex: model the number of crimes committed on a college campus. \n\n- With discrete random variables, the associated probabilities can be calculated for each possible value using a __probability mass function__ (pmf). \\index{probability mass function (pmf)} \n- A pmf is a function that calculates $P(Y=y)$, given each variable's parameters.\n\n\n## Binary Random Variable {.smaller}\n\n- Consider the event of flipping a (possibly unfair) coin. \n- If the coin lands heads, let's consider this a success and record $Y = 1$.\n- A series of these events is a  __Bernoulli process__, \\index{Bernoulli process} independent trials that take on one of two values (e.g., 0 or 1). \n- These values are often referred to as a failure and a success\n  - the probability of success is identical for each trial.\n  \n## Binary Random Variable{.smaller}\n\n- Suppose we only flip the coin once, so we only have one parameter, the probability of flipping heads, $p$. \n- If we know this value, we can express $P(Y=1) = p$ and $P(Y=0) = 1-p$. \n- In general, if we have a Bernoulli process with only one trial, we have a __binary distribution__ (also called a __Bernoulli distribution__) \\index{Bernoulli distribution} where\n\n. . .\n\n\\begin{equation*} \nP(Y = y) = p^y(1-p)^{1-y} \\quad \\textrm{for} \\quad y = 0, 1.\n\\end{equation*}\n\n- If $Y \\sim \\textrm{Binary}(p)$, then $Y$ has mean $\\E(Y) = p$ and standard deviation $\\SD(Y) = \\sqrt{p(1-p)}$.\n\n## Example 1 {.smaller}\n- Your playlist of 200 songs has 5 which you cannot stand. What is the probability that when you hit shuffle, a song you tolerate comes on?\n- We want to understand the example and be able to translate that into notation/model it with a distribution\n- Assuming all songs have equal odds of playing, we can calculate $p = \\frac{200-5}{200} = 0.975$ \n  - so there is a 97.5\\% chance of a song you tolerate playing, since $P(Y=1)=.975^1*(1-.975)^0$.\n\n\n## Binomial Random Variable (1/2) {.smaller}\n\n- Does anybody know how this relates to a Bernoulli random variable? \n- We can extend our knowledge of binary random variables. \n- Suppose we flipped an *unfair* coin $n$ times and recorded $Y$, the number of heads after $n$ flips. \n- If we consider a case where $p = 0.25$ and $n = 4$, then here $P(Y=0)$ represents the probability of no successes in 4 trials\n  - 4 consecutive failures. \n  - The probability of 4 consecutive failures is $P(Y = 0) = P(TTTT) = (1-p)^4 = 0.75^4$. \n  \n## Binomial Random Variable (2/2) {.smaller}\n\n- Next we consider $P(Y = 1)$, and are interested in the probability of exactly 1 success _anywhere_ among the 4 trials. \n- How many different ways can we get exactly 1 success in the four trials? \n- To find the probability of this what else do we need to count? \n- What is the probability? \n\n- There are $\\binom{4}{1} = 4$ ways to have exactly 1 success in 4 trials, \n$P(Y = 1) = \\binom{4}{1}p^1(1-p)^{4-1} = (4)(0.25)(0.75)^3$. \n\n## Binomial Distribution {.smaller}\n\n- In general, if we carry out a sequence of $n$ Bernoulli trials (with probability of success $p$) and record $Y$, the total number of successes, then $Y$ follows a __binomial distribution__, \\index{binomial distribution} where\n\n. . .\n\n\\begin{equation}\nP(Y=y) = \\binom{n}{y} p^y (1-p)^{n-y} \\quad \\textrm{for} \\quad y = 0, 1, \\ldots, n.\n\\end{equation}\n\n. . .\n\n- If $Y \\sim \\textrm{Binomial}(n,p)$, then $\\E(Y) = np$ and $\\SD(Y) = \\sqrt{np(1-p)}$.\n- $E(y)$ is the expected value of $Y$. If we repeated the experiment many times we would expect on average $n*p$ successes. \n\n\n## Binomial Distribution Graphs {.smaller}\n- Typical shapes of a binomial distribution\n- I will show you these in R\n\n\n## Binomial Distribution Graphs {.smaller}\n- On the left side $n$ remains constant. \n- We see that as $p$ increases, the center of the distribution ($\\E(Y) = np$) shifts right. \n- On the right, $p$ is held constant. \n- As $n$ increases, the distribution becomes less skewed.\n\n\n\n## Binomial Distribution vs Bernoulli {.smaller}\n\n- Note that if $n=1$,\n\n. . .\n\n\\begin{align*}\n P(Y=y) &= \\binom{1}{y} p^y(1-p)^{1-y} \\\\\n        &= p^y(1-p)^{1-y}\\quad \\textrm{for}\\quad y = 0, 1,\n\\end{align*}\na Bernoulli distribution! \n- In fact, Bernoulli random variables are a special case of binomial random variables where $n=1$.\n\n## Graphing/calling in R \n- In R we can use the function `dbinom(y, n, p)`, which outputs the probability of $y$ successes given $n$ trials with probability $p$, i.e., $P(Y=y)$ for $Y \\sim \\textrm{Binomial}(n,p)$.\n- Lets try it\n\n## Example 2 (1/2) {.smaller}\n- While taking a multiple choice test, a student encountered 10 problems where she ended up completely guessing, randomly selecting one of the four options. \n- What is the chance that she got exactly 2 of the 10 correct?\n- What assumption do we need to make about the questions? \n- How would we do this without using a distribution? \n\n## Example 2 (2/2) {.smaller}\n\n- Knowing that the student randomly selected her answers, we assume she has a 25\\% chance of a correct response. \n- n *factorial* is denoted $n!$ and $n!=n\\cdot(n-1)\\cdot...\\cdot 3\\cdot 2\\cdot 1$ \n- Here we used a *combination* ${n \\choose p}=\\frac{n!}{p!(n-p)!}$ read *n choose p*\n- Thus, $P(Y=2) = {10 \\choose 2}(.25)^2(.75)^8 = 0.282$. \n\n. . .\n\n- We can use R to verify this:\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndbinom(2, size = 10, prob = .25)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.2815676\n```\n\n\n:::\n:::\n\n\nTherefore, there is a 28\\% chance of exactly 2 correct answers out of 10.\n\n## Negative Binomial Random Variable {.smaller}\n\n- What if we were to carry out multiple independent and identical Bernoulli trails until the $r$^th^ success occurs?\n- If we model $Y$, the number of failures before the $r$^th^ success, then $Y$ follows a __negative binomial distribution__ \\index{negative binomial distribution} where\n\n. . .\n\n\\begin{equation}\nP(Y=y) = \\binom{y + r - 1}{r-1} (1-p)^{y}(p)^r \\quad \\textrm{for}\\quad y = 0, 1, \\ldots, \\infty.\n\\end{equation}\n\n## Negative Binomial RV  {.smaller}\n\n- If $Y \\sim \\textrm{Negative Binomial}(r, p)$ then $\\E(Y) = \\frac{r(1-p)}{p}$ and $\\SD(Y) = \\sqrt{\\frac{r(1-p)}{p^2}}$. \n\n## Negative Binomial RV  {.smaller}\n\n:::{.panel-tabset}\n\n### Plot\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Negative Binomial](03_distribution_ch3_files/figure-revealjs/multNBinom-1.png){fig-align='center' width=60%}\n:::\n:::\n\n\n- Notice how centers shift right as $r$ increases, and left as $p$ increases.\n\n### Code \n\n\n::: {.cell}\n\n```{.r .cell-code}\n#negativeBinomialPlots\nplotNBinom <- function(p, r){\n  ynb <- 0:10000\n  nbd <- tibble(x = rnbinom(ynb, r, p))\n  #breaks <- pretty(range(nbd$x), n = nclass.FD(nbd$x), min.n = 1)  # pretty binning\n  #bwidth <- breaks[2] - breaks[1]\n  ggplot(nbd, aes(x = x)) +\n    geom_histogram(aes(y=..count../sum(..count..)), binwidth = .25) +\n    labs(title = paste(\"p = \", p, \", r = \", r),\n         x = \"number of failures\",\n         y = \"probability\") +\n    xlim(-1,30)\n}\n\nNBin1 <- plotNBinom(0.35, 3)\nNBin2 <- plotNBinom(0.35, 5)\nNBin3 <- plotNBinom(0.70, 5)\ngrid.arrange(NBin1, NBin2, NBin3, ncol = 1)\n```\n:::\n\n\n:::\n\n\n\n## Negative Binomial RV {.smaller}\n\n- Note that if we set $r=1$, then\n\n. . .\n\n\\begin{align*}\n P(Y=y) &= \\binom{y}{0} (1-p)^yp \\\\\n        &= (1-p)^yp \\quad \\textrm{for} \\quad y = 0, 1, \\ldots, \\infty,\n\\end{align*}\nwhich is the probability mass function of a geometric random variable! \n\n- Thus, a geometric random variable is, in fact, a special case of a negative binomial random variable.\n\n- While negative binomial random variables typically are expressed as above using binomial coefficients (expressions such as $\\binom{x}{y}$), we can generalize our definition to allow non-integer values of $r$. \n\n- R function `dnbinom(y, r, p)` for the probability of $y$ failures before the $r$^th^ success given probability $p$.\n\n## Poisson Random Variable {.smaller}\n\n- Sometimes, random variables are based on a __Poisson process__. \\index{Poisson process} \n- In a Poisson process, we are counting the number of events per unit of time or space and the number of events depends only on the length or size of the interval. \n* We can then model $Y$, the number of events in one of these sections with the __Poisson distribution__, \\index{Poisson distribution} where\n\n. . .\n\n\\begin{equation}\nP(Y=y) = \\frac{e^{-\\lambda}\\lambda^y}{y!} \\quad \\textrm{for} \\quad y = 0, 1, \\ldots, \\infty,\n\\end{equation}\nwhere $\\lambda$ is the mean or expected count in the unit of time or space of interest.\n- This probability mass function has $\\E(Y) = \\lambda$ and $\\SD(Y) = \\sqrt{\\lambda}$. \n\n## Poisson Distribution Graphs {.smaller}\n\n:::{.panel-tabset}\n\n### Plot \n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](03_distribution_ch3_files/figure-revealjs/multPois-1.png){fig-align='center' width=60%}\n:::\n:::\n\n\n- Notice how distributions become more symmetric as $\\lambda$ increases.\n- If we wish to use R, `dpois(y, lambda)` outputs the probability of $y$ events given $\\lambda$.\n\n\n\n### Code \n\n\n::: {.cell}\n\n```{.r .cell-code}\n#poissonPlots\nplotPois <- function(lam){\nyp = 0:10000 # possible values\npd <- data.frame(x=rpois(yp, lam))  # generate random deviates\nbreaks <- pretty(range(pd$x), n = nclass.FD(pd$x), min.n = 1)  # pretty binning\n#bwidth <- breaks[2] - breaks[1]\nggplot(pd, aes(x = x)) + geom_histogram(aes(y=..count../sum(..count..)), binwidth = .25) +\n  xlab(\"number of events\") + ylab(\"probability\") + \n  labs(title=paste(\"Poisson lambda = \", lam)) + xlim(-1,13)\n}\n\nPois1 <- plotPois(0.5)\nPois2 <- plotPois(1)\nPois3 <- plotPois(5) + scale_y_continuous(breaks = c(0, 0.1))\ngrid.arrange(Pois1,Pois2,Pois3,ncol=1)\n```\n:::\n\n\n:::\n\n# Continuous Random Variables\n\n## Continuous Random Variables {.smaller}\n\n- A continuous random variable can take on an uncountably infinite number of values. \n- With continuous random variables, we define probabilities using __probability density functions__ (pdfs). \\index{probability density function (pdf)} \n- Probabilities are calculated by computing the area under the density curve over the interval of interest. So, given a pdf, $f(y)$, we can compute\n\n. . .\n\n\\begin{align*}\nP(a \\le Y \\le b) = \\int_a^b f(y)dy.\n\\end{align*}\n\n## Continuous Random Variables {.smaller}\n\nA few properties of continuous random variables:\n\n- The area under their density curve is 1. ($\\int_{-\\infty}^{\\infty} f(y)dy = 1$).  \n- For any value $y$, $P(Y = y) =  \\int_y^y f(y)dy = 0$.  Why?\n- Because of the above property, $P(y < Y) = P(y \\le Y)$. We will typically use the first notation rather than the second, but both are equally valid.\n\n\n## Exponential Random Variable {.smaller}\n\n- Suppose we have a Poisson process with rate $\\lambda$, and we wish to model the wait time $Y$ until the first event. \n- We could model $Y$ using an __exponential distribution__, \\index{exponential distribution} where\n\n. . .\n\n\\begin{equation}\nf(y) = \\lambda e^{-\\lambda y} \\quad \\textrm{for} \\quad y > 0,\n\\end{equation}\n- $\\E(Y) = 1/\\lambda$ and $\\SD(Y) = 1/\\lambda$. \n\n\n## Exponential Distribution {.smaller}\n\nExponential distributions with $\\lambda = 0.5, 1,$ and $5$.\n\n:::{.panel-tabset}\n\n### Plot \n\n::::{.columns}\n:::{.column width=\"70%\"}\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Exponential Distribution](03_distribution_ch3_files/figure-revealjs/multExp-1.png){fig-align='center' width=60%}\n:::\n:::\n\n\n:::\n\n:::{.column}\n\n\n- As $\\lambda$ increases, $\\E(Y)$ tends towards 0, and distributions \"die off\" quicker. \n\n:::\n::::\n\n### Code \n\n\n::: {.cell}\n\n```{.r .cell-code}\n#exponentialPlots\nx=seq(0,4,by=0.01)  # possible values\nprobex1 <- dexp(x,.5)  # P(Y=y)\nprobex2 <- dexp(x,1)\nprobex3 <- dexp(x,5)\nExpdf <- tibble(x,probex1, probex2, probex3) %>%\n  rename(x = x,\n         `0.5` = probex1,\n         `1` = probex2,\n         `5` = probex3) %>%\n  gather(2:4, key = \"Lambda\", value = \"value\") %>%\n  mutate(Lambda = factor(Lambda, levels = c(\"0.5\", \"1\", \"5\")))\nggplot(data = Expdf, aes(x = x, y = value, color = Lambda)) +\n  geom_line(aes(linetype = Lambda)) +\n  xlab(\"values\") + ylab(\"density\") + \n  labs(title = \"Exponential Distributions\") + \n  xlim(0,4) + ylim(0,3)\n```\n\n::: {.cell-output-display}\n![](03_distribution_ch3_files/figure-revealjs/multExp2-1.png){width=960}\n:::\n:::\n\n\n- To use R, `pexp(y, lambda)` outputs the probability $P(Y < y)$ given $\\lambda$. \n\n:::\n\n## Gamma Random Variable {.smaller}\n\n- Once again consider a Poisson process. \n- When discussing exponential random variables, we modeled the wait time before one event occurred. \n- If $Y$ represents the wait time before $r$ events occur in a Poisson process with rate $\\lambda$, $Y$ follows a __gamma distribution__ \\index{gamma distribution} where\n\n. . .\n\n\\begin{equation}\nf(y) = \\frac{\\lambda^r}{\\Gamma(r)} y^{r-1} e^{-\\lambda y}\\quad \\textrm{for} \\quad y >0.\n\\end{equation}\n\n- If $Y \\sim \\textrm{Gamma}(r, \\lambda)$ then $\\E(Y) = r/\\lambda$ and $\\SD(Y) = \\sqrt{r/\\lambda^2}$. \n- Note: $\\Gamma(r)=(r-1)!$ (There is more to it)\n\n## Gamma Distribution\n\n:::{.panel-tabset}\n\n### Plot \n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](03_distribution_ch3_files/figure-revealjs/multGamma-1.png){fig-align='center' width=60%}\n:::\n:::\n\n\n- Means increase as $r$ increases, but decrease as $\\lambda$ increases.\n\n### Code\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- seq(0, 7, by = 0.01)\n`r = 1, lambda = 1` <- dgamma(x, 1, rate = 1)\n`r = 2, lambda = 1` <- dgamma(x, 2, rate = 1) \n`r = 5, lambda = 5` <- dgamma(x, 5, rate = 5)\n`r = 5, lambda = 7` <- dgamma(x, 5, rate = 7)\n\ngammaDf <- tibble(x, `r = 1, lambda = 1`, `r = 2, lambda = 1`, `r = 5, lambda = 5`, `r = 5, lambda = 7`) %>%\n  gather(2:5, key = \"Distribution\", value = \"value\") %>%\n  mutate(Distribution = factor(Distribution, \n                               levels = c(\"r = 2, lambda = 1\", \n                                          \"r = 1, lambda = 1\", \n                                          \"r = 5, lambda = 5\", \n                                          \"r = 5, lambda = 7\")))\n\nggplot(data = gammaDf, aes(x = x, y = value, \n                           color = Distribution)) +\n  geom_line(aes(linetype = Distribution)) +\n  xlab(\"values\") + ylab(\"density\") + \n  labs(title = \"Gamma Distributions\") +\n  theme(legend.title = element_blank())\n```\n:::\n\n\n:::\n\n\n## Gamma vs Others {.smaller}\n- Note that if we let $r = 1$, we have the following pdf, \n\n. . .\n\n\\begin{align*}\n f(y) &= \\frac{\\lambda}{\\Gamma(1)} y^{1-1} e^{-\\lambda y} \\\\\n      &= \\lambda e^{-\\lambda y} \\quad \\textrm{for} \\quad y > 0,\n\\end{align*}\nan exponential distribution. \n- Just as how the geometric distribution was a special case of the negative binomial, exponential distributions are in fact a special case of gamma distributions!\n\n- Just like negative binomial, the pdf of a gamma distribution is defined for all real, non-negative $r$.\n- In R, `pgamma(y, r, lambda)` outputs the probability $P(Y < y)$ given $r$ and $\\lambda$.\n\n\n## Distributions Used in Testing {.smaller}\n\n- We have spent most of this chapter discussing probability distributions that may come in handy when modeling. \n- The following distributions, while rarely used in modeling, prove useful in hypothesis testing as certain commonly used test statistics follow these distributions. \n- $\\chi^2$ distribution (requires a degree of freedom)\n- Student $t$ distribution\n- $F$ distribution (need 2 different degrees of freedom)\n- Since we have used these In the past, we will leave their definitions to be referenced if needed\n\n\n\n\n## Distribution Table!\n\n- Would not fit on a slide\n\n- Click [HERE](https://bookdown.org/roback/bookdown-BeyondMLR/ch-distthry.html#tab:distTable)\n\n- The \"web\" of distributions: <https://www.acsu.buffalo.edu/~adamcunn/probability/poisson.html>\n\n## Acknowledgements\n\nThese slides are based on content in [BMLR: Chapter 1 - Review of Multiple Linear Regression](https://bookdown.org/roback/bookdown-BeyondMLR/ch-MLRreview.html)\n\n\n\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}