{
  "hash": "6acbad0ea6e64dca1c43bb188732e5da",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Welcome and Chapter 1\"\nauthor: \"Tyler George\"\nformat: \n  revealjs:\n    output-file: \"01-welcome_ch1.html\"\n    slide-number: true\n  html:\n    output-file: \"01-welcome_ch1_o.html\"\nlogo: \"../img/favicon.png\"\n---\n\n\n# Welcome!\n\n## Instructor\n\n- {{< var instructor.name >}}: [{{< var instructor.email >}}](mailto:tgeorge@cornellcollege.edu)\n\n\n\n## Course logistics\n\n- Course Dates: {{< var course.dates >}}\n- Course sessions: {{< var course.time >}}\n- Exam Dates: {{< var course.exam_dates >}}\n\n## Generalized Linear Models {.smaller}\n\n*In statistics, a generalized linear model (GLM) is a flexible generalization of ordinary linear regression. The GLM generalizes linear regression by allowing the linear model to be related to the response variable via a link function and by allowing the magnitude of the variance of each measurement to be a function of its predicted value.* \n\n::: aside\n\n[Wikipedia](https://en.wikipedia.org/wiki/Generalized_linear_model)\n\n:::\n\n. . .\n\n**Logistic regression**\n\n$$\\begin{aligned}\\pi = P(y = 1 | x) \\hspace{2mm} &\\Rightarrow \\hspace{2mm} \\text{Link function: } \\log\\big(\\frac{\\pi}{1-\\pi}\\big) \\\\\n&\\Rightarrow \\log\\big(\\frac{\\pi}{1-\\pi}\\big) = \\beta_0 + \\beta_1~x\\end{aligned}$$\n\n\n## What we're covering this semester(1/3)\n\n**Generalized Linear Models (Ch 1 - 6)**\n\n- Introduce models for non-normal response variables\n- Estimation, interpretation, and inference\n- Mathematical details showing how GLMs are connected\n\n## What we're covering this semester(2/3)\n\n**Modeling correlated data (Ch 7 - 9)**\n\n- Introduce multilevel models for correlated and longitudinal data\n- Estimation, interpretation, and inference\n- Mathematical details, particularly diving into covariance structures\n\n## What we're covering this semester(3/3)\n\n**More Regression Models (ITSL Chapter 7)**\n- Polynomial Regression\n- Regression Splines\n- Smoothing Splines\n- Generalized Additive Models (GAMS)\n\n\n## Meet your classmates! {.smaller}\n\n- Create larger groups\n- Quick introductions - Name, year, and major\n- Choose a reporter\n  - Need help choosing? Person with birthday closest to December 1st. \n- Identify 8 things everyone in the group has in common\n  - Not being a Cornell Student\n  - Not clothes (we're all wearing socks)\n  - Not body parts (we all have a nose)\n\n. . .\n\n**Reporter will share list with the class**\n\n## What background is assumed for the course? {.smaller}\n\n. . .\n\n**Pre-reqs**\n\n- STA 201, 202 and DSC 223\n\n. . .\n\n**Background knowledge** \n\n:::: {.columns}\n\n::: {.column }\n- Statistical content\n  - Linear and logistic regression \n  - Statistical inference\n  - Basic understanding of random variables\n  \n:::\n\n::: {.column}\n- Computing \n  - Using R for data analysis\n  - Writing reports using R Markdown or Quarto\n  \n:::\n::::\n\n## Course Toolkit (1/2)\n\n\n- **Website**\n  - [{{< var course.url >}}]({{< var course.url >}})\n  - Central hub for the course\n  - Notes\n  - Labs\n  - Datasets\n\n## Course Toolkit (1/2)\n\n- **Moodle**: \n  - [{{< var course.moodle >}}]({{< var course.moodle >}})\n  - Submissions\n  - Gradebook\n  - Announcements\n  \n\n\n\n## Class Meetings \n\n**Lectures**\n\n- Some traditional lecture\n- Individual and group labs\n- Bring fully-charged laptop\n- Mini-projects \n- Exams\n\n. . .\n\n**Attendance is expected (if you are healthy!)**\n\n\n## Textbook\n\n:::: {.columns}\n\n::: {.column }\n\n![](img/bmlr.jpeg)\n\n\n:::\n\n::: {.column}\n*Beyond Multiple Linear Regression* by Paul Roback and Julie Legler\n\n- Available [online](https://bookdown.org/roback/bookdown-BeyondMLR/)\n- Hard copies available for purchase\n\n:::\n::::\n\n## Textbook 2\n\n\n{{< var course.text2 >}}\n\n\n\n- Hard copies available for purchase\n\n\n## Using R / RStudio\n  - RStudio Server is installed and should be used\n  - [{{< var college.rstudio >}}]({{< var college.rstudio >}})\n\n\n## Activities & Assessments {.smaller}\n\n\n**Readings**\n\n  - Primarily from *Beyond Multiple Linear Regression* \n  - Recommend reading assigned text before lecture\n\n. . .\n\n**Homework**\n  - Primarily from *Beyond Multiple Linear Regression* \n  - Individual assignments\n  - Work together but must complete your own work. Discuss but don't copy. \n\n\n## Activities & Assessments {.smaller}\n\n**Mini-projects**\n\nExamples: \n\n- Mini-project 01: Focused on models for non-normal response variables, such as count data\n- Mini-project 02: Focused on models for correlated data\n\n. . .\n\n- Short write up and short presentation\n- Team-based\n\n\n## Exams\n- Two exams this block, {{< var course.exam_dates >}}. \n\n- Each will have two components\n  - Component 1 will be on these dates and you will get a choice of oral or written format. \n  - Component 2 will be a take-home, open-book, open-note, exam. \n  - You will have 12 hours or more to complete this component. \n\n\n## Grading\n\nFinal grades will be calculated as follows\n\n\n| Category              | Points     |\n|-----------------------|------------|\n| Homework              | 200        |\n| Participation         | 100        |\n| Labs and Mini Projects| 300        |\n| Exams                 | 400        |\n| Total                 | 1000       |\n\n\nSee Syllabus on [website]({{< var course.url >}}) for letter grade thresholds.\n\n\n\n## Resources {.smaller}\n\n- **Office hours** to meet with your instructor in {{< var instructor.office >}}\n  - Typically {{< var instructor.officehrs >}}\n  - Double check [course calendar]({{< var course.coursecal >}})\n  - Make appointments by going to [{{< var instructor.appointment_url >}}]({{< var instructor.appointment_url >}})\n\n- **Email** {{< var instructor.name_no_title >}} for private questions regarding personal matters or grades. \n  - Please put **{{< var course.number >}}** in the subject line since I am also teaching capstone this semester\n\n- College support at [{{< var college.support >}}]({{< var college.support >}}).\n\n# Chapter 1 - MLR Review\n\n## Setup - R Packages\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(GGally)\nlibrary(knitr)\nlibrary(patchwork)\nlibrary(viridis)\nlibrary(ggfortify)\n```\n:::\n\n\n## Assumptions for linear regression {.smaller}\n\nWhat are the assumptions for linear regression? \n. . .\n\n**L**inearity: Linear relationship between mean response and predictor variable(s) \n\n. . .\n\n**I**ndependence: Residuals are independent. There is no connection between how far any two points lie above or below regression line.\n\n. . .\n\n**N**ormality: Response follows a normal distribution at each level of the predictor (or combination of predictors)\n\n. . .\n\n**E**qual variance: Variability (variance or standard deviation) of the response is equal for all levels of the predictor (or combination of predictors)\n\n. . .\n\n**Use residual plots to check that the conditions hold before using the model for statistical inference.**\n\n\n\n## Assumptions for linear regression {.smaller}\n\n:::: {.columns}\n::: {.column width=\"40%\"}\n\n::: {.cell}\n::: {.cell-output-display}\n![](01-welcome_ch1_files/figure-revealjs/unnamed-chunk-2-1.png){width=100%}\n:::\n:::\n\n\n::: aside\nModified from Figure 1.1. in BMLR]\n:::\n\n:::\n\n::: {.column}\n\n- **L**inearity: Linear relationship between mean of the response $Y$ and the predictor $X$\n\n- **I**ndependence: No connection between how any two points lie above or below the regression line\n\n- **N**ormality: Response, $Y$, follows a normal distribution at each level of the predictor, $X$ (indicated by red curves)\n\n- **E**qual variance: Variance (or standard deviation) of the response, $Y$, is equal for all levels of the predictor, $X$\n\n:::\n::::\n\n\n\n## Questions\n\nHow do we assess these conditions? \n\n\n\n\n## Beyond linear regression {.smaller}\n\n- When we use linear least squares regression to draw conclusions, we do so under the assumption that L.I.N.E. are all met. \n\n- **Generalized linear models** require different assumptions and can accommodate violations in L.I.N.E.\n  - Relationship between response and predictor(s) can be nonlinear\n  - Response variable can be non-normal \n  - Variance in response can differ at each level of predictor(s) \n\n. . .\n\n**But the independence assumption must hold!**\n\n- **Multilevel models** will be used for data with correlated observations\n\n\n## Review of multiple linear regression \n\n\n\n## Data: Kentucky Derby Winners {.smaller}\n\n\nToday's data is from the Kentucky Derby, an annual 1.25-mile horse race held at the Churchill Downs race track in Louisville, KY. The data is in the file [derbyplus.csv](data/derbyplus.csv) and contains information for races 1896 - 2017. \n\n. . .\n\n::::{.columns}\n:::{.column}\n\n**Response variable**\n\n- `speed`: Average speed of the winner in feet per second (ft/s)]\n\n\n**Additional variable**\n\n- `winner`: Winning horse\n:::\n\n:::{.column}\n\n**Predictor variables**\n\n- `year`: Year of the race\n- `condition`: Condition of the track (good, fast, slow)\n- `starters`: Number of horses who raced]\n\n:::\n::::\n\n\n## Data\n\n\n::: {.cell}\n\n```{.r .cell-code}\nderby <- read_csv(\"data/derbyplus.csv\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nderby |>\n  head(5) |> kable()\n```\n\n::: {.cell-output-display}\n\n\n| year|winner        |condition | speed| starters|\n|----:|:-------------|:---------|-----:|--------:|\n| 1896|Ben Brush     |good      | 51.66|        8|\n| 1897|Typhoon II    |slow      | 49.81|        6|\n| 1898|Plaudit       |good      | 51.16|        4|\n| 1899|Manuel        |fast      | 50.00|        5|\n| 1900|Lieut. Gibson |fast      | 52.28|        7|\n\n\n:::\n:::\n\n\n## Data Analysis Life Cycle\n\n![](img/data-analysis-life-cycle.png)\n\n\n\n## Exploratory data analysis (EDA) {.smaller}\n\n- Once you're ready for the statistical analysis (explore), the first step should always be **exploratory data analysis**.\n\n- The EDA will help you \n  - begin to understand the variables and observations\n  - identify outliers or potential data entry errors\n  - begin to see relationships between variables\n  - identify the appropriate model and identify a strategy\n\n- The EDA is exploratory; formal modeling and statistical inference should be used to draw conclusions.\n\n\n\n## Plots for univariate EDA\n\n\n::: {.panel-tabset}\n\n### Plot \n\n\n::: {.cell}\n::: {.cell-output-display}\n![](01-welcome_ch1_files/figure-revealjs/univar-eda-plot-1.png){width=960}\n:::\n:::\n\n\n\n### Code \n\n\n::: {.cell}\n\n```{.r .cell-code}\np1 <- ggplot(data = derby, aes(x = speed)) + \n  geom_histogram(fill = \"forestgreen\", color = \"black\") + \n  labs(x = \"Winning speed (ft/s)\", y = \"Count\")\n\np2 <- ggplot(data = derby, aes(x = starters)) + \n  geom_histogram(fill = \"forestgreen\", color = \"black\") + \n  labs(x = \"Starters\", y = \"Count\")\n\np3 <- ggplot(data = derby, aes(x = condition)) +\n   geom_bar(fill = \"forestgreen\", color = \"black\", aes(x = ))\n\np1 + (p2 / p3) + \n  plot_annotation(title = \"Univariate data analysis\")\n```\n:::\n\n\n:::\n\n\n## Plots for bivariate EDA {.smaller}\n\n::: {.panel-tabset}\n\n### Plot\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](01-welcome_ch1_files/figure-revealjs/bivar-eda-plot-1.png){width=960}\n:::\n:::\n\n\n### Code\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\np4 <- ggplot(data = derby, aes(x = starters, y = speed)) + \n  geom_point() + \n  labs(x = \"Starters\", y = \"Speed (ft / s)\")\n\np5 <- ggplot(data = derby, aes(x = year, y = speed)) + \n  geom_point() + \n  labs(x = \"Year\", y = \"Speed (ft / s)\")\n\np6 <- ggplot(data = derby, aes(x = condition, y = speed)) + \n  geom_boxplot(fill = \"forestgreen\", color = \"black\") + \n  labs(x = \"Conditions\", y = \"Speed (ft / s)\")\n\n(p4 + p5) + p6 +\n  plot_annotation(title = \"Bivariate data analysis\")\n```\n:::\n\n\n:::\n\n## Scatterplot matrix {.smaller}\n\n\nA **scatterplot matrix** helps quickly visualize relationships between many variable pairs. They are particularly useful to identify potentially correlated predictors.\n\n. . .\n\n::: {.panel-tabset}\n\n### Plot\n\n::: {.cell}\n::: {.cell-output-display}\n![](01-welcome_ch1_files/figure-revealjs/scatterplot-matrix-plot-1.png){width=960}\n:::\n:::\n\n\n\n### Code \n\n\n::: {.cell}\n\n```{.r .cell-code}\n#library(GGally)\nggpairs(data = derby, \n        columns = c(\"condition\", \"year\", \"starters\", \"speed\"))\n```\n:::\n\n\n:::\n\n\n## Plots for multivariate EDA {.smaller}\n\nPlot the relationship between the response and a predictor based on levels of another predictor to assess potential interactions. \n\n::: {.panel-tabset}\n\n### Plot\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](01-welcome_ch1_files/figure-revealjs/multivar-eda-plot-1.png){width=960}\n:::\n:::\n\n\n### Code \n\n\n::: {.cell}\n\n```{.r .cell-code}\n#library(viridis)\nggplot(data = derby, aes(x = year, y = speed, color = condition, \n                         shape = condition, linetype = condition)) + \n  geom_point() + \n  geom_smooth(method = \"lm\", se = FALSE, aes(linetype = condition)) + \n  labs(x = \"Year\", y = \"Speed (ft/s)\", color = \"Condition\",\n       title = \"Speed vs. year\", \n       subtitle = \"by track condition\") +\n  guides(lty = FALSE, shape = FALSE) +\n  scale_color_viridis_d(end = 0.9)\n```\n:::\n\n\n:::\n\n\n\n## Model 1: Main effects model\n\n::: {.panel-tabset}\n\n### Output\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n\n|term          | estimate| std.error| statistic| p.value|\n|:-------------|--------:|---------:|---------:|-------:|\n|(Intercept)   |    8.197|     4.508|     1.818|   0.072|\n|starters      |   -0.005|     0.017|    -0.299|   0.766|\n|year          |    0.023|     0.002|     9.766|   0.000|\n|conditiongood |   -0.443|     0.231|    -1.921|   0.057|\n|conditionslow |   -1.543|     0.161|    -9.616|   0.000|\n\n\n:::\n:::\n\n\n### Code \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Fit and display model\nmodel1 <- lm(speed ~ starters + year + condition, data = derby)\ntidy(model1) |> \n  kable(digits = 3)\n```\n:::\n\n\n:::\n\n## Interpretation {.smaller}\n\n$$\\widehat{speed} = 8.197 - 0.005 ~ starters + 0.023 ~ year - 0.443 ~ good - 1.543 ~ slow$$\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n\n|term          | estimate| std.error| statistic| p.value|\n|:-------------|--------:|---------:|---------:|-------:|\n|(Intercept)   |    8.197|     4.508|     1.818|   0.072|\n|starters      |   -0.005|     0.017|    -0.299|   0.766|\n|year          |    0.023|     0.002|     9.766|   0.000|\n|conditiongood |   -0.443|     0.231|    -1.921|   0.057|\n|conditionslow |   -1.543|     0.161|    -9.616|   0.000|\n\n\n:::\n:::\n\n\n. . .\n\n\n1. Write out the interpretations for `starters` and `conditiongood`. \n2. Does the intercept have a meaningful interpretation? \n\n\n\n\n## Centering \n\n**Centering**: Subtract a constant from each observation of a given variable\n\n- Do this to make interpretation of model parameters more meaningful (particularly intercept)\n\n- In STA 202, we used **mean-centering** where we subtracted the mean from each observation of given variable\n\n- How does centering change the model? \n\n\n\n## Centering `year` {.smaller}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nderby <- derby |>\n  mutate(yearnew = year - 1896) #1896 = starting year\n```\n:::\n\n\n. . .\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n\n|term          | estimate| std.error| statistic| p.value|\n|:-------------|--------:|---------:|---------:|-------:|\n|(Intercept)   |   52.175|     0.194|   269.079|   0.000|\n|starters      |   -0.005|     0.017|    -0.299|   0.766|\n|yearnew       |    0.023|     0.002|     9.766|   0.000|\n|conditiongood |   -0.443|     0.231|    -1.921|   0.057|\n|conditionslow |   -1.543|     0.161|    -9.616|   0.000|\n\n\n:::\n:::\n\n\n. . .\n\n$$\\widehat{speed} = 52.175 - 0.005 ~ starters + 0.023 ~ yearnew - 0.443 ~ good - 1.543 ~ slow$$\n\n\n\n## Model 1: Check model assumptions {.smaller}\n\n::: {.panel-tabset}\n\n### Plots\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#library(ggfortify)\nautoplot(model1Cent)\n```\n\n::: {.cell-output-display}\n![](01-welcome_ch1_files/figure-revealjs/unnamed-chunk-9-1.png){width=960}\n:::\n:::\n\n\n### Questions\n\nWhich of the model assumptions (LINE) does this pass and/or fail? \n\n:::\n\n## Model 2: Add quadratic effect for year? {.smaller}\n\n::: {.panel-tabset}\n\n### Plot\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](01-welcome_ch1_files/figure-revealjs/year-quad-plot-1.png){width=960}\n:::\n:::\n\n\n### Code \n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(data = derby, aes(x = yearnew, y = speed)) + \n  geom_point(alpha = 0.7) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"blue\") + \n  geom_smooth(se = FALSE, color = \"red\", linetype = 2) + \n  labs(x = \"Years since 1896\", y = \"Speed (ft/s)\", \n       title = \"Speed vs. Years since 1896\")\n```\n:::\n\n\n:::\n\n\n\n## Model 2: Add $yearnew^2$\n\n::: {.panel-tabset}\n\n### Plot\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n\n|term          | estimate| std.error| statistic| p.value|\n|:-------------|--------:|---------:|---------:|-------:|\n|(Intercept)   |  51.4130|    0.1826|  281.5645|  0.0000|\n|starters      |  -0.0253|    0.0136|   -1.8588|  0.0656|\n|yearnew       |   0.0700|    0.0061|   11.4239|  0.0000|\n|I(yearnew^2)  |  -0.0004|    0.0000|   -8.0411|  0.0000|\n|conditiongood |  -0.4770|    0.1857|   -2.5689|  0.0115|\n|conditionslow |  -1.3927|    0.1305|  -10.6701|  0.0000|\n\n\n:::\n:::\n\n\n### Code \n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel2 <- lm(speed ~ starters + yearnew + I(yearnew^2) + condition, \n             data = derby)\ntidy(model2) |> kable(digits = 4)\n```\n:::\n\n\n:::\n\n\n## Interpreting quadratic effects\n\n$$\\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 ~ x_1  + \\hat{\\beta}_2 ~ x_2 + \\hat{\\beta}_3 ~ x_2^2$$\n\n**General interpretation**: When $x_2$ increases from a to b, $y$ is expected to change by $\\hat{\\beta}_2(b - a) + \\hat{\\beta}_3(b^2 - a^2)$, holding $x_1$ constant.\n\n. . .\n\n\n\n## Interpreting quadratic effects\n\n$$\\begin{aligned}\\widehat{speed} = &51.413 - 0.025 ~ starters + 0.070 ~ yearnew \\\\\n& - 0.0004 ~ yearnew^2 - 0.477 ~ good - 1.393 ~ slow\\end{aligned}$$\n\n. . .\n\nQuestions: \n\n*Interpret the effect of year for the 5 most recent years (2013 - 2017).*\n\n\n\n## Model 2: Check model assumptions\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](01-welcome_ch1_files/figure-revealjs/unnamed-chunk-11-1.png){width=70%}\n:::\n:::\n\n\n## Model 3: Include interaction term? {.smaller}\n\nRecall from the EDA...\n\n::: {.panel-tabset}\n\n### Plot\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](01-welcome_ch1_files/figure-revealjs/unnamed-chunk-12-1.png){width=960}\n:::\n:::\n\n\n### Code \n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(viridis)\nggplot(data = derby, aes(x = year, y = speed, color = condition, \n                         shape = condition, linetype = condition)) + \n  geom_point() + \n  geom_smooth(method = \"lm\", se = FALSE, aes(linetype = condition)) + \n  labs(x = \"Year\", y = \"Speed (ft/s)\", color = \"Condition\",\n       title = \"Speed vs. year\", \n       subtitle = \"by track condition\") +\n  guides(lty = FALSE, shape = FALSE) +\n  scale_color_viridis_d(end = 0.9)\n```\n:::\n\n\n:::\n\n## Model 3: Add interaction term {.smaller}\n\n$$\\begin{aligned}\\widehat{speed} = & 52.387 - 0.003 ~ starters + 0.020 ~ yearnew - 1.070 ~ good - 2.183 ~ slow \\\\ &+0.012 ~ yearnew \\times good + 0.012 ~ yearnew \\times slow \\end{aligned}$$\n\n::: {.panel-tabset}\n\n### Output \n\n\n::: {.cell}\n::: {.cell-output-display}\n\n\n|term                  | estimate| std.error| statistic| p.value|\n|:---------------------|--------:|---------:|---------:|-------:|\n|(Intercept)           |   52.387|     0.200|   262.350|   0.000|\n|starters              |   -0.003|     0.016|    -0.189|   0.850|\n|yearnew               |    0.020|     0.003|     7.576|   0.000|\n|conditiongood         |   -1.070|     0.423|    -2.527|   0.013|\n|conditionslow         |   -2.183|     0.270|    -8.097|   0.000|\n|yearnew:conditiongood |    0.012|     0.008|     1.598|   0.113|\n|yearnew:conditionslow |    0.012|     0.004|     2.866|   0.005|\n\n\n:::\n:::\n\n\n### Code\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel3 <- lm(speed ~ starters + yearnew + condition +\n               yearnew * condition, \n             data = derby)\ntidy(model3) |> kable(digits = 4)\n```\n:::\n\n\n### Assumptions\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](01-welcome_ch1_files/figure-revealjs/unnamed-chunk-15-1.png){width=960}\n:::\n:::\n\n\n:::\n\n\n## Interpreting interaction effects {.smaller}\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n\n|term                  | estimate| std.error| statistic| p.value|\n|:---------------------|--------:|---------:|---------:|-------:|\n|(Intercept)           |   52.387|     0.200|   262.350|   0.000|\n|starters              |   -0.003|     0.016|    -0.189|   0.850|\n|yearnew               |    0.020|     0.003|     7.576|   0.000|\n|conditiongood         |   -1.070|     0.423|    -2.527|   0.013|\n|conditionslow         |   -2.183|     0.270|    -8.097|   0.000|\n|yearnew:conditiongood |    0.012|     0.008|     1.598|   0.113|\n|yearnew:conditionslow |    0.012|     0.004|     2.866|   0.005|\n\n\n:::\n:::\n\n\n- Write out the interpretation of...\n\n## Which model would you choose?\n\n::: {.panel-tabset}\n\n\n### Model 1: Main effects\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n\n| r.squared| adj.r.squared|     AIC|     BIC|\n|---------:|-------------:|-------:|-------:|\n|      0.73|         0.721| 259.478| 276.302|\n\n\n:::\n:::\n\n\n### Model 2: Main effects + $year^2$\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n\n| r.squared| adj.r.squared|     AIC|     BIC|\n|---------:|-------------:|-------:|-------:|\n|     0.827|         0.819| 207.429| 227.057|\n\n\n:::\n:::\n\n\n### Model 3: Main effects + interaction\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n\n| r.squared| adj.r.squared|     AIC|     BIC|\n|---------:|-------------:|-------:|-------:|\n|     0.751|         0.738| 253.584| 276.016|\n\n\n:::\n:::\n\n\n### Code\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Model 1\nglance(model1Cent) |>\n  select(r.squared, adj.r.squared, AIC, BIC) |>\n  kable(digits = 3)\n\n# Model2\nglance(model2) |>\n  select(r.squared, adj.r.squared, AIC, BIC) |>\n  kable(digits = 3)\n\n# Model 3\nglance(model3) |>\n  select(r.squared, adj.r.squared, AIC, BIC) |>\n  kable(digits = 3)\n```\n:::\n\n\n:::\n\n## What are these model quality metrics? \n\n- **How do we define RSquared?**\n\n- **What is adj.r.squared?**\n\n\n## Measures of model performance {.smaller}\n\n- $\\color{#4187aa}{R^2}$: Proportion of variability in the response explained by the model.\n  -  Will always increase as predictors are added, so it shouldn't be used to compare models\n  \n- $\\color{#4187aa}{Adj. R^2}$: Similar to $R^2$ with a penalty for extra terms\n\n. . .\n\n- $\\color{#4187aa}{AIC}$: Likelihood-based approach balancing model performance and complexity\n\n- $\\color{#4187aa}{BIC}$: Similar to AIC with stronger penalty for extra terms\n\n. . . \n\n- **Nested F Test (extra sum of squares F test)**: Generalization of t-test for individual coefficients to perform significance tests on nested models\n\n\n## Which model would you choose?\n\nUse the **`glance`** function to get model statistics.\n\n::: {.panel-tabset}\n\n### Output\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n\n|model  | r.squared| adj.r.squared|     AIC|     BIC|\n|:------|---------:|-------------:|-------:|-------:|\n|Model1 |     0.730|         0.721| 259.478| 276.302|\n|Model2 |     0.827|         0.819| 207.429| 227.057|\n|Model3 |     0.751|         0.738| 253.584| 276.016|\n\n\n:::\n:::\n\n\n\n### Code \n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel1_glance <- glance(model1Cent) |>\n  select(r.squared, adj.r.squared, AIC, BIC)\nmodel2_glance <- glance(model2) |>\n  select(r.squared, adj.r.squared, AIC, BIC)\nmodel3_glance <- glance(model3) |>\n  select(r.squared, adj.r.squared, AIC, BIC)\n\nmodel1_glance |>\n  bind_rows(model2_glance) |>\n  bind_rows(model3_glance) |>\n  bind_cols(model = c(\"Model1\", \"Model2\", \"Model3\")) |>\n  select(model, everything()) |>\nkable(digits = 3)\n```\n:::\n\n\n:::\n\n## Characteristics of a \"good\" final model {.smaller}\n\n- Model can be used to answer primary research questions\n- Predictor variables control for important covariates\n- Potential interactions have been investigated\n- Variables are centered, as needed, for more meaningful interpretations \n- unnecessary terms are removed \n- Assumptions are met and influential points have been addressed\n- model tells a \"persuasive story parsimoniously\"\n\n::: aside\n[List from Section 1.6.7 of BMLR](https://bookdown.org/roback/bookdown-BeyondMLR/)\n:::\n\n\n## Inference for multiple linear regression {.smaller}\n\nUse statistical inference to \n\n- Determine if predictors are statistically significant (not necessarily practically significant!)\n- Quantify uncertainty in coefficient estimates\n- Quantify uncertainty in model predictions\n\n. . .\n\nIf L.I.N.E. assumptions are met, we can conduct inference using the $t$ distribution and estimated standard errors \n\n## Inference for regression {.smaller}\n\n::: {.panel-tabset}\n\n### When L.I.N.E. conditions are met \n\n\n::: {.cell}\n::: {.cell-output-display}\n![](01-welcome_ch1_files/figure-revealjs/unnamed-chunk-23-1.png){width=100%}\n:::\n:::\n\n\n### We can\n\n- Use least squares regression to get the estimates $\\hat{\\beta}_0$, $\\hat{\\beta}_1$, and $\\hat{\\sigma}^2$\n\n- $\\hat{\\sigma}$ is the **regression standard error** \n\n. . .\n\n$$\\hat{\\sigma} = \\sqrt{\\frac{\\sum_{i=1}^n(y_i - \\hat{y}_i)^2}{n - p - 1}} = \\sqrt{\\frac{\\sum_{i=1}^n e_i^2}{n-p-1}}$$\n:::\n\n\n## Acknowledgements\n\nThese slides are based on content in [BMLR: Chapter 1 - Review of Multiple Linear Regression](https://bookdown.org/roback/bookdown-BeyondMLR/ch-MLRreview.html)\n\nInitial versions of the slides are by Dr. Maria Tackett, Duke University\n\n\n\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}