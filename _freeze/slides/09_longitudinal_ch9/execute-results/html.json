{
  "hash": "c9a729469f37d4ba8155bea701a56ce1",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Modeling two-level longitudinal data\"\nsubtitle: \"BMLR Chapter 9\"\nformat: \n  revealjs:\n    output-file: \"09_longitudinal_ch9.html\"\n    slide-number: true\n  html:\n    output-file: \"09_longitudinal_ch9_o.html\"\nlogo: \"../img/favicon.png\"\ncategories: [\"⚠️ Warning: Page Under Revision\"]\n---\n\n## Setup\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(GGally)\nlibrary(knitr)\nlibrary(patchwork)\nlibrary(viridis)\nlibrary(ggfortify)\nlibrary(kableExtra)\nlibrary(lme4)\nlibrary(broom.mixed)\n```\n:::\n\n\n## Learning goals {.smaller}\n\n- Describe general process for fitting and comparing multilevel models\n\n- Fit and interpret multilevel models for longitudinal data\n\n- Compare multilevel models\n\n- Conduct inference for random effects \n\n- Conduct inference for fixed effects\n\n\n## Data: Charter schools in MN{.smaller}\n\nThe data set [`charter-long.csv`](data/charter-long.csv) contains standardized test scores and demographic information for schools in Minneapolis, MN from 2008 to 2010. The data were collected by the Minnesota Department of Education. Understanding the effectiveness of charter schools is of particular interest, since they often incorporate unique methods of instruction and learning that differ from public schools.\n\n- **`MathAvgScore`**: Average MCA-II score for all 6th grade students in a school (response variable)\n- **`urban`**: urban (1) or rural (0) location school location\n- **`charter`**: charter school (1) or a non-charter public school (0)\n- **`schPctfree`**: proportion of students who receive free or reduced lunches in a school (based on 2010 figures).\n- **`year08`**: Years since 2008\n\n\n\n## Data {.smaller}\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncharter <- read_csv(\"data/charter-long.csv\")\n\ncharter |>\n  select(schoolName, year08, urban, charter, schPctfree, MathAvgScore) |>\n  slice(1:3, 1852:1854) |>\n  kable(digits = 3)\n```\n\n::: {.cell-output-display}\n\n\n|schoolName                         | year08| urban| charter| schPctfree| MathAvgScore|\n|:----------------------------------|------:|-----:|-------:|----------:|------------:|\n|RIPPLESIDE ELEMENTARY              |      0|     0|       0|      0.363|        652.8|\n|RIPPLESIDE ELEMENTARY              |      1|     0|       0|      0.363|        656.6|\n|RIPPLESIDE ELEMENTARY              |      2|     0|       0|      0.363|        652.6|\n|RICHARD ALLEN MATH&SCIENCE ACADEMY |      0|     1|       1|      0.545|           NA|\n|RICHARD ALLEN MATH&SCIENCE ACADEMY |      1|     1|       1|      0.545|           NA|\n|RICHARD ALLEN MATH&SCIENCE ACADEMY |      2|     1|       1|      0.545|        631.2|\n\n\n:::\n:::\n\n\n\n\n## Assess missingness (1/2) {.smaller}\n\nMissing data is common in longitudinal data. Before starting the analysis, it is important to understand the missing data patterns. Use the `skim` function from the **skimr** R package to get a quick view of the missingness.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(skimr)\ncharter |> skim() |> select(skim_variable, n_missing, complete_rate)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 9 × 3\n  skim_variable n_missing complete_rate\n  <chr>             <int>         <dbl>\n1 schoolid              0         1    \n2 schoolName            0         1    \n3 urban                 0         1    \n4 charter               0         1    \n5 schPctnonw            0         1    \n6 schPctsped            0         1    \n7 schPctfree            0         1    \n8 year08                0         1    \n9 MathAvgScore        121         0.935\n```\n\n\n:::\n:::\n\n\n## Assess missingness (2/2) {.smaller}\n\nAnother option is from the `DataExplorer` package. The function creations a plot. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(DataExplorer)\nplot_missing(charter)\n```\n\n::: {.cell-output-display}\n![](09_longitudinal_ch9_files/figure-revealjs/unnamed-chunk-4-1.png){width=960}\n:::\n:::\n\n\n\n\n## Closer look at missing {.smaller}\n\n- **Question:** If we are going to fit a multilevel by *schoolid*, what types of missing data will be particularly problematic? \n\n. . .\n\n:::{.panel-tabset}\n\n### Output\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n\n| MathAvgScore0_miss| MathAvgScore1_miss| MathAvgScore2_miss|   n|\n|------------------:|------------------:|------------------:|---:|\n|                  0|                  0|                  0| 540|\n|                  0|                  0|                  1|   6|\n|                  0|                  1|                  0|   4|\n|                  0|                  1|                  1|   7|\n|                  1|                  0|                  0|  25|\n|                  1|                  0|                  1|   1|\n|                  1|                  1|                  0|  35|\n\n\n:::\n:::\n\n\n\n### Code\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncharter |>\n  select(schoolid, schoolName, year08, MathAvgScore) |>\n  pivot_wider(id_cols = c(schoolid, schoolName), names_from = year08,\n              names_prefix = \"MathAvgScore.\", values_from = MathAvgScore) |> \n  mutate(MathAvgScore0_miss = if_else(is.na(MathAvgScore.0), 1, 0),\n         MathAvgScore1_miss = if_else(is.na(MathAvgScore.1), 1, 0),\n         MathAvgScore2_miss = if_else(is.na(MathAvgScore.2), 1, 0)) |> \n  count(MathAvgScore0_miss, MathAvgScore1_miss, MathAvgScore2_miss) |> \n  kable()\n```\n:::\n\n\n:::\n\n\n\n## Dealing with missing data {.smaller}\n\n- **Complete case analysis**: Only include schools with complete data for all three years. This would remove 12.6% of observations in this data. \n\n\n\n- **Last observation carried forward**: Keep the last observation from each group (school) and conduct analysis for independent observations. \n\n\n\n- **Impute missing observations**: \"Fill in\" values of missing observations using the typical observed trends from groups with similar covariates.   \n\n\n\n- **Apply multilevel methods**: Estimate patterns using available data recognizing that trends for groups with complete data are more precise than for those with fewer measurements. This is under the condition that the probability of missingness does not depend on unobserved predictors or the response.\n\n. . .\n\n**Question:** What is an advantage of each method? What is a disadvantage?\n\n\n## Strategy for building multilevel models {.smaller}\n\n- Conduct exploratory data analysis for Level One and Level Two variables.\n\n- Fit model with no covariates to assess variability at each level.\n\n- Create Level One models. Start with a single term, then add terms as needed. \n\n- Create Level Two models. Start with a single term, then add terms as needed. Start with equation for intercept term. \n\n- Begin with the full set of variance components, then remove variance terms as needed. \n\n. . .\n\n*Alternate model building strategies in BMLR Section 8.6*\n\n\n\n## Exploratory data analysis{.smaller}\n\nGiven the longitudinal structure of the data, we are able to answer questions at two levels\n\n- **Level One (within school)**: How did average math scores for a given school change over time? \n\n- **Level Two (between schools)**: What is the effect of school-specific covariates on the average math scores in 2008 and the rate of change from 2008 to 2010? \n\n. . .\n\nWe can conduct exploratory data analysis at both levels, e.g., \n\n- Univariate and bivariate EDA\n- lattice plots\n- Spaghetti plots \n\n\n\n- Let's do some EDA together.\n\n\nSee BMLR Section 9.3 for full exploratory data analysis.\n\n\n\n## Unconditional means model {.smaller}\n\nStart with the **unconditional means model**, a model with no covariates at any level. This is also called the random intercepts model.\n\n**Level One** : $Y_{ij} = a_{i} + \\epsilon_{ij}, \\hspace{5mm} \\epsilon_{ij} \\sim N(0, \\sigma^2)$ \n\n**Level Two**: $a_i = \\alpha_0 + u_i, \\hspace{5mm} u_{i} \\sim N(0, \\sigma^2_u)$\n\n<br> <br>\n\n**Question:** Write the composite model.\n\n\n\n\n## Intraclass correlation (1/2) {.smaller}\n\nThe **intraclass correlation** is the relative variability between groups\n\n$$\\hat{\\rho} = \\frac{\\text{Between variability}}{\\text{Total variability}} = \\frac{\\hat{\\sigma}_u^2}{\\hat{\\sigma}^2_u + \\hat{\\sigma}^2}$$\n\n- What is the meaning of $\\hat{\\rho}$ close to 0? \n\n- What is the meaning of $\\hat{\\rho}$ close to 1? \n\n. . .\n\n**Question:** Fit the unconditional means model and calculate the intraclass correlation.\n\n\n\n\n\n\n\n## Intraclass correlation (2/2) {.smaller}\n\nThe **intraclass correlation** is the relative variability between groups\n\n$\\hat{\\rho} = 0.798$. This means...\n\n\n- About 79.8% of the variability in math scores can be attributed to differences between schools (school-to-school variability). About 20.2% of the variability can be attributed to changes over time.\n\n\n- The average correlation between any two responses from the same school is about 0.798.\n\n\n- The **effective sample size** (number of independent pieces of information available for modeling) is closer to the number of schools $(\\rho \\text{ close to 1})$ than the number of observations $(\\rho\\text{ close to 0})$\n\n\n## Unconditional growth model {.smaller}\n\nA next step in the model building is the **unconditional growth model**, a  model with Level One predictors but no Level Two predictors. \n\n**Level One**: $Y_{ij} = a_i + b_iYear08_{ij} + \\epsilon_{ij}$\n\n**Level Two**: \n  - $a_i = \\alpha_0 + u_i$\n  - $b_i = \\beta_0 + v_i$\n. . .\n\n. . .\n\n### Questions:\n\n- Write the composite model.\n- What can we learn from this model? \n- Fit the unconditional growth model.\n\n\n\n## Pseudo $R^2$ {.smaller}\n\nWe can use **Pseudo R<sup>2</sup>** to explain changes in variance components between two models \n\n- **Note**: This should only be used when the definition of the variance component is the same between the two models \n\n. . .\n\n$$\\text{Pseudo }R^2 = \\frac{\\hat{\\sigma}^2(\\text{Model 1})  -  \\hat{\\sigma}^2(\\text{Model 2})}{\\hat{\\sigma}^2(\\text{Model 1})}$$\n\n. . .\n\n\n**Question:** Calculate the $\\text{Pseudo }R^2$ to estimate the change of within school variance between the unconditional means and unconditional growth models.\n\n\n\n\n\n## Model with school-level covariates {.smaller}\n\nFit a model with school-level covariates that takes the following form: \n\n**Level One**\n\n\\begin{equation*}\nY_{ij}= a_{i} + b_{i}Year08_{ij} + \\epsilon_{ij}\n\\end{equation*}\n\n**Level Two**\n\n\\begin{align*}\na_{i} & = \\alpha_{0} + \\alpha_{1}Charter_i + \\alpha_{2}urban_i + \\alpha_{3}schpctfree_i + u_{i} \\\\\nb_{i} & = \\beta_{0} + \\beta_{1}Charter_i + \\beta_{2}urban_i + v_{i}\n\\end{align*}\n\n\n\n## Questions {.smaller}\n\n- Write out the composite model. \n- Fit the model in R. \n- Use the model to describe how the average math scores differed between charter and non-charter schools.\n\n\n## Consider a simpler model {.smaller}\n\n\n**Would a model without the effects $v_i$ and $\\rho_{uv}$ be preferable?**\n\n\n. . .\n\n**Level One**\n\n\\begin{equation*}\nY_{ij}= a_{i} + b_{i}Year08_{ij} + \\epsilon_{ij}\n\\end{equation*}\n\n**Level Two**\n\n\\begin{align*}\na_{i} & = \\alpha_{0} + \\alpha_{1}Charter_i + \\alpha_{2}urban_i + \\alpha_{3}schpctfree_i + u_{i} \\\\\nb_{i} & = \\beta_{0} + \\beta_{1}Charter_i + \\beta_{2}urban_i \n\\end{align*}\n\n<br> \n\n. . .\n\nIn this model, the effect of year is the same for all schools with a given combination of `Charter` and `urban`\n\n\n\n\n## Compare two models {.smaller}\n\n**Full model**\n\n$$\\begin{aligned}Y_{ij} = [&\\alpha_{0} + \\alpha_{1}Charter_i + \\alpha_{2}Urban_i+ \\alpha_{3}schpctfree_i \\\\ &+ \\beta_0Year08_{ij} + \\beta_1Charter_i:Year08_{ij} + \\beta_2Urban_i:Year_{ij}] \\\\&+ [u_i + v_iYear08_{ij} + \\epsilon_{ij}]\\end{aligned}$$\n\nwhere \n\n$$\\left[ \\begin{array}{c}\n            u_{i} \\\\ v_{i}\n          \\end{array}  \\right] \\sim N \\left( \\left[\n          \\begin{array}{c}\n            0 \\\\ 0\n          \\end{array} \\right], \\left[\n          \\begin{array}{cc}\n            \\sigma_{u}^{2} & \\sigma_{uv} \\\\\n            \\sigma_{uv} & \\sigma_{v}^{2}\n          \\end{array} \\right] \\right) \\hspace{2mm} \\text{ and }\\hspace{2mm} \\epsilon_{ij} \\sim N(0, \\sigma^2)$$\n\n. . .\n\n**Estimated fixed effects**: $\\alpha_0, \\alpha_1, \\alpha_2, \\alpha_3, \\beta_0, \\beta_1, \\beta_2$\n\n**Variance components to estimate**: $\\sigma, \\sigma_u, \\sigma_v, \\rho_{uv}$ (Note: $\\sigma_{uv} = \\rho_{uv}\\sigma_u\\sigma_v$)\n\n\n\n## Compare two models {.smaller}\n\n**Null Model (simplified variance structure)**\n\n$$\\begin{aligned}Y_{ij} = [&\\alpha_{0} + \\alpha_{1}Charter_i + \\alpha_{2}Urban_i+ \\alpha_{3}schpctfree_i \\\\ &+ \\beta_0Year08_{ij} + \\beta_1Charter_i:Year08_{ij} + \\beta_2Urban_i:Year_{ij}] \\\\&+ [u_i + \\epsilon_{ij}]\\end{aligned}$$\n\nwhere \n\n$$u_i \\sim N(0, \\sigma^2_u) \\hspace{2mm} \\text{ and }\\hspace{2mm} \\epsilon_{ij} \\sim N(0, \\sigma^2)$$\n\n. . .\n\n**Estimated fixed effects**: $\\alpha_0, \\alpha_1, \\alpha_2, \\alpha_3, \\beta_0, \\beta_1, \\beta_2$\n\n**Variance components to estimate**: $\\sigma, \\sigma_u$\n\n\n\n## Full and reduced models  {.smaller}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfull_model <- lmer(MathAvgScore ~ charter + urban + schPctfree + \n                     charter:year08  + urban:year08 + year08 + \n                     (year08|schoolid), REML = T, data = charter) #<<\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nreduced_model <-  lmer(MathAvgScore ~ charter + urban + schPctfree + \n                     charter:year08  + urban:year08 + year08 +\n                       (1 | schoolid), REML = T, data = charter) #<<\n```\n:::\n\n\n\n. . .\n\n**Hypotheses**\n\n$$\\begin{aligned}&H_0: \\sigma_v = \\rho_{uv} = 0\\\\\n&H_a: \\text{at least one of the parameters is not equal to 0}\\end{aligned}$$\n\n*Note*: $\\rho_{uv} \\neq 0 \\Rightarrow \\sigma_v \\neq 0$\n\n## Issues with the drop-in-deviance test {.smaller}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nanova(full_model, reduced_model, test = \"Chisq\") |>\n  kable(digits = 3)\n```\n\n::: {.cell-output-display}\n\n\n|              | npar|      AIC|      BIC|    logLik| -2*log(L)| Chisq| Df| Pr(>Chisq)|\n|:-------------|----:|--------:|--------:|---------:|---------:|-----:|--:|----------:|\n|reduced_model |    9| 9952.992| 10002.11| -4967.496|  9934.992|    NA| NA|         NA|\n|full_model    |   11| 9953.793| 10013.83| -4965.897|  9931.793| 3.199|  2|      0.202|\n\n\n:::\n:::\n\n\n- $\\chi^2$ test conservative, i.e., the p-values are larger than they should be, when testing random effects at the boundary ( e.g., $\\sigma_v^2 = 0$) or those with bounded ranges (e.g., $\\rho_{uv}$ )\n\n- If you observe small p-values, you can feel relatively certain the tested effects are statistically significant\n\n- Use bootstrap methods to obtain more accurate p-values \n\n\n\n## Parametric bootstrapping {.smaller}\n\n- **Bootstrapping** is from the phrase \"pulling oneself up by one’s bootstraps\"\n\n- Accomplishing a difficult task without any outside help\n\n- **Task**: conduct inference for model parameters (fixed and random effects) using only the sample data \n\n\n\n## Parametric bootstrapping for likelihood ratio test {.smaller}\n\n1️⃣  Fit the null (reduced) model to obtain the fixed effects and variance components (*parametric* part).\n\n2️⃣  Use the estimated fixed effects and variance components to generate a new set of response values with the same sample size and associated covariates for each observation as the original data (*bootstrap* part).\n\n3️⃣  Fit the full and null models to the newly generated data.\n\n4️⃣   Compute the likelihood test statistic comparing the models from the previous step.\n\n5️⃣  Repeat steps 2 - 4 many times (~ 1000).\n\n\n\n## Parametric bootstrapping for likelihood ratio test {.smaller}\n\n\n- 6️⃣   Create a histogram of the likelihood ratio statistics to get the distribution of likelihood ratio statistic under the null hypothesis. \n\n- 7️⃣   Get the p-value by calculating the proportion of bootstrapped test statistics greater than the observed statistic.\n\n\n- Let's calculate the bootstrapped p-value for the likelihood ratio test statistic testing whether  $v_i$ and $\\rho_{uv}$ can be removed from the model.\n\n\n## LRT using $\\chi^2$ and parametric bootstrap {.smaller}\n\n**Likelihood ratio test using $\\chi^2$ distribution**\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n\n|              | npar|      AIC|      BIC|    logLik| -2*log(L)| Chisq| Df| Pr(>Chisq)|\n|:-------------|----:|--------:|--------:|---------:|---------:|-----:|--:|----------:|\n|reduced_model |    9| 9952.992| 10002.11| -4967.496|  9934.992|    NA| NA|         NA|\n|full_model    |   11| 9953.793| 10013.83| -4965.897|  9931.793| 3.199|  2|      0.202|\n\n\n:::\n:::\n\n\n**Likelihood ratio test using parametric bootstrap**\n\n| term <chr> | npar <dbl> | AIC <dbl> | BIC <dbl> | logLik <dbl> | deviance <dbl> | statistic <dbl> | df <dbl> | Pr_boot..Chisq. <dbl> |\n|------------|-----------:|----------:|----------:|-------------:|---------------:|----------------:|---------:|----------------------:|\n| m0       |          9 |  9952.992 |  10002.11 |    -4967.496 |       9934.992 |              NA |       NA |                    NA |\n| mA         |         11 |  9953.793 |  10013.83 |    -4965.897 |       9931.793 |        3.199347 |        2 |                 0.144 |\n\n\n\n## Inference for fixed effects  (1/2) {.smaller}\n\n- The output for multilevel models do not contain p-values for the coefficients of fixed effects \n\n- The exact distribution of the test statistic under the null hypothesis (no fixed effect) is unknown, because the exact degrees of freedom are unknown\n  - Finding suitable approximations is an area of ongoing research \n\n- In the tidy function, with its variant from `broom.mixed`, you can ask for `conf.int = T` to get confidence intervals.\n\n- We can look up how that works in the packages vignette. Let's look up the package! \n\n\n## Inference for fixed effects (2/2) {.smaller}\n\n- We can use likelihood ratio test with an approximate $\\chi^2$ distribution to test these effects, since we're not testing on the boundary and fixed effects do not have limited ranges\n  - Some research suggests the p-values are too low but approximations are generally pretty good\n  - Can also calculate the p-values using parametric bootstrap approach\n\n- Let's test whether `schPctFree` should be included in the current model using likelihood ratio test with the $\\chi^2$ distribution and the parametric bootstrap.\n\n## Methods to conduct inference for individual coefficients {.smaller}\n\n- Use the t-value $\\big(\\frac{estimate}{std.error}\\big)$ in the model output \n  - General rule: Coefficients with |t-value| > 2 considered to be statistically significant, i.e., different from 0 \n  \n- The confidence intervals given by `tidy` are not proven to be good\n  - We better use multiple approaches and see if they agree. \n  \n- Calculate confidence intervals using **nonparametric bootstrapping** \n\n\n\n## Nonparametric bootstrapping {.smaller}\n\n1️⃣ &nbsp; Take a sample, with replacement, of size $n$ (the size of the original data) (called *case resampling*).\n    \n2️⃣ &nbsp; Fit the model to obtain estimates of the coefficients.\n    \n3️⃣ &nbsp; Repeat steps 1 - 2 many times (~ 1000) to obtain the bootstrap distribution.\n    \n4️⃣ &nbsp; Get the coefficients for the 95% confidence interval by taking the middle 95% of the bootstrap distribution.\n\n\n\n\n\n## Bootstrapped CI for coefficients {.smaller}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nconfint(reduced_model, method = \"boot\", level = 0.95, oldNames = F) |>\n  kable(digits = 3)\n```\n\n::: {.cell-output-display}\n\n\n|                             |   2.5 %|  97.5 %|\n|:----------------------------|-------:|-------:|\n|sd_(Intercept)&#124;schoolid |   4.238|   4.856|\n|sigma                        |   2.855|   3.103|\n|(Intercept)                  | 659.437| 661.493|\n|charter                      |  -4.336|  -1.490|\n|urban                        |  -1.974|  -0.235|\n|schPctfree                   | -19.550| -16.217|\n|year08                       |   1.220|   1.815|\n|charter:year08               |   0.390|   1.552|\n|urban:year08                 |  -0.907|  -0.162|\n\n\n:::\n:::\n\n\n\n\n\n## Summary: Model comparisons {.smaller}\n\n**Methods to compare models with different fixed effects**\n\n- Likelihood ratio tests based on $\\chi^2$ distribution \n- Likelihood ratio test based on parametric bootstrapped p-values\n- AIC or BIC\n\n\n. . .\n\n**Methods to compare models with different variance components**\n\n- Likelihood ratio test based on parametric bootstrapped p-values\n- AIC or BIC\n\n\n \n## Summary: Understanding the model {.smaller}\n\n**Methods to understand individual coefficients**\n\n- Likelihood ratio tests\n- Bootstrap confidence intervals\n- Pseudo $R^2$ for variance components (if meaning is unchanged between models)\n\n. . .\n\n**Methods to understand data structure**\n\n- Calculate intraclass correlation coefficient using unconditional means model\n\n\n\n\n\n## Acknowledgements {.smaller}\n\nThe content in the slides is from \n \n- [BMLR: Chapter 9 - Two-level Longitudinal Data](https://bookdown.org/roback/bookdown-BeyondMLR/ch-lon.html)\n  - Sections 9.1 - 9.6\n\n- [Hierarchical Linear Modeling with Maximum Likelihood, Restricted Maximum Likelihood, and Fully Bayesian Estimation](https://scholarworks.umass.edu/cgi/viewcontent.cgi?article=1353&context=pare) by Peter Boedeker\n\n- *Applied longitudinal data analysis: Modeling change and event occurrence*. by J.D. Singer and J.B. Willett \n  - Online copy available through Duke library\n  \n- *Extending the linear model with R*. by Julian Faraway\n  - Online copy available through Duke library\n\n- Initial versions of the slides are by Dr. Maria Tackett, Duke University\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n  \n  \n  \n  \n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}