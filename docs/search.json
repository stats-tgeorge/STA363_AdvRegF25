[
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Advanced Regression",
    "section": "",
    "text": "Instructor Dr. Tyler George\n   Cornell College, West 311\n    tgeorge@cornellcollege.edu \n\n\n\n\nAugust 26th to September 18th\n   By Appointment\n   West 201\n\n\n\n\n   MWTh 3:05pm-4:05pm and by appt.\n   West 311\n   Optional Appointment",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#ai-policy",
    "href": "syllabus.html#ai-policy",
    "title": "Advanced Regression",
    "section": "AI Policy",
    "text": "AI Policy\nThe beta release of Dall-E-Mini in July 2022 and ChatGPT in November 2022 are among many tools using artificial intelligence. There is a good possibility that using tools like these are going to become an important skill for careers in the not distant future (https://www.theguardian.com/commentisfree/2023/jan/07/chatgpt-bot-excel-ai-chatbot-tech).\nIn the meantime though, it’s going to take a while for society to figure out when using these tools is/isn’t acceptable.\nWork created by AI tools may not be considered original work and, instead, considered automated plagiarism. It is derived from previously created texts from other sources that the models were trained on, yet doesn’t cite sources. AI models have built-in biases (ie, they are trained on limited underlying sources; they reproduce, rather than challenge, errors in the sources) AI tools have limitations (ie, they lack critical thinking to evaluate and reflect on criteria; they lack abductive reasoning to make judgments with incomplete information at hand; they make up or use inaccurate information and may “hallucinate” sources that do not exist)\nIn this course, all informal writing should be written without the use of AI. The purpose of informal writing is to help you think through your ideas, connect with your lived experiences, and to figure out your thoughts and opinions. Using AI here subverts that process.\nA final note: Other courses may have different AI policies, and it is important to be aware of the policy in each class.\n\nDISABILITIES AND ACCOMODATIONS POLICY\nCornell College makes reasonable accommodations for persons with disabilities. Students should notify the Office of Academic Support and Advising and their course instructor of any disability related accommodations within the first three days of the term for which the accommodations are required, due to the fast pace of the block format. For more information on the documentation required to establish the need for accommodations and the process of requesting the accommodations.\n\n\nACADEMIC HONESTY POLICY\nCornell College expects all members of the Cornell community to act with academic integrity. An important aspect of academic integrity is respecting the work of others. A student is expected to explicitly acknowledge ideas, claims, observations, or data of others, unless generally known. When a piece of work is submitted for credit, a student is asserting that the submission is her or his work unless there is a citation of a specific source. If there is no appropriate acknowledgment of sources, whether intended or not, this may constitute a violation of the College’s requirement for honesty in academic work and may be treated as a case of academic dishonesty. The procedures regarding how the College deals with cases of academic dishonesty appear in The Catalog, under the heading “Academic Honesty.”\n\n\nIllness Policy\nIf you are experiencing COVID-19 symptoms, do not attend class. Perform a home test or contact Director of Student Health Services Lynn O’Brien at student_health@cornellcollege.edu immediately to arrange a COVID-19 test at the Health Center. If you need to isolate due to COVID-19, or if you become unable to attend class for any other health reason, contact me as soon as possible to determine if you are able to continue in the class. A Withdrawal for Health Reasons may be required.\n\n\nMandatory Reporter Reminder\nIt is my goal that you feel supported and able to share information related to your life experiences during classroom discussions, in your written work, and in any one-on-one meetings with me. You should also know that all Cornell College faculty and staff are mandatory reporters. This means that I will keep information you share with me private to the greatest extent possible. However, I am required to share information regarding sexual assault, abuse, criminal behavior, or about a student who may be a danger to themselves or to others. If you wish to speak to someone confidentially who is not a mandatory reporter, you can schedule an appointment with one of the counselors in the Ebersole Health and Wellbeing Center or contact the College Chaplain, Rev. Melea White, at mwhite@cornelllcollege.edu.",
    "crumbs": [
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "slides/09_longitudinal_ch9.html#setup",
    "href": "slides/09_longitudinal_ch9.html#setup",
    "title": "Modeling two-level longitudinal data",
    "section": "Setup",
    "text": "Setup\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(GGally)\nlibrary(knitr)\nlibrary(patchwork)\nlibrary(viridis)\nlibrary(ggfortify)\nlibrary(kableExtra)\nlibrary(lme4)\nlibrary(broom.mixed)"
  },
  {
    "objectID": "slides/09_longitudinal_ch9.html#learning-goals",
    "href": "slides/09_longitudinal_ch9.html#learning-goals",
    "title": "Modeling two-level longitudinal data",
    "section": "Learning goals",
    "text": "Learning goals\n\nDescribe general process for fitting and comparing multilevel models\nFit and interpret multilevel models for longitudinal data\nCompare multilevel models\nConduct inference for random effects\nConduct inference for fixed effects"
  },
  {
    "objectID": "slides/09_longitudinal_ch9.html#data-charter-schools-in-mn",
    "href": "slides/09_longitudinal_ch9.html#data-charter-schools-in-mn",
    "title": "Modeling two-level longitudinal data",
    "section": "Data: Charter schools in MN",
    "text": "Data: Charter schools in MN\nThe data set charter-long.csv contains standardized test scores and demographic information for schools in Minneapolis, MN from 2008 to 2010. The data were collected by the Minnesota Department of Education. Understanding the effectiveness of charter schools is of particular interest, since they often incorporate unique methods of instruction and learning that differ from public schools.\n\nMathAvgScore: Average MCA-II score for all 6th grade students in a school (response variable)\nurban: urban (1) or rural (0) location school location\ncharter: charter school (1) or a non-charter public school (0)\nschPctfree: proportion of students who receive free or reduced lunches in a school (based on 2010 figures).\nyear08: Years since 2008"
  },
  {
    "objectID": "slides/09_longitudinal_ch9.html#data",
    "href": "slides/09_longitudinal_ch9.html#data",
    "title": "Modeling two-level longitudinal data",
    "section": "Data",
    "text": "Data\n\ncharter &lt;- read_csv(\"data/charter-long.csv\")\n\nRows: 1854 Columns: 9\n── Column specification ──────────────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): schoolid, schoolName\ndbl (7): urban, charter, schPctnonw, schPctsped, schPctfree, year08, MathAvgScore\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ncharter |&gt;\n  select(schoolName, year08, urban, charter, schPctfree, MathAvgScore) |&gt;\n  slice(1:3, 1852:1854) |&gt;\n  kable(digits = 3)\n\n\n\n\n\n\n\n\n\n\n\n\nschoolName\nyear08\nurban\ncharter\nschPctfree\nMathAvgScore\n\n\n\n\nRIPPLESIDE ELEMENTARY\n0\n0\n0\n0.363\n652.8\n\n\nRIPPLESIDE ELEMENTARY\n1\n0\n0\n0.363\n656.6\n\n\nRIPPLESIDE ELEMENTARY\n2\n0\n0\n0.363\n652.6\n\n\nRICHARD ALLEN MATH&SCIENCE ACADEMY\n0\n1\n1\n0.545\nNA\n\n\nRICHARD ALLEN MATH&SCIENCE ACADEMY\n1\n1\n1\n0.545\nNA\n\n\nRICHARD ALLEN MATH&SCIENCE ACADEMY\n2\n1\n1\n0.545\n631.2"
  },
  {
    "objectID": "slides/09_longitudinal_ch9.html#assess-missingness-12",
    "href": "slides/09_longitudinal_ch9.html#assess-missingness-12",
    "title": "Modeling two-level longitudinal data",
    "section": "Assess missingness (1/2)",
    "text": "Assess missingness (1/2)\nMissing data is common in longitudinal data. Before starting the analysis, it is important to understand the missing data patterns. Use the skim function from the skimr R package to get a quick view of the missingness.\n\nlibrary(skimr)\n\nWarning: package 'skimr' was built under R version 4.4.1\n\ncharter |&gt; skim() |&gt; select(skim_variable, n_missing, complete_rate)\n\n# A tibble: 9 × 3\n  skim_variable n_missing complete_rate\n  &lt;chr&gt;             &lt;int&gt;         &lt;dbl&gt;\n1 schoolid              0         1    \n2 schoolName            0         1    \n3 urban                 0         1    \n4 charter               0         1    \n5 schPctnonw            0         1    \n6 schPctsped            0         1    \n7 schPctfree            0         1    \n8 year08                0         1    \n9 MathAvgScore        121         0.935"
  },
  {
    "objectID": "slides/09_longitudinal_ch9.html#assess-missingness-22",
    "href": "slides/09_longitudinal_ch9.html#assess-missingness-22",
    "title": "Modeling two-level longitudinal data",
    "section": "Assess missingness (2/2)",
    "text": "Assess missingness (2/2)\nAnother option is from the DataExplorer package. The function creations a plot.\n\nlibrary(DataExplorer)\n\nWarning: package 'DataExplorer' was built under R version 4.4.1\n\nplot_missing(charter)"
  },
  {
    "objectID": "slides/09_longitudinal_ch9.html#closer-look-at-missing",
    "href": "slides/09_longitudinal_ch9.html#closer-look-at-missing",
    "title": "Modeling two-level longitudinal data",
    "section": "Closer look at missing",
    "text": "Closer look at missing\n\nQuestion: If we are going to fit a multilevel by schoolid, what types of missing data will be particularly problematic?\n\n\n\nOutputCode\n\n\n\n\n\n\n\nMathAvgScore0_miss\nMathAvgScore1_miss\nMathAvgScore2_miss\nn\n\n\n\n\n0\n0\n0\n540\n\n\n0\n0\n1\n6\n\n\n0\n1\n0\n4\n\n\n0\n1\n1\n7\n\n\n1\n0\n0\n25\n\n\n1\n0\n1\n1\n\n\n1\n1\n0\n35\n\n\n\n\n\n\n\n\ncharter |&gt;\n  select(schoolid, schoolName, year08, MathAvgScore) |&gt;\n  pivot_wider(id_cols = c(schoolid, schoolName), names_from = year08,\n              names_prefix = \"MathAvgScore.\", values_from = MathAvgScore) |&gt; \n  mutate(MathAvgScore0_miss = if_else(is.na(MathAvgScore.0), 1, 0),\n         MathAvgScore1_miss = if_else(is.na(MathAvgScore.1), 1, 0),\n         MathAvgScore2_miss = if_else(is.na(MathAvgScore.2), 1, 0)) |&gt; \n  count(MathAvgScore0_miss, MathAvgScore1_miss, MathAvgScore2_miss) |&gt; \n  kable()"
  },
  {
    "objectID": "slides/09_longitudinal_ch9.html#dealing-with-missing-data",
    "href": "slides/09_longitudinal_ch9.html#dealing-with-missing-data",
    "title": "Modeling two-level longitudinal data",
    "section": "Dealing with missing data",
    "text": "Dealing with missing data\n\nComplete case analysis: Only include schools with complete data for all three years. This would remove 12.6% of observations in this data.\nLast observation carried forward: Keep the last observation from each group (school) and conduct analysis for independent observations.\nImpute missing observations: “Fill in” values of missing observations using the typical observed trends from groups with similar covariates.\nApply multilevel methods: Estimate patterns using available data recognizing that trends for groups with complete data are more precise than for those with fewer measurements. This is under the condition that the probability of missingness does not depend on unobserved predictors or the response.\n\n\nQuestion: What is an advantage of each method? What is a disadvantage?"
  },
  {
    "objectID": "slides/09_longitudinal_ch9.html#strategy-for-building-multilevel-models",
    "href": "slides/09_longitudinal_ch9.html#strategy-for-building-multilevel-models",
    "title": "Modeling two-level longitudinal data",
    "section": "Strategy for building multilevel models",
    "text": "Strategy for building multilevel models\n\nConduct exploratory data analysis for Level One and Level Two variables.\nFit model with no covariates to assess variability at each level.\nCreate Level One models. Start with a single term, then add terms as needed.\nCreate Level Two models. Start with a single term, then add terms as needed. Start with equation for intercept term.\nBegin with the full set of variance components, then remove variance terms as needed.\n\n\nAlternate model building strategies in BMLR Section 8.6"
  },
  {
    "objectID": "slides/09_longitudinal_ch9.html#exploratory-data-analysis",
    "href": "slides/09_longitudinal_ch9.html#exploratory-data-analysis",
    "title": "Modeling two-level longitudinal data",
    "section": "Exploratory data analysis",
    "text": "Exploratory data analysis\nGiven the longitudinal structure of the data, we are able to answer questions at two levels\n\nLevel One (within school): How did average math scores for a given school change over time?\nLevel Two (between schools): What is the effect of school-specific covariates on the average math scores in 2008 and the rate of change from 2008 to 2010?\n\n\nWe can conduct exploratory data analysis at both levels, e.g.,\n\nUnivariate and bivariate EDA\nlattice plots\nSpaghetti plots\nLet’s do some EDA together.\n\nSee BMLR Section 9.3 for full exploratory data analysis."
  },
  {
    "objectID": "slides/09_longitudinal_ch9.html#unconditional-means-model",
    "href": "slides/09_longitudinal_ch9.html#unconditional-means-model",
    "title": "Modeling two-level longitudinal data",
    "section": "Unconditional means model",
    "text": "Unconditional means model\nStart with the unconditional means model, a model with no covariates at any level. This is also called the random intercepts model.\nLevel One : \\(Y_{ij} = a_{i} + \\epsilon_{ij}, \\hspace{5mm} \\epsilon_{ij} \\sim N(0, \\sigma^2)\\)\nLevel Two: \\(a_i = \\alpha_0 + u_i, \\hspace{5mm} u_{i} \\sim N(0, \\sigma^2_u)\\)\n \nQuestion: Write the composite model."
  },
  {
    "objectID": "slides/09_longitudinal_ch9.html#intraclass-correlation-12",
    "href": "slides/09_longitudinal_ch9.html#intraclass-correlation-12",
    "title": "Modeling two-level longitudinal data",
    "section": "Intraclass correlation (1/2)",
    "text": "Intraclass correlation (1/2)\nThe intraclass correlation is the relative variability between groups\n\\[\\hat{\\rho} = \\frac{\\text{Between variability}}{\\text{Total variability}} = \\frac{\\hat{\\sigma}_u^2}{\\hat{\\sigma}^2_u + \\hat{\\sigma}^2}\\]\n\nWhat is the meaning of \\(\\hat{\\rho}\\) close to 0?\nWhat is the meaning of \\(\\hat{\\rho}\\) close to 1?\n\n\nQuestion: Fit the unconditional means model and calculate the intraclass correlation."
  },
  {
    "objectID": "slides/09_longitudinal_ch9.html#intraclass-correlation-22",
    "href": "slides/09_longitudinal_ch9.html#intraclass-correlation-22",
    "title": "Modeling two-level longitudinal data",
    "section": "Intraclass correlation (2/2)",
    "text": "Intraclass correlation (2/2)\nThe intraclass correlation is the relative variability between groups\n\\(\\hat{\\rho} = 0.798\\). This means…\n\nAbout 79.8% of the variability in math scores can be attributed to differences between schools (school-to-school variability). About 20.2% of the variability can be attributed to changes over time.\nThe average correlation between any two responses from the same school is about 0.798.\nThe effective sample size (number of independent pieces of information available for modeling) is closer to the number of schools \\((\\rho \\text{ close to 1})\\) than the number of observations \\((\\rho\\text{ close to 0})\\)"
  },
  {
    "objectID": "slides/09_longitudinal_ch9.html#unconditional-growth-model",
    "href": "slides/09_longitudinal_ch9.html#unconditional-growth-model",
    "title": "Modeling two-level longitudinal data",
    "section": "Unconditional growth model",
    "text": "Unconditional growth model\nA next step in the model building is the unconditional growth model, a model with Level One predictors but no Level Two predictors.\nLevel One: \\(Y_{ij} = a_i + b_iYear08_{ij} + \\epsilon_{ij}\\)\nLevel Two: - \\(a_i = \\alpha_0 + u_i\\) - \\(b_i = \\beta_0 + v_i\\) . . .\n\nQuestions:\n\nWrite the composite model.\nWhat can we learn from this model?\nFit the unconditional growth model."
  },
  {
    "objectID": "slides/09_longitudinal_ch9.html#pseudo-r2",
    "href": "slides/09_longitudinal_ch9.html#pseudo-r2",
    "title": "Modeling two-level longitudinal data",
    "section": "Pseudo \\(R^2\\)",
    "text": "Pseudo \\(R^2\\)\nWe can use Pseudo R2 to explain changes in variance components between two models\n\nNote: This should only be used when the definition of the variance component is the same between the two models\n\n\n\\[\\text{Pseudo }R^2 = \\frac{\\hat{\\sigma}^2(\\text{Model 1})  -  \\hat{\\sigma}^2(\\text{Model 2})}{\\hat{\\sigma}^2(\\text{Model 1})}\\]\n\n\nQuestion: Calculate the \\(\\text{Pseudo }R^2\\) to estimate the change of within school variance between the unconditional means and unconditional growth models."
  },
  {
    "objectID": "slides/09_longitudinal_ch9.html#model-with-school-level-covariates",
    "href": "slides/09_longitudinal_ch9.html#model-with-school-level-covariates",
    "title": "Modeling two-level longitudinal data",
    "section": "Model with school-level covariates",
    "text": "Model with school-level covariates\nFit a model with school-level covariates that takes the following form:\nLevel One\n\\[\\begin{equation*}\nY_{ij}= a_{i} + b_{i}Year08_{ij} + \\epsilon_{ij}\n\\end{equation*}\\]\nLevel Two\n\\[\\begin{align*}\na_{i} & = \\alpha_{0} + \\alpha_{1}Charter_i + \\alpha_{2}urban_i + \\alpha_{3}schpctfree_i + u_{i} \\\\\nb_{i} & = \\beta_{0} + \\beta_{1}Charter_i + \\beta_{2}urban_i + v_{i}\n\\end{align*}\\]"
  },
  {
    "objectID": "slides/09_longitudinal_ch9.html#questions-1",
    "href": "slides/09_longitudinal_ch9.html#questions-1",
    "title": "Modeling two-level longitudinal data",
    "section": "Questions",
    "text": "Questions\n\nWrite out the composite model.\nFit the model in R.\nUse the model to describe how the average math scores differed between charter and non-charter schools."
  },
  {
    "objectID": "slides/09_longitudinal_ch9.html#consider-a-simpler-model",
    "href": "slides/09_longitudinal_ch9.html#consider-a-simpler-model",
    "title": "Modeling two-level longitudinal data",
    "section": "Consider a simpler model",
    "text": "Consider a simpler model\nWould a model without the effects \\(v_i\\) and \\(\\rho_{uv}\\) be preferable?\n\nLevel One\n\\[\\begin{equation*}\nY_{ij}= a_{i} + b_{i}Year08_{ij} + \\epsilon_{ij}\n\\end{equation*}\\]\nLevel Two\n\\[\\begin{align*}\na_{i} & = \\alpha_{0} + \\alpha_{1}Charter_i + \\alpha_{2}urban_i + \\alpha_{3}schpctfree_i + u_{i} \\\\\nb_{i} & = \\beta_{0} + \\beta_{1}Charter_i + \\beta_{2}urban_i\n\\end{align*}\\]\n\n\n\nIn this model, the effect of year is the same for all schools with a given combination of Charter and urban"
  },
  {
    "objectID": "slides/09_longitudinal_ch9.html#compare-two-models",
    "href": "slides/09_longitudinal_ch9.html#compare-two-models",
    "title": "Modeling two-level longitudinal data",
    "section": "Compare two models",
    "text": "Compare two models\nFull model\n\\[\\begin{aligned}Y_{ij} = [&\\alpha_{0} + \\alpha_{1}Charter_i + \\alpha_{2}Urban_i+ \\alpha_{3}schpctfree_i \\\\ &+ \\beta_0Year08_{ij} + \\beta_1Charter_i:Year08_{ij} + \\beta_2Urban_i:Year_{ij}] \\\\&+ [u_i + v_iYear08_{ij} + \\epsilon_{ij}]\\end{aligned}\\]\nwhere\n\\[\\left[ \\begin{array}{c}\n            u_{i} \\\\ v_{i}\n          \\end{array}  \\right] \\sim N \\left( \\left[\n          \\begin{array}{c}\n            0 \\\\ 0\n          \\end{array} \\right], \\left[\n          \\begin{array}{cc}\n            \\sigma_{u}^{2} & \\sigma_{uv} \\\\\n            \\sigma_{uv} & \\sigma_{v}^{2}\n          \\end{array} \\right] \\right) \\hspace{2mm} \\text{ and }\\hspace{2mm} \\epsilon_{ij} \\sim N(0, \\sigma^2)\\]\n\nEstimated fixed effects: \\(\\alpha_0, \\alpha_1, \\alpha_2, \\alpha_3, \\beta_0, \\beta_1, \\beta_2\\)\nVariance components to estimate: \\(\\sigma, \\sigma_u, \\sigma_v, \\rho_{uv}\\) (Note: \\(\\sigma_{uv} = \\rho_{uv}\\sigma_u\\sigma_v\\))"
  },
  {
    "objectID": "slides/09_longitudinal_ch9.html#compare-two-models-1",
    "href": "slides/09_longitudinal_ch9.html#compare-two-models-1",
    "title": "Modeling two-level longitudinal data",
    "section": "Compare two models",
    "text": "Compare two models\nNull Model (simplified variance structure)\n\\[\\begin{aligned}Y_{ij} = [&\\alpha_{0} + \\alpha_{1}Charter_i + \\alpha_{2}Urban_i+ \\alpha_{3}schpctfree_i \\\\ &+ \\beta_0Year08_{ij} + \\beta_1Charter_i:Year08_{ij} + \\beta_2Urban_i:Year_{ij}] \\\\&+ [u_i + \\epsilon_{ij}]\\end{aligned}\\]\nwhere\n\\[u_i \\sim N(0, \\sigma^2_u) \\hspace{2mm} \\text{ and }\\hspace{2mm} \\epsilon_{ij} \\sim N(0, \\sigma^2)\\]\n\nEstimated fixed effects: \\(\\alpha_0, \\alpha_1, \\alpha_2, \\alpha_3, \\beta_0, \\beta_1, \\beta_2\\)\nVariance components to estimate: \\(\\sigma, \\sigma_u\\)"
  },
  {
    "objectID": "slides/09_longitudinal_ch9.html#full-and-reduced-models",
    "href": "slides/09_longitudinal_ch9.html#full-and-reduced-models",
    "title": "Modeling two-level longitudinal data",
    "section": "Full and reduced models",
    "text": "Full and reduced models\n\nfull_model &lt;- lmer(MathAvgScore ~ charter + urban + schPctfree + \n                     charter:year08  + urban:year08 + year08 + \n                     (year08|schoolid), REML = T, data = charter) #&lt;&lt;\n\n\nreduced_model &lt;-  lmer(MathAvgScore ~ charter + urban + schPctfree + \n                     charter:year08  + urban:year08 + year08 +\n                       (1 | schoolid), REML = T, data = charter) #&lt;&lt;\n\n\nHypotheses\n\\[\\begin{aligned}&H_0: \\sigma_v = \\rho_{uv} = 0\\\\\n&H_a: \\text{at least one of the parameters is not equal to 0}\\end{aligned}\\]\nNote: \\(\\rho_{uv} \\neq 0 \\Rightarrow \\sigma_v \\neq 0\\)"
  },
  {
    "objectID": "slides/09_longitudinal_ch9.html#issues-with-the-drop-in-deviance-test",
    "href": "slides/09_longitudinal_ch9.html#issues-with-the-drop-in-deviance-test",
    "title": "Modeling two-level longitudinal data",
    "section": "Issues with the drop-in-deviance test",
    "text": "Issues with the drop-in-deviance test\n\nanova(full_model, reduced_model, test = \"Chisq\") |&gt;\n  kable(digits = 3)\n\nrefitting model(s) with ML (instead of REML)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nnpar\nAIC\nBIC\nlogLik\ndeviance\nChisq\nDf\nPr(&gt;Chisq)\n\n\n\n\nreduced_model\n9\n9952.992\n10002.11\n-4967.496\n9934.992\nNA\nNA\nNA\n\n\nfull_model\n11\n9953.793\n10013.83\n-4965.897\n9931.793\n3.199\n2\n0.202\n\n\n\n\n\n\n\\(\\chi^2\\) test conservative, i.e., the p-values are larger than they should be, when testing random effects at the boundary ( e.g., \\(\\sigma_v^2 = 0\\)) or those with bounded ranges (e.g., \\(\\rho_{uv}\\) )\nIf you observe small p-values, you can feel relatively certain the tested effects are statistically significant\nUse bootstrap methods to obtain more accurate p-values"
  },
  {
    "objectID": "slides/09_longitudinal_ch9.html#parametric-bootstrapping",
    "href": "slides/09_longitudinal_ch9.html#parametric-bootstrapping",
    "title": "Modeling two-level longitudinal data",
    "section": "Parametric bootstrapping",
    "text": "Parametric bootstrapping\n\nBootstrapping is from the phrase “pulling oneself up by one’s bootstraps”\nAccomplishing a difficult task without any outside help\nTask: conduct inference for model parameters (fixed and random effects) using only the sample data"
  },
  {
    "objectID": "slides/09_longitudinal_ch9.html#parametric-bootstrapping-for-likelihood-ratio-test",
    "href": "slides/09_longitudinal_ch9.html#parametric-bootstrapping-for-likelihood-ratio-test",
    "title": "Modeling two-level longitudinal data",
    "section": "Parametric bootstrapping for likelihood ratio test",
    "text": "Parametric bootstrapping for likelihood ratio test\n1️⃣ Fit the null (reduced) model to obtain the fixed effects and variance components (parametric part).\n2️⃣ Use the estimated fixed effects and variance components to generate a new set of response values with the same sample size and associated covariates for each observation as the original data (bootstrap part).\n3️⃣ Fit the full and null models to the newly generated data.\n4️⃣ Compute the likelihood test statistic comparing the models from the previous step.\n5️⃣ Repeat steps 2 - 4 many times (~ 1000)."
  },
  {
    "objectID": "slides/09_longitudinal_ch9.html#parametric-bootstrapping-for-likelihood-ratio-test-1",
    "href": "slides/09_longitudinal_ch9.html#parametric-bootstrapping-for-likelihood-ratio-test-1",
    "title": "Modeling two-level longitudinal data",
    "section": "Parametric bootstrapping for likelihood ratio test",
    "text": "Parametric bootstrapping for likelihood ratio test\n\n6️⃣ Create a histogram of the likelihood ratio statistics to get the distribution of likelihood ratio statistic under the null hypothesis.\n7️⃣ Get the p-value by calculating the proportion of bootstrapped test statistics greater than the observed statistic.\nLet’s calculate the bootstrapped p-value for the likelihood ratio test statistic testing whether \\(v_i\\) and \\(\\rho_{uv}\\) can be removed from the model."
  },
  {
    "objectID": "slides/09_longitudinal_ch9.html#lrt-using-chi2-and-parametric-bootstrap",
    "href": "slides/09_longitudinal_ch9.html#lrt-using-chi2-and-parametric-bootstrap",
    "title": "Modeling two-level longitudinal data",
    "section": "LRT using \\(\\chi^2\\) and parametric bootstrap",
    "text": "LRT using \\(\\chi^2\\) and parametric bootstrap\nLikelihood ratio test using \\(\\chi^2\\) distribution\n\n\nrefitting model(s) with ML (instead of REML)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nnpar\nAIC\nBIC\nlogLik\ndeviance\nChisq\nDf\nPr(&gt;Chisq)\n\n\n\n\nreduced_model\n9\n9952.992\n10002.11\n-4967.496\n9934.992\nNA\nNA\nNA\n\n\nfull_model\n11\n9953.793\n10013.83\n-4965.897\n9931.793\n3.199\n2\n0.202\n\n\n\n\n\nLikelihood ratio test using parametric bootstrap\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nterm \nnpar \nAIC \nBIC \nlogLik \ndeviance \nstatistic \ndf \nPr_boot..Chisq. \n\n\n\n\nm0\n9\n9952.992\n10002.11\n-4967.496\n9934.992\nNA\nNA\nNA\n\n\nmA\n11\n9953.793\n10013.83\n-4965.897\n9931.793\n3.199347\n2\n0.144"
  },
  {
    "objectID": "slides/09_longitudinal_ch9.html#inference-for-fixed-effects-12",
    "href": "slides/09_longitudinal_ch9.html#inference-for-fixed-effects-12",
    "title": "Modeling two-level longitudinal data",
    "section": "Inference for fixed effects (1/2)",
    "text": "Inference for fixed effects (1/2)\n\nThe output for multilevel models do not contain p-values for the coefficients of fixed effects\nThe exact distribution of the test statistic under the null hypothesis (no fixed effect) is unknown, because the exact degrees of freedom are unknown\n\nFinding suitable approximations is an area of ongoing research\n\nIn the tidy function, with its variant from broom.mixed, you can ask for conf.int = T to get confidence intervals.\nWe can look up how that works in the packages vignette. Let’s look up the package!"
  },
  {
    "objectID": "slides/09_longitudinal_ch9.html#inference-for-fixed-effects-22",
    "href": "slides/09_longitudinal_ch9.html#inference-for-fixed-effects-22",
    "title": "Modeling two-level longitudinal data",
    "section": "Inference for fixed effects (2/2)",
    "text": "Inference for fixed effects (2/2)\n\nWe can use likelihood ratio test with an approximate \\(\\chi^2\\) distribution to test these effects, since we’re not testing on the boundary and fixed effects do not have limited ranges\n\nSome research suggests the p-values are too low but approximations are generally pretty good\nCan also calculate the p-values using parametric bootstrap approach\n\nLet’s test whether schPctFree should be included in the current model using likelihood ratio test with the \\(\\chi^2\\) distribution and the parametric bootstrap."
  },
  {
    "objectID": "slides/09_longitudinal_ch9.html#methods-to-conduct-inference-for-individual-coefficients",
    "href": "slides/09_longitudinal_ch9.html#methods-to-conduct-inference-for-individual-coefficients",
    "title": "Modeling two-level longitudinal data",
    "section": "Methods to conduct inference for individual coefficients",
    "text": "Methods to conduct inference for individual coefficients\n\nUse the t-value \\(\\big(\\frac{estimate}{std.error}\\big)\\) in the model output\n\nGeneral rule: Coefficients with |t-value| &gt; 2 considered to be statistically significant, i.e., different from 0\n\nThe confidence intervals given by tidy are not proven to be good\n\nWe better use multiple approaches and see if they agree.\n\nCalculate confidence intervals using nonparametric bootstrapping"
  },
  {
    "objectID": "slides/09_longitudinal_ch9.html#nonparametric-bootstrapping",
    "href": "slides/09_longitudinal_ch9.html#nonparametric-bootstrapping",
    "title": "Modeling two-level longitudinal data",
    "section": "Nonparametric bootstrapping",
    "text": "Nonparametric bootstrapping\n1️⃣   Take a sample, with replacement, of size \\(n\\) (the size of the original data) (called case resampling).\n2️⃣   Fit the model to obtain estimates of the coefficients.\n3️⃣   Repeat steps 1 - 2 many times (~ 1000) to obtain the bootstrap distribution.\n4️⃣   Get the coefficients for the 95% confidence interval by taking the middle 95% of the bootstrap distribution."
  },
  {
    "objectID": "slides/09_longitudinal_ch9.html#bootstrapped-ci-for-coefficients",
    "href": "slides/09_longitudinal_ch9.html#bootstrapped-ci-for-coefficients",
    "title": "Modeling two-level longitudinal data",
    "section": "Bootstrapped CI for coefficients",
    "text": "Bootstrapped CI for coefficients\n\nconfint(reduced_model, method = \"boot\", level = 0.95, oldNames = F) |&gt;\n  kable(digits = 3)\n\nComputing bootstrap confidence intervals ...\n\n\n\n\n\n\n2.5 %\n97.5 %\n\n\n\n\nsd_(Intercept)|schoolid\n4.264\n4.829\n\n\nsigma\n2.870\n3.110\n\n\n(Intercept)\n659.315\n661.415\n\n\ncharter\n-4.374\n-1.395\n\n\nurban\n-1.958\n-0.218\n\n\nschPctfree\n-19.523\n-16.289\n\n\nyear08\n1.259\n1.784\n\n\ncharter:year08\n0.397\n1.639\n\n\nurban:year08\n-0.900\n-0.193"
  },
  {
    "objectID": "slides/09_longitudinal_ch9.html#summary-model-comparisons",
    "href": "slides/09_longitudinal_ch9.html#summary-model-comparisons",
    "title": "Modeling two-level longitudinal data",
    "section": "Summary: Model comparisons",
    "text": "Summary: Model comparisons\nMethods to compare models with different fixed effects\n\nLikelihood ratio tests based on \\(\\chi^2\\) distribution\nLikelihood ratio test based on parametric bootstrapped p-values\nAIC or BIC\n\n\nMethods to compare models with different variance components\n\nLikelihood ratio test based on parametric bootstrapped p-values\nAIC or BIC"
  },
  {
    "objectID": "slides/09_longitudinal_ch9.html#summary-understanding-the-model",
    "href": "slides/09_longitudinal_ch9.html#summary-understanding-the-model",
    "title": "Modeling two-level longitudinal data",
    "section": "Summary: Understanding the model",
    "text": "Summary: Understanding the model\nMethods to understand individual coefficients\n\nLikelihood ratio tests\nBootstrap confidence intervals\nPseudo \\(R^2\\) for variance components (if meaning is unchanged between models)\n\n\nMethods to understand data structure\n\nCalculate intraclass correlation coefficient using unconditional means model"
  },
  {
    "objectID": "slides/09_longitudinal_ch9.html#acknowledgements",
    "href": "slides/09_longitudinal_ch9.html#acknowledgements",
    "title": "Modeling two-level longitudinal data",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThe content in the slides is from\n\nBMLR: Chapter 9 - Two-level Longitudinal Data\n\nSections 9.1 - 9.6\n\nHierarchical Linear Modeling with Maximum Likelihood, Restricted Maximum Likelihood, and Fully Bayesian Estimation by Peter Boedeker\nApplied longitudinal data analysis: Modeling change and event occurrence. by J.D. Singer and J.B. Willett\n\nOnline copy available through Duke library\n\nExtending the linear model with R. by Julian Faraway\n\nOnline copy available through Duke library\n\nInitial versions of the slides are by Dr. Maria Tackett, Duke University"
  },
  {
    "objectID": "slides/09_longitudinal_ch9_o.html",
    "href": "slides/09_longitudinal_ch9_o.html",
    "title": "Modeling two-level longitudinal data",
    "section": "",
    "text": "library(tidyverse)\nlibrary(tidymodels)\nlibrary(GGally)\nlibrary(knitr)\nlibrary(patchwork)\nlibrary(viridis)\nlibrary(ggfortify)\nlibrary(kableExtra)\nlibrary(lme4)\nlibrary(broom.mixed)"
  },
  {
    "objectID": "slides/09_longitudinal_ch9_o.html#setup",
    "href": "slides/09_longitudinal_ch9_o.html#setup",
    "title": "Modeling two-level longitudinal data",
    "section": "",
    "text": "library(tidyverse)\nlibrary(tidymodels)\nlibrary(GGally)\nlibrary(knitr)\nlibrary(patchwork)\nlibrary(viridis)\nlibrary(ggfortify)\nlibrary(kableExtra)\nlibrary(lme4)\nlibrary(broom.mixed)"
  },
  {
    "objectID": "slides/09_longitudinal_ch9_o.html#learning-goals",
    "href": "slides/09_longitudinal_ch9_o.html#learning-goals",
    "title": "Modeling two-level longitudinal data",
    "section": "Learning goals",
    "text": "Learning goals\n\nDescribe general process for fitting and comparing multilevel models\nFit and interpret multilevel models for longitudinal data\nCompare multilevel models\nConduct inference for random effects\nConduct inference for fixed effects"
  },
  {
    "objectID": "slides/09_longitudinal_ch9_o.html#data-charter-schools-in-mn",
    "href": "slides/09_longitudinal_ch9_o.html#data-charter-schools-in-mn",
    "title": "Modeling two-level longitudinal data",
    "section": "Data: Charter schools in MN",
    "text": "Data: Charter schools in MN\nThe data set charter-long.csv contains standardized test scores and demographic information for schools in Minneapolis, MN from 2008 to 2010. The data were collected by the Minnesota Department of Education. Understanding the effectiveness of charter schools is of particular interest, since they often incorporate unique methods of instruction and learning that differ from public schools.\n\nMathAvgScore: Average MCA-II score for all 6th grade students in a school (response variable)\nurban: urban (1) or rural (0) location school location\ncharter: charter school (1) or a non-charter public school (0)\nschPctfree: proportion of students who receive free or reduced lunches in a school (based on 2010 figures).\nyear08: Years since 2008"
  },
  {
    "objectID": "slides/09_longitudinal_ch9_o.html#data",
    "href": "slides/09_longitudinal_ch9_o.html#data",
    "title": "Modeling two-level longitudinal data",
    "section": "Data",
    "text": "Data\n\ncharter &lt;- read_csv(\"data/charter-long.csv\")\n\nRows: 1854 Columns: 9\n── Column specification ──────────────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): schoolid, schoolName\ndbl (7): urban, charter, schPctnonw, schPctsped, schPctfree, year08, MathAvgScore\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ncharter |&gt;\n  select(schoolName, year08, urban, charter, schPctfree, MathAvgScore) |&gt;\n  slice(1:3, 1852:1854) |&gt;\n  kable(digits = 3)\n\n\n\n\n\n\n\n\n\n\n\n\nschoolName\nyear08\nurban\ncharter\nschPctfree\nMathAvgScore\n\n\n\n\nRIPPLESIDE ELEMENTARY\n0\n0\n0\n0.363\n652.8\n\n\nRIPPLESIDE ELEMENTARY\n1\n0\n0\n0.363\n656.6\n\n\nRIPPLESIDE ELEMENTARY\n2\n0\n0\n0.363\n652.6\n\n\nRICHARD ALLEN MATH&SCIENCE ACADEMY\n0\n1\n1\n0.545\nNA\n\n\nRICHARD ALLEN MATH&SCIENCE ACADEMY\n1\n1\n1\n0.545\nNA\n\n\nRICHARD ALLEN MATH&SCIENCE ACADEMY\n2\n1\n1\n0.545\n631.2"
  },
  {
    "objectID": "slides/09_longitudinal_ch9_o.html#assess-missingness-12",
    "href": "slides/09_longitudinal_ch9_o.html#assess-missingness-12",
    "title": "Modeling two-level longitudinal data",
    "section": "Assess missingness (1/2)",
    "text": "Assess missingness (1/2)\nMissing data is common in longitudinal data. Before starting the analysis, it is important to understand the missing data patterns. Use the skim function from the skimr R package to get a quick view of the missingness.\n\nlibrary(skimr)\n\nWarning: package 'skimr' was built under R version 4.4.1\n\ncharter |&gt; skim() |&gt; select(skim_variable, n_missing, complete_rate)\n\n# A tibble: 9 × 3\n  skim_variable n_missing complete_rate\n  &lt;chr&gt;             &lt;int&gt;         &lt;dbl&gt;\n1 schoolid              0         1    \n2 schoolName            0         1    \n3 urban                 0         1    \n4 charter               0         1    \n5 schPctnonw            0         1    \n6 schPctsped            0         1    \n7 schPctfree            0         1    \n8 year08                0         1    \n9 MathAvgScore        121         0.935"
  },
  {
    "objectID": "slides/09_longitudinal_ch9_o.html#assess-missingness-22",
    "href": "slides/09_longitudinal_ch9_o.html#assess-missingness-22",
    "title": "Modeling two-level longitudinal data",
    "section": "Assess missingness (2/2)",
    "text": "Assess missingness (2/2)\nAnother option is from the DataExplorer package. The function creations a plot.\n\nlibrary(DataExplorer)\n\nWarning: package 'DataExplorer' was built under R version 4.4.1\n\nplot_missing(charter)"
  },
  {
    "objectID": "slides/09_longitudinal_ch9_o.html#closer-look-at-missing",
    "href": "slides/09_longitudinal_ch9_o.html#closer-look-at-missing",
    "title": "Modeling two-level longitudinal data",
    "section": "Closer look at missing",
    "text": "Closer look at missing\n\nQuestion: If we are going to fit a multilevel by schoolid, what types of missing data will be particularly problematic?\n\n. . .\n\nOutputCode\n\n\n\n\n\n\n\nMathAvgScore0_miss\nMathAvgScore1_miss\nMathAvgScore2_miss\nn\n\n\n\n\n0\n0\n0\n540\n\n\n0\n0\n1\n6\n\n\n0\n1\n0\n4\n\n\n0\n1\n1\n7\n\n\n1\n0\n0\n25\n\n\n1\n0\n1\n1\n\n\n1\n1\n0\n35\n\n\n\n\n\n\n\n\ncharter |&gt;\n  select(schoolid, schoolName, year08, MathAvgScore) |&gt;\n  pivot_wider(id_cols = c(schoolid, schoolName), names_from = year08,\n              names_prefix = \"MathAvgScore.\", values_from = MathAvgScore) |&gt; \n  mutate(MathAvgScore0_miss = if_else(is.na(MathAvgScore.0), 1, 0),\n         MathAvgScore1_miss = if_else(is.na(MathAvgScore.1), 1, 0),\n         MathAvgScore2_miss = if_else(is.na(MathAvgScore.2), 1, 0)) |&gt; \n  count(MathAvgScore0_miss, MathAvgScore1_miss, MathAvgScore2_miss) |&gt; \n  kable()"
  },
  {
    "objectID": "slides/09_longitudinal_ch9_o.html#dealing-with-missing-data",
    "href": "slides/09_longitudinal_ch9_o.html#dealing-with-missing-data",
    "title": "Modeling two-level longitudinal data",
    "section": "Dealing with missing data",
    "text": "Dealing with missing data\n\nComplete case analysis: Only include schools with complete data for all three years. This would remove 12.6% of observations in this data.\nLast observation carried forward: Keep the last observation from each group (school) and conduct analysis for independent observations.\nImpute missing observations: “Fill in” values of missing observations using the typical observed trends from groups with similar covariates.\nApply multilevel methods: Estimate patterns using available data recognizing that trends for groups with complete data are more precise than for those with fewer measurements. This is under the condition that the probability of missingness does not depend on unobserved predictors or the response.\n\n. . .\nQuestion: What is an advantage of each method? What is a disadvantage?"
  },
  {
    "objectID": "slides/09_longitudinal_ch9_o.html#strategy-for-building-multilevel-models",
    "href": "slides/09_longitudinal_ch9_o.html#strategy-for-building-multilevel-models",
    "title": "Modeling two-level longitudinal data",
    "section": "Strategy for building multilevel models",
    "text": "Strategy for building multilevel models\n\nConduct exploratory data analysis for Level One and Level Two variables.\nFit model with no covariates to assess variability at each level.\nCreate Level One models. Start with a single term, then add terms as needed.\nCreate Level Two models. Start with a single term, then add terms as needed. Start with equation for intercept term.\nBegin with the full set of variance components, then remove variance terms as needed.\n\n. . .\nAlternate model building strategies in BMLR Section 8.6"
  },
  {
    "objectID": "slides/09_longitudinal_ch9_o.html#exploratory-data-analysis",
    "href": "slides/09_longitudinal_ch9_o.html#exploratory-data-analysis",
    "title": "Modeling two-level longitudinal data",
    "section": "Exploratory data analysis",
    "text": "Exploratory data analysis\nGiven the longitudinal structure of the data, we are able to answer questions at two levels\n\nLevel One (within school): How did average math scores for a given school change over time?\nLevel Two (between schools): What is the effect of school-specific covariates on the average math scores in 2008 and the rate of change from 2008 to 2010?\n\n. . .\nWe can conduct exploratory data analysis at both levels, e.g.,\n\nUnivariate and bivariate EDA\nlattice plots\nSpaghetti plots\nLet’s do some EDA together.\n\nSee BMLR Section 9.3 for full exploratory data analysis."
  },
  {
    "objectID": "slides/09_longitudinal_ch9_o.html#unconditional-means-model",
    "href": "slides/09_longitudinal_ch9_o.html#unconditional-means-model",
    "title": "Modeling two-level longitudinal data",
    "section": "Unconditional means model",
    "text": "Unconditional means model\nStart with the unconditional means model, a model with no covariates at any level. This is also called the random intercepts model.\nLevel One : \\(Y_{ij} = a_{i} + \\epsilon_{ij}, \\hspace{5mm} \\epsilon_{ij} \\sim N(0, \\sigma^2)\\)\nLevel Two: \\(a_i = \\alpha_0 + u_i, \\hspace{5mm} u_{i} \\sim N(0, \\sigma^2_u)\\)\n \nQuestion: Write the composite model."
  },
  {
    "objectID": "slides/09_longitudinal_ch9_o.html#intraclass-correlation-12",
    "href": "slides/09_longitudinal_ch9_o.html#intraclass-correlation-12",
    "title": "Modeling two-level longitudinal data",
    "section": "Intraclass correlation (1/2)",
    "text": "Intraclass correlation (1/2)\nThe intraclass correlation is the relative variability between groups\n\\[\\hat{\\rho} = \\frac{\\text{Between variability}}{\\text{Total variability}} = \\frac{\\hat{\\sigma}_u^2}{\\hat{\\sigma}^2_u + \\hat{\\sigma}^2}\\]\n\nWhat is the meaning of \\(\\hat{\\rho}\\) close to 0?\nWhat is the meaning of \\(\\hat{\\rho}\\) close to 1?\n\n. . .\nQuestion: Fit the unconditional means model and calculate the intraclass correlation."
  },
  {
    "objectID": "slides/09_longitudinal_ch9_o.html#intraclass-correlation-22",
    "href": "slides/09_longitudinal_ch9_o.html#intraclass-correlation-22",
    "title": "Modeling two-level longitudinal data",
    "section": "Intraclass correlation (2/2)",
    "text": "Intraclass correlation (2/2)\nThe intraclass correlation is the relative variability between groups\n\\(\\hat{\\rho} = 0.798\\). This means…\n\nAbout 79.8% of the variability in math scores can be attributed to differences between schools (school-to-school variability). About 20.2% of the variability can be attributed to changes over time.\nThe average correlation between any two responses from the same school is about 0.798.\nThe effective sample size (number of independent pieces of information available for modeling) is closer to the number of schools \\((\\rho \\text{ close to 1})\\) than the number of observations \\((\\rho\\text{ close to 0})\\)"
  },
  {
    "objectID": "slides/09_longitudinal_ch9_o.html#unconditional-growth-model",
    "href": "slides/09_longitudinal_ch9_o.html#unconditional-growth-model",
    "title": "Modeling two-level longitudinal data",
    "section": "Unconditional growth model",
    "text": "Unconditional growth model\nA next step in the model building is the unconditional growth model, a model with Level One predictors but no Level Two predictors.\nLevel One: \\(Y_{ij} = a_i + b_iYear08_{ij} + \\epsilon_{ij}\\)\nLevel Two: - \\(a_i = \\alpha_0 + u_i\\) - \\(b_i = \\beta_0 + v_i\\) . . .\n. . .\n\nQuestions:\n\nWrite the composite model.\nWhat can we learn from this model?\nFit the unconditional growth model."
  },
  {
    "objectID": "slides/09_longitudinal_ch9_o.html#pseudo-r2",
    "href": "slides/09_longitudinal_ch9_o.html#pseudo-r2",
    "title": "Modeling two-level longitudinal data",
    "section": "Pseudo \\(R^2\\)",
    "text": "Pseudo \\(R^2\\)\nWe can use Pseudo R2 to explain changes in variance components between two models\n\nNote: This should only be used when the definition of the variance component is the same between the two models\n\n. . .\n\\[\\text{Pseudo }R^2 = \\frac{\\hat{\\sigma}^2(\\text{Model 1})  -  \\hat{\\sigma}^2(\\text{Model 2})}{\\hat{\\sigma}^2(\\text{Model 1})}\\]\n. . .\nQuestion: Calculate the \\(\\text{Pseudo }R^2\\) to estimate the change of within school variance between the unconditional means and unconditional growth models."
  },
  {
    "objectID": "slides/09_longitudinal_ch9_o.html#model-with-school-level-covariates",
    "href": "slides/09_longitudinal_ch9_o.html#model-with-school-level-covariates",
    "title": "Modeling two-level longitudinal data",
    "section": "Model with school-level covariates",
    "text": "Model with school-level covariates\nFit a model with school-level covariates that takes the following form:\nLevel One\n\\[\\begin{equation*}\nY_{ij}= a_{i} + b_{i}Year08_{ij} + \\epsilon_{ij}\n\\end{equation*}\\]\nLevel Two\n\\[\\begin{align*}\na_{i} & = \\alpha_{0} + \\alpha_{1}Charter_i + \\alpha_{2}urban_i + \\alpha_{3}schpctfree_i + u_{i} \\\\\nb_{i} & = \\beta_{0} + \\beta_{1}Charter_i + \\beta_{2}urban_i + v_{i}\n\\end{align*}\\]"
  },
  {
    "objectID": "slides/09_longitudinal_ch9_o.html#questions-1",
    "href": "slides/09_longitudinal_ch9_o.html#questions-1",
    "title": "Modeling two-level longitudinal data",
    "section": "Questions",
    "text": "Questions\n\nWrite out the composite model.\nFit the model in R.\nUse the model to describe how the average math scores differed between charter and non-charter schools."
  },
  {
    "objectID": "slides/09_longitudinal_ch9_o.html#consider-a-simpler-model",
    "href": "slides/09_longitudinal_ch9_o.html#consider-a-simpler-model",
    "title": "Modeling two-level longitudinal data",
    "section": "Consider a simpler model",
    "text": "Consider a simpler model\nWould a model without the effects \\(v_i\\) and \\(\\rho_{uv}\\) be preferable?\n. . .\nLevel One\n\\[\\begin{equation*}\nY_{ij}= a_{i} + b_{i}Year08_{ij} + \\epsilon_{ij}\n\\end{equation*}\\]\nLevel Two\n\\[\\begin{align*}\na_{i} & = \\alpha_{0} + \\alpha_{1}Charter_i + \\alpha_{2}urban_i + \\alpha_{3}schpctfree_i + u_{i} \\\\\nb_{i} & = \\beta_{0} + \\beta_{1}Charter_i + \\beta_{2}urban_i\n\\end{align*}\\]\n\n. . .\nIn this model, the effect of year is the same for all schools with a given combination of Charter and urban"
  },
  {
    "objectID": "slides/09_longitudinal_ch9_o.html#compare-two-models",
    "href": "slides/09_longitudinal_ch9_o.html#compare-two-models",
    "title": "Modeling two-level longitudinal data",
    "section": "Compare two models",
    "text": "Compare two models\nFull model\n\\[\\begin{aligned}Y_{ij} = [&\\alpha_{0} + \\alpha_{1}Charter_i + \\alpha_{2}Urban_i+ \\alpha_{3}schpctfree_i \\\\ &+ \\beta_0Year08_{ij} + \\beta_1Charter_i:Year08_{ij} + \\beta_2Urban_i:Year_{ij}] \\\\&+ [u_i + v_iYear08_{ij} + \\epsilon_{ij}]\\end{aligned}\\]\nwhere\n\\[\\left[ \\begin{array}{c}\n            u_{i} \\\\ v_{i}\n          \\end{array}  \\right] \\sim N \\left( \\left[\n          \\begin{array}{c}\n            0 \\\\ 0\n          \\end{array} \\right], \\left[\n          \\begin{array}{cc}\n            \\sigma_{u}^{2} & \\sigma_{uv} \\\\\n            \\sigma_{uv} & \\sigma_{v}^{2}\n          \\end{array} \\right] \\right) \\hspace{2mm} \\text{ and }\\hspace{2mm} \\epsilon_{ij} \\sim N(0, \\sigma^2)\\]\n. . .\nEstimated fixed effects: \\(\\alpha_0, \\alpha_1, \\alpha_2, \\alpha_3, \\beta_0, \\beta_1, \\beta_2\\)\nVariance components to estimate: \\(\\sigma, \\sigma_u, \\sigma_v, \\rho_{uv}\\) (Note: \\(\\sigma_{uv} = \\rho_{uv}\\sigma_u\\sigma_v\\))"
  },
  {
    "objectID": "slides/09_longitudinal_ch9_o.html#compare-two-models-1",
    "href": "slides/09_longitudinal_ch9_o.html#compare-two-models-1",
    "title": "Modeling two-level longitudinal data",
    "section": "Compare two models",
    "text": "Compare two models\nNull Model (simplified variance structure)\n\\[\\begin{aligned}Y_{ij} = [&\\alpha_{0} + \\alpha_{1}Charter_i + \\alpha_{2}Urban_i+ \\alpha_{3}schpctfree_i \\\\ &+ \\beta_0Year08_{ij} + \\beta_1Charter_i:Year08_{ij} + \\beta_2Urban_i:Year_{ij}] \\\\&+ [u_i + \\epsilon_{ij}]\\end{aligned}\\]\nwhere\n\\[u_i \\sim N(0, \\sigma^2_u) \\hspace{2mm} \\text{ and }\\hspace{2mm} \\epsilon_{ij} \\sim N(0, \\sigma^2)\\]\n. . .\nEstimated fixed effects: \\(\\alpha_0, \\alpha_1, \\alpha_2, \\alpha_3, \\beta_0, \\beta_1, \\beta_2\\)\nVariance components to estimate: \\(\\sigma, \\sigma_u\\)"
  },
  {
    "objectID": "slides/09_longitudinal_ch9_o.html#full-and-reduced-models",
    "href": "slides/09_longitudinal_ch9_o.html#full-and-reduced-models",
    "title": "Modeling two-level longitudinal data",
    "section": "Full and reduced models",
    "text": "Full and reduced models\n\nfull_model &lt;- lmer(MathAvgScore ~ charter + urban + schPctfree + \n                     charter:year08  + urban:year08 + year08 + \n                     (year08|schoolid), REML = T, data = charter) #&lt;&lt;\n\n\nreduced_model &lt;-  lmer(MathAvgScore ~ charter + urban + schPctfree + \n                     charter:year08  + urban:year08 + year08 +\n                       (1 | schoolid), REML = T, data = charter) #&lt;&lt;\n\n. . .\nHypotheses\n\\[\\begin{aligned}&H_0: \\sigma_v = \\rho_{uv} = 0\\\\\n&H_a: \\text{at least one of the parameters is not equal to 0}\\end{aligned}\\]\nNote: \\(\\rho_{uv} \\neq 0 \\Rightarrow \\sigma_v \\neq 0\\)"
  },
  {
    "objectID": "slides/09_longitudinal_ch9_o.html#issues-with-the-drop-in-deviance-test",
    "href": "slides/09_longitudinal_ch9_o.html#issues-with-the-drop-in-deviance-test",
    "title": "Modeling two-level longitudinal data",
    "section": "Issues with the drop-in-deviance test",
    "text": "Issues with the drop-in-deviance test\n\nanova(full_model, reduced_model, test = \"Chisq\") |&gt;\n  kable(digits = 3)\n\nrefitting model(s) with ML (instead of REML)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nnpar\nAIC\nBIC\nlogLik\ndeviance\nChisq\nDf\nPr(&gt;Chisq)\n\n\n\n\nreduced_model\n9\n9952.992\n10002.11\n-4967.496\n9934.992\nNA\nNA\nNA\n\n\nfull_model\n11\n9953.793\n10013.83\n-4965.897\n9931.793\n3.199\n2\n0.202\n\n\n\n\n\n\n\\(\\chi^2\\) test conservative, i.e., the p-values are larger than they should be, when testing random effects at the boundary ( e.g., \\(\\sigma_v^2 = 0\\)) or those with bounded ranges (e.g., \\(\\rho_{uv}\\) )\nIf you observe small p-values, you can feel relatively certain the tested effects are statistically significant\nUse bootstrap methods to obtain more accurate p-values"
  },
  {
    "objectID": "slides/09_longitudinal_ch9_o.html#parametric-bootstrapping",
    "href": "slides/09_longitudinal_ch9_o.html#parametric-bootstrapping",
    "title": "Modeling two-level longitudinal data",
    "section": "Parametric bootstrapping",
    "text": "Parametric bootstrapping\n\nBootstrapping is from the phrase “pulling oneself up by one’s bootstraps”\nAccomplishing a difficult task without any outside help\nTask: conduct inference for model parameters (fixed and random effects) using only the sample data"
  },
  {
    "objectID": "slides/09_longitudinal_ch9_o.html#parametric-bootstrapping-for-likelihood-ratio-test",
    "href": "slides/09_longitudinal_ch9_o.html#parametric-bootstrapping-for-likelihood-ratio-test",
    "title": "Modeling two-level longitudinal data",
    "section": "Parametric bootstrapping for likelihood ratio test",
    "text": "Parametric bootstrapping for likelihood ratio test\n1️⃣ Fit the null (reduced) model to obtain the fixed effects and variance components (parametric part).\n2️⃣ Use the estimated fixed effects and variance components to generate a new set of response values with the same sample size and associated covariates for each observation as the original data (bootstrap part).\n3️⃣ Fit the full and null models to the newly generated data.\n4️⃣ Compute the likelihood test statistic comparing the models from the previous step.\n5️⃣ Repeat steps 2 - 4 many times (~ 1000)."
  },
  {
    "objectID": "slides/09_longitudinal_ch9_o.html#parametric-bootstrapping-for-likelihood-ratio-test-1",
    "href": "slides/09_longitudinal_ch9_o.html#parametric-bootstrapping-for-likelihood-ratio-test-1",
    "title": "Modeling two-level longitudinal data",
    "section": "Parametric bootstrapping for likelihood ratio test",
    "text": "Parametric bootstrapping for likelihood ratio test\n\n6️⃣ Create a histogram of the likelihood ratio statistics to get the distribution of likelihood ratio statistic under the null hypothesis.\n7️⃣ Get the p-value by calculating the proportion of bootstrapped test statistics greater than the observed statistic.\nLet’s calculate the bootstrapped p-value for the likelihood ratio test statistic testing whether \\(v_i\\) and \\(\\rho_{uv}\\) can be removed from the model."
  },
  {
    "objectID": "slides/09_longitudinal_ch9_o.html#lrt-using-chi2-and-parametric-bootstrap",
    "href": "slides/09_longitudinal_ch9_o.html#lrt-using-chi2-and-parametric-bootstrap",
    "title": "Modeling two-level longitudinal data",
    "section": "LRT using \\(\\chi^2\\) and parametric bootstrap",
    "text": "LRT using \\(\\chi^2\\) and parametric bootstrap\nLikelihood ratio test using \\(\\chi^2\\) distribution\n\n\nrefitting model(s) with ML (instead of REML)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nnpar\nAIC\nBIC\nlogLik\ndeviance\nChisq\nDf\nPr(&gt;Chisq)\n\n\n\n\nreduced_model\n9\n9952.992\n10002.11\n-4967.496\n9934.992\nNA\nNA\nNA\n\n\nfull_model\n11\n9953.793\n10013.83\n-4965.897\n9931.793\n3.199\n2\n0.202\n\n\n\n\n\nLikelihood ratio test using parametric bootstrap\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nterm \nnpar \nAIC \nBIC \nlogLik \ndeviance \nstatistic \ndf \nPr_boot..Chisq. \n\n\n\n\nm0\n9\n9952.992\n10002.11\n-4967.496\n9934.992\nNA\nNA\nNA\n\n\nmA\n11\n9953.793\n10013.83\n-4965.897\n9931.793\n3.199347\n2\n0.144"
  },
  {
    "objectID": "slides/09_longitudinal_ch9_o.html#inference-for-fixed-effects-12",
    "href": "slides/09_longitudinal_ch9_o.html#inference-for-fixed-effects-12",
    "title": "Modeling two-level longitudinal data",
    "section": "Inference for fixed effects (1/2)",
    "text": "Inference for fixed effects (1/2)\n\nThe output for multilevel models do not contain p-values for the coefficients of fixed effects\nThe exact distribution of the test statistic under the null hypothesis (no fixed effect) is unknown, because the exact degrees of freedom are unknown\n\nFinding suitable approximations is an area of ongoing research\n\nIn the tidy function, with its variant from broom.mixed, you can ask for conf.int = T to get confidence intervals.\nWe can look up how that works in the packages vignette. Let’s look up the package!"
  },
  {
    "objectID": "slides/09_longitudinal_ch9_o.html#inference-for-fixed-effects-22",
    "href": "slides/09_longitudinal_ch9_o.html#inference-for-fixed-effects-22",
    "title": "Modeling two-level longitudinal data",
    "section": "Inference for fixed effects (2/2)",
    "text": "Inference for fixed effects (2/2)\n\nWe can use likelihood ratio test with an approximate \\(\\chi^2\\) distribution to test these effects, since we’re not testing on the boundary and fixed effects do not have limited ranges\n\nSome research suggests the p-values are too low but approximations are generally pretty good\nCan also calculate the p-values using parametric bootstrap approach\n\nLet’s test whether schPctFree should be included in the current model using likelihood ratio test with the \\(\\chi^2\\) distribution and the parametric bootstrap."
  },
  {
    "objectID": "slides/09_longitudinal_ch9_o.html#methods-to-conduct-inference-for-individual-coefficients",
    "href": "slides/09_longitudinal_ch9_o.html#methods-to-conduct-inference-for-individual-coefficients",
    "title": "Modeling two-level longitudinal data",
    "section": "Methods to conduct inference for individual coefficients",
    "text": "Methods to conduct inference for individual coefficients\n\nUse the t-value \\(\\big(\\frac{estimate}{std.error}\\big)\\) in the model output\n\nGeneral rule: Coefficients with |t-value| &gt; 2 considered to be statistically significant, i.e., different from 0\n\nThe confidence intervals given by tidy are not proven to be good\n\nWe better use multiple approaches and see if they agree.\n\nCalculate confidence intervals using nonparametric bootstrapping"
  },
  {
    "objectID": "slides/09_longitudinal_ch9_o.html#nonparametric-bootstrapping",
    "href": "slides/09_longitudinal_ch9_o.html#nonparametric-bootstrapping",
    "title": "Modeling two-level longitudinal data",
    "section": "Nonparametric bootstrapping",
    "text": "Nonparametric bootstrapping\n1️⃣   Take a sample, with replacement, of size \\(n\\) (the size of the original data) (called case resampling).\n2️⃣   Fit the model to obtain estimates of the coefficients.\n3️⃣   Repeat steps 1 - 2 many times (~ 1000) to obtain the bootstrap distribution.\n4️⃣   Get the coefficients for the 95% confidence interval by taking the middle 95% of the bootstrap distribution."
  },
  {
    "objectID": "slides/09_longitudinal_ch9_o.html#bootstrapped-ci-for-coefficients",
    "href": "slides/09_longitudinal_ch9_o.html#bootstrapped-ci-for-coefficients",
    "title": "Modeling two-level longitudinal data",
    "section": "Bootstrapped CI for coefficients",
    "text": "Bootstrapped CI for coefficients\n\nconfint(reduced_model, method = \"boot\", level = 0.95, oldNames = F) |&gt;\n  kable(digits = 3)\n\nComputing bootstrap confidence intervals ...\n\n\n\n\n\n\n2.5 %\n97.5 %\n\n\n\n\nsd_(Intercept)|schoolid\n4.264\n4.829\n\n\nsigma\n2.870\n3.110\n\n\n(Intercept)\n659.315\n661.415\n\n\ncharter\n-4.374\n-1.395\n\n\nurban\n-1.958\n-0.218\n\n\nschPctfree\n-19.523\n-16.289\n\n\nyear08\n1.259\n1.784\n\n\ncharter:year08\n0.397\n1.639\n\n\nurban:year08\n-0.900\n-0.193"
  },
  {
    "objectID": "slides/09_longitudinal_ch9_o.html#summary-model-comparisons",
    "href": "slides/09_longitudinal_ch9_o.html#summary-model-comparisons",
    "title": "Modeling two-level longitudinal data",
    "section": "Summary: Model comparisons",
    "text": "Summary: Model comparisons\nMethods to compare models with different fixed effects\n\nLikelihood ratio tests based on \\(\\chi^2\\) distribution\nLikelihood ratio test based on parametric bootstrapped p-values\nAIC or BIC\n\n. . .\nMethods to compare models with different variance components\n\nLikelihood ratio test based on parametric bootstrapped p-values\nAIC or BIC"
  },
  {
    "objectID": "slides/09_longitudinal_ch9_o.html#summary-understanding-the-model",
    "href": "slides/09_longitudinal_ch9_o.html#summary-understanding-the-model",
    "title": "Modeling two-level longitudinal data",
    "section": "Summary: Understanding the model",
    "text": "Summary: Understanding the model\nMethods to understand individual coefficients\n\nLikelihood ratio tests\nBootstrap confidence intervals\nPseudo \\(R^2\\) for variance components (if meaning is unchanged between models)\n\n. . .\nMethods to understand data structure\n\nCalculate intraclass correlation coefficient using unconditional means model"
  },
  {
    "objectID": "slides/09_longitudinal_ch9_o.html#acknowledgements",
    "href": "slides/09_longitudinal_ch9_o.html#acknowledgements",
    "title": "Modeling two-level longitudinal data",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThe content in the slides is from\n\nBMLR: Chapter 9 - Two-level Longitudinal Data\n\nSections 9.1 - 9.6\n\nHierarchical Linear Modeling with Maximum Likelihood, Restricted Maximum Likelihood, and Fully Bayesian Estimation by Peter Boedeker\nApplied longitudinal data analysis: Modeling change and event occurrence. by J.D. Singer and J.B. Willett\n\nOnline copy available through Duke library\n\nExtending the linear model with R. by Julian Faraway\n\nOnline copy available through Duke library\n\nInitial versions of the slides are by Dr. Maria Tackett, Duke University"
  },
  {
    "objectID": "slides/07_correlated_data_ch7.html#setup",
    "href": "slides/07_correlated_data_ch7.html#setup",
    "title": "Correlated Data",
    "section": "Setup",
    "text": "Setup\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(GGally)\nlibrary(knitr)\nlibrary(patchwork)\nlibrary(viridis)\nlibrary(ggfortify)\nlibrary(gridExtra)\nlibrary(kableExtra)"
  },
  {
    "objectID": "slides/07_correlated_data_ch7.html#learning-goals",
    "href": "slides/07_correlated_data_ch7.html#learning-goals",
    "title": "Correlated Data",
    "section": "Learning goals",
    "text": "Learning goals\n\nRecognize a potential for correlation in a data set\nIdentify observational units at varying levels\nUnderstand issues correlated data may cause in modeling\nUnderstand how random effects models can be used to take correlation into account"
  },
  {
    "objectID": "slides/07_correlated_data_ch7.html#examples-of-correlated-data",
    "href": "slides/07_correlated_data_ch7.html#examples-of-correlated-data",
    "title": "Correlated Data",
    "section": "Examples of correlated data",
    "text": "Examples of correlated data\n\nIn an education study, scores for students from a particular teacher are typically more similar than scores of other students with a different teacher\nIn a study measuring depression indices weekly over a month, the four measures for the same patient tend to be more similar than depression indices from other patients\nIn political polling, opinions of members from the same household tend to be more similar than opinions of members from another household\n\n\nCorrelation among outcomes within the same group (teacher, patient, household) is called intraclass correlation"
  },
  {
    "objectID": "slides/07_correlated_data_ch7.html#multilevel-data",
    "href": "slides/07_correlated_data_ch7.html#multilevel-data",
    "title": "Correlated Data",
    "section": "Multilevel data",
    "text": "Multilevel data\n\nWe can think of correlated data as a multilevel structure\n\nPopulation elements are aggregated into groups\nThere are observational units and measurements at each level\n\nFor now we will focus on data with two levels:\n\nLevel one: Most basic level of observation\nLevel two: Groups formed from aggregated level-one observations\n\nExample: political polling\n\nLevel one: individual members of household\nLevel two: household"
  },
  {
    "objectID": "slides/07_correlated_data_ch7.html#two-types-of-effects",
    "href": "slides/07_correlated_data_ch7.html#two-types-of-effects",
    "title": "Correlated Data",
    "section": "Two types of effects",
    "text": "Two types of effects\n\nFixed effects: Effects that are of interest in the study\n\nCan think of these as effects whose interpretations would be included in a write up of the study\n\nRandom effects: Effects we’re not interested in studying but whose variability we want to understand\n\nCan think of these as effects whose interpretations would not necessarily be included in a write up of the study"
  },
  {
    "objectID": "slides/07_correlated_data_ch7.html#example",
    "href": "slides/07_correlated_data_ch7.html#example",
    "title": "Correlated Data",
    "section": "Example",
    "text": "Example\nResearchers are interested in understanding the effect social media has on opinions about a proposed economic plan. They randomly select 1000 households. They ask each adult in the household how many minutes they spend on social media daily and whether they support the proposed economic plan.\n\ndaily minutes on social media is the fixed effect\nhousehold is the random effect"
  },
  {
    "objectID": "slides/07_correlated_data_ch7.html#practice",
    "href": "slides/07_correlated_data_ch7.html#practice",
    "title": "Correlated Data",
    "section": "Practice",
    "text": "Practice\nResearchers conducted a randomized controlled study where patients were randomly assigned to either an anti-epileptic drug or a placebo. For each patient, the number of seizures at baseline was measured over a 2-week period. For four consecutive visits the number of seizures were determined over the past 2-week period. Patient age and sex along with visit number were recorded.\n\nQuestions\n\nWhat are the level one and level two observational units?\nWhat is the response variable and what is its type (normal, Poisson, etc.)?\nDescribe the within-group variation.\nWhat are the fixed effects? What are the random effects?\n\n\n\n\nEx. 1 from Section 7.10.1 in BMLR"
  },
  {
    "objectID": "slides/07_correlated_data_ch7.html#data-teratogen-and-rat-pups",
    "href": "slides/07_correlated_data_ch7.html#data-teratogen-and-rat-pups",
    "title": "Correlated Data",
    "section": "Data: Teratogen and rat pups",
    "text": "Data: Teratogen and rat pups\nToday’s data are simulated results of an experiment with 24 dams (mother rats) randomly divided into four groups that received different doses of teratogen, a substance that could potentially cause harm to developing fetuses. The four groups are\n\nHigh dose (3 mg)\nMedium dose (2 mg)\nLow dose (1 mg)\nNo dose (Control)\n\n\nEach dam produced 10 rat pups and the presence of a deformity was noted.\n\n\nGoal: Understand the association between teratogen exposure and the probability a pup is born with a deformity."
  },
  {
    "objectID": "slides/07_correlated_data_ch7.html#scenario-1-no-dose-effect-1",
    "href": "slides/07_correlated_data_ch7.html#scenario-1-no-dose-effect-1",
    "title": "Correlated Data",
    "section": "Scenario 1: No dose effect",
    "text": "Scenario 1: No dose effect\nAssume dose has no effect on, \\(p\\), the probability of a pup born with a deformity.\n\nScenario 1a.: \\(p = 0.5\\) for each dam\nScenario 1b.: \\(p \\sim Beta(0.5, 0.5)\\) (expected value = 0.5)"
  },
  {
    "objectID": "slides/07_correlated_data_ch7.html#scenario-1-no-dose-effect-2",
    "href": "slides/07_correlated_data_ch7.html#scenario-1-no-dose-effect-2",
    "title": "Correlated Data",
    "section": "Scenario 1: No dose effect",
    "text": "Scenario 1: No dose effect\n\nOuputCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntheoretical_pi &lt;- tibble(x = 1:250000,\n                         p1 = rbeta(x, 0.5, 0.5))\n\ntibble(x = 1:24, pi_1b) |&gt;\n  ggplot() +\n    geom_histogram(bins = 5, aes(x = pi_1b, y = ..density..),\n                   color = \"black\", fill = \"blue\", alpha = 0.2) + \n    coord_cartesian(xlim = c(0,1)) +\n    geom_density(data = theoretical_pi, aes(x = p1), \n                  linetype = 3, color = \"blue\", lwd = 2) +\n    geom_vline(xintercept = 0.5, color = \"red\", lwd = 2) +\n    labs(title = \"Probability of deformity\", \n         subtitle = \"Red = Scenario 1a, Blue dashed line  = Scenario 1b\", \n         x = \"Probability of Deformity\")\n\n\n\n\n\n\nFrom Figure 7.1 in BMLR"
  },
  {
    "objectID": "slides/07_correlated_data_ch7.html#questions",
    "href": "slides/07_correlated_data_ch7.html#questions",
    "title": "Correlated Data",
    "section": "Questions",
    "text": "Questions\n\nWould you expect the number of pups with a deformity for dams in Scenario 1a to follow a distribution similar to the binomial distribution with \\(n=10\\) and \\(p=0.5\\)? Why or why not?\nWould you expect the number of pups with a deformity for dams in Scenario 1b to follow a distribution similar to the binomial distribution with \\(n=10\\) and \\(p=0.5\\)? Why or why not?\nWhich scenario do you think is more realistic - Scenario 1a or 1b?"
  },
  {
    "objectID": "slides/07_correlated_data_ch7.html#investigating",
    "href": "slides/07_correlated_data_ch7.html#investigating",
    "title": "Correlated Data",
    "section": "Investigating",
    "text": "Investigating\n\nOutputCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmean_1a\nsd_1a\nmean_1b\nsd_1b\n\n\n\n\n5.166667\n1.493949\n5.666667\n4.103727\n\n\n\n\n\n\n\n\n\npi_1a &lt;- rep(0.5, 24)\ncount_1a &lt;- rbinom(24, 10, pi_1a)\n\npi_1b &lt;- rbeta(24,.5,.5)  \ncount_1b &lt;- rbinom(24, 10, pi_1b)  \n\n\nscenario_1 &lt;- \n  tibble(pi_1a, count_1a, pi_1b, count_1b) |&gt;\n  mutate(phat_1a = count_1a / 10, \n         phat_1b = count_1b / 10)\n\nhist_1a &lt;- ggplot(data = scenario_1, aes(x = count_1a)) + \n  geom_histogram(bins = 5, color = \"black\", fill = \"steelblue\") +\n  coord_cartesian(xlim = c(0, 10)) +\n  labs(title = \"Scenario 1a: Binomial, p = 0.5\",\n       x = \"Count of deformed pups per dam\")\n\nhist_1b &lt;- ggplot(data = scenario_1, aes(x = count_1b)) + \n  geom_histogram(bins = 5, color = \"black\", fill = \"steelblue\") +\n  coord_cartesian(xlim = c(0, 10)) +\n  labs(title = \"Scenario 1b: Binomial, p ~ Beta(0.5, 0.5)\",\n       x = \"Count of deformed pups per dam\")\n\nhist_1a / hist_1b\n\n\nscenario_1 |&gt; \n  summarise(mean_1a = mean(count_1a), sd_1a = sd(count_1a),\n            mean_1b = mean(count_1b), sd_1b = sd(count_1b) ) |&gt;\n  kable()\n\n\n\n\n\nLet’s take a look at a binomial and quasibinomial model for Scenarios 1a and 1b."
  },
  {
    "objectID": "slides/07_correlated_data_ch7.html#scenario-2-dose-effect-1",
    "href": "slides/07_correlated_data_ch7.html#scenario-2-dose-effect-1",
    "title": "Correlated Data",
    "section": "Scenario 2: Dose effect",
    "text": "Scenario 2: Dose effect\nNow we will consider the effect of the dose of teratogen on the probability of a pup born with a deformity. The 24 pups have been randomly divided into four groups:\n\nHigh dose (dose = 3)\nMedium dose (dose = 2)\nLow dose (dose = 1)\nNo dose (dose = 0)\n\n\nWe will assume the true relationship between \\(p\\) and dose is the following:\n\\[\\log\\Big(\\frac{p}{1-p}\\Big) = -2 + 1.33 ~ dose\\]"
  },
  {
    "objectID": "slides/07_correlated_data_ch7.html#scenario-2",
    "href": "slides/07_correlated_data_ch7.html#scenario-2",
    "title": "Correlated Data",
    "section": "Scenario 2",
    "text": "Scenario 2\nScenario 2a.\n\\[p = \\frac{e^{-2 + 1.33 ~ dose}}{1 + e^{-2 + 1.33 ~ dose}}\\]\n\nScenario 2b.:\n\\[p \\sim Beta\\Big(\\frac{2p}{(1-p)}, 2\\Big)\\]\nOn average, dams who receive dose \\(x\\) have the same probability of deformed pup as dams with dose \\(x\\) under Scenario 2a."
  },
  {
    "objectID": "slides/07_correlated_data_ch7.html#distributions-under-scenario-2",
    "href": "slides/07_correlated_data_ch7.html#distributions-under-scenario-2",
    "title": "Correlated Data",
    "section": "Distributions under Scenario 2",
    "text": "Distributions under Scenario 2\n\n\n\n\n\n\n\n\n\n\n\nReplicated from Figure 7.3 in BMLR"
  },
  {
    "objectID": "slides/07_correlated_data_ch7.html#summary-stats-under-scen.2",
    "href": "slides/07_correlated_data_ch7.html#summary-stats-under-scen.2",
    "title": "Correlated Data",
    "section": "Summary stats under Scen.2",
    "text": "Summary stats under Scen.2\n\nOutputCode\n\n\n\n\n\n\n\nmean_2a\nsd_2a\nmean_2b\nsd_2b\n\n\n\n\n4.791667\n3.202976\n4.666667\n3.583375\n\n\n\n\n\n\n\n\nSummary statistics of Scenario 2 by dose.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nScenario 2a\n\n\nScenario 2b\n\n\n\nDosage\nMean p\nSD p\nMean Count\nSD Count\nMean p\nSD p\nMean Count\nSD Count\n\n\n\n\n0\n0.119\n0\n1.333\n1.366\n0.061\n0.069\n0.500\n0.837\n\n\n1\n0.339\n0\n3.167\n1.835\n0.239\n0.208\n3.500\n2.881\n\n\n2\n0.661\n0\n5.833\n1.472\n0.615\n0.195\n5.833\n1.941\n\n\n3\n0.881\n0\n8.833\n1.169\n0.872\n0.079\n8.833\n1.169\n\n\n\n\n\n\n\n\n\n\nscenario_2 |&gt; \n  summarise(mean_2a = mean(count_2a), sd_2a = sd(count_2a),\n            mean_2b = mean(count_2b), sd_2b = sd(count_2b) )\n\n\nscenario2Tab &lt;- scenario_2 |&gt;\n                  group_by(dose) |&gt;\n                  summarise(mean_2a_pi = round(mean(pi_2a),3), sd_2a_pi = round(sd(pi_2a),3),\n                            mean_2a_cnt = round(mean(count_2a),3), sd_2a_cnt = round(sd(count_2a),3),\n                            mean_2b_pi = round(mean(pi_2b),3), sd_2b_pi = round(sd(pi_2b),3),\n                            mean_2b_cnt = round(mean(count_2b),3), sd_2b_cnt = round(sd(count_2b),3)) |&gt;\n                  as.data.frame()\ncolnames(scenario2Tab) &lt;- c(\"Dosage\",\"Mean p\", \"SD p\",\n    \"Mean Count\", \"SD Count\", \"Mean p\", \"SD p\",\n    \"Mean Count\", \"SD Count\")\nkable(scenario2Tab, booktabs = T, \n    caption=\"Summary statistics of Scenario 2 by dose.\") |&gt;\n    add_header_above(c(\" \" = 1, \"Scenario 2a\" = 4, \n                       \"Scenario 2b\" = 4)) |&gt;\n    kable_styling(latex_options = \"scale_down\") |&gt;\n    column_spec(c(4:5,8:9), width = \"1cm\")\n\n\n\n\n\n\nFrom Table 7.2 in BMLR"
  },
  {
    "objectID": "slides/07_correlated_data_ch7.html#questions-1",
    "href": "slides/07_correlated_data_ch7.html#questions-1",
    "title": "Correlated Data",
    "section": "Questions",
    "text": "Questions\n\nIn Scenario 2a, dams produced 4.79 deformed pups on average, with standard deviation 3.20. Scenario 2b saw an average of 4.67 with standard deviation 3.58. Why are comparisons by dose more meaningful than these overall comparisons?\nWe will use binomial and quasibinomial regression to model the relationship between dose and probability of pup born with a deformity. What can you say about the center and the width of the confidence intervals under Scenarios 2a and 2b?\n\n\n\nWhich will be similar and why?\n\nWhich will be different and how?"
  },
  {
    "objectID": "slides/07_correlated_data_ch7.html#scenario-2-estimated-odds-ratio",
    "href": "slides/07_correlated_data_ch7.html#scenario-2-estimated-odds-ratio",
    "title": "Correlated Data",
    "section": "Scenario 2: Estimated odds ratio",
    "text": "Scenario 2: Estimated odds ratio\nThe estimated effect of dose and the 95% CI from the binomial and quasibinomial models are below:\n\nScenario 2a\n\n\n\n\nOdds Ratio\n95% CI\n\n\n\n\nBinomial\n3.536\n(2.604, 4.958)\n\n\nQuasibinomial\n3.536\n(2.512, 5.186)\n\n\n\nScenario 2b\n\n\n\n\nOdds Ratio\n95% CI\n\n\n\n\nBinomial\n4.311\n(3.086, 6.271)\n\n\nQuasibinomial\n4.311\n(2.735, 7.352)"
  },
  {
    "objectID": "slides/07_correlated_data_ch7.html#questions-2",
    "href": "slides/07_correlated_data_ch7.html#questions-2",
    "title": "Correlated Data",
    "section": "Questions",
    "text": "Questions\n\nDescribe how the quasibinomial analysis of Scenario 2b differs from the binomial analysis of the same simulated data. Do confidence intervals contain the true model parameters? Is this what you expected? Why?\nWhy are differences between quasibinomial and binomial models of Scenario 2a less noticeable than the differences in Scenario 2b?"
  },
  {
    "objectID": "slides/07_correlated_data_ch7.html#summary",
    "href": "slides/07_correlated_data_ch7.html#summary",
    "title": "Correlated Data",
    "section": "Summary",
    "text": "Summary\n\nThe structure of the data set may imply correlation between observations.\nCorrelated observations provide less information than independent observations; we need to account for this reduction in information.\nFailing to account for this reduction could result in underestimating standard error, thus resulting in overstating significance and the precision of the estimates.\nWe showed how we can account for this by incorporating the dispersion parameter or a random effect."
  },
  {
    "objectID": "slides/07_correlated_data_ch7.html#acknowledgements",
    "href": "slides/07_correlated_data_ch7.html#acknowledgements",
    "title": "Correlated Data",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThese slides are based on content in\n\nBMLR: Chapter 7 - Logistic Regression\nInitial versions of the slides are by Dr. Maria Tackett, Duke University"
  },
  {
    "objectID": "slides/07_correlated_data_ch7_o.html",
    "href": "slides/07_correlated_data_ch7_o.html",
    "title": "Correlated Data",
    "section": "",
    "text": "library(tidyverse)\nlibrary(tidymodels)\nlibrary(GGally)\nlibrary(knitr)\nlibrary(patchwork)\nlibrary(viridis)\nlibrary(ggfortify)\nlibrary(gridExtra)\nlibrary(kableExtra)"
  },
  {
    "objectID": "slides/07_correlated_data_ch7_o.html#setup",
    "href": "slides/07_correlated_data_ch7_o.html#setup",
    "title": "Correlated Data",
    "section": "",
    "text": "library(tidyverse)\nlibrary(tidymodels)\nlibrary(GGally)\nlibrary(knitr)\nlibrary(patchwork)\nlibrary(viridis)\nlibrary(ggfortify)\nlibrary(gridExtra)\nlibrary(kableExtra)"
  },
  {
    "objectID": "slides/07_correlated_data_ch7_o.html#learning-goals",
    "href": "slides/07_correlated_data_ch7_o.html#learning-goals",
    "title": "Correlated Data",
    "section": "Learning goals",
    "text": "Learning goals\n\nRecognize a potential for correlation in a data set\nIdentify observational units at varying levels\nUnderstand issues correlated data may cause in modeling\nUnderstand how random effects models can be used to take correlation into account"
  },
  {
    "objectID": "slides/07_correlated_data_ch7_o.html#examples-of-correlated-data",
    "href": "slides/07_correlated_data_ch7_o.html#examples-of-correlated-data",
    "title": "Correlated Data",
    "section": "Examples of correlated data",
    "text": "Examples of correlated data\n\nIn an education study, scores for students from a particular teacher are typically more similar than scores of other students with a different teacher\nIn a study measuring depression indices weekly over a month, the four measures for the same patient tend to be more similar than depression indices from other patients\nIn political polling, opinions of members from the same household tend to be more similar than opinions of members from another household\n\n. . .\nCorrelation among outcomes within the same group (teacher, patient, household) is called intraclass correlation"
  },
  {
    "objectID": "slides/07_correlated_data_ch7_o.html#multilevel-data",
    "href": "slides/07_correlated_data_ch7_o.html#multilevel-data",
    "title": "Correlated Data",
    "section": "Multilevel data",
    "text": "Multilevel data\n\nWe can think of correlated data as a multilevel structure\n\nPopulation elements are aggregated into groups\nThere are observational units and measurements at each level\n\nFor now we will focus on data with two levels:\n\nLevel one: Most basic level of observation\nLevel two: Groups formed from aggregated level-one observations\n\nExample: political polling\n\nLevel one: individual members of household\nLevel two: household"
  },
  {
    "objectID": "slides/07_correlated_data_ch7_o.html#two-types-of-effects",
    "href": "slides/07_correlated_data_ch7_o.html#two-types-of-effects",
    "title": "Correlated Data",
    "section": "Two types of effects",
    "text": "Two types of effects\n\nFixed effects: Effects that are of interest in the study\n\nCan think of these as effects whose interpretations would be included in a write up of the study\n\nRandom effects: Effects we’re not interested in studying but whose variability we want to understand\n\nCan think of these as effects whose interpretations would not necessarily be included in a write up of the study"
  },
  {
    "objectID": "slides/07_correlated_data_ch7_o.html#example",
    "href": "slides/07_correlated_data_ch7_o.html#example",
    "title": "Correlated Data",
    "section": "Example",
    "text": "Example\nResearchers are interested in understanding the effect social media has on opinions about a proposed economic plan. They randomly select 1000 households. They ask each adult in the household how many minutes they spend on social media daily and whether they support the proposed economic plan.\n\ndaily minutes on social media is the fixed effect\nhousehold is the random effect"
  },
  {
    "objectID": "slides/07_correlated_data_ch7_o.html#practice",
    "href": "slides/07_correlated_data_ch7_o.html#practice",
    "title": "Correlated Data",
    "section": "Practice",
    "text": "Practice\nResearchers conducted a randomized controlled study where patients were randomly assigned to either an anti-epileptic drug or a placebo. For each patient, the number of seizures at baseline was measured over a 2-week period. For four consecutive visits the number of seizures were determined over the past 2-week period. Patient age and sex along with visit number were recorded.\n. . .\nQuestions\n\nWhat are the level one and level two observational units?\nWhat is the response variable and what is its type (normal, Poisson, etc.)?\nDescribe the within-group variation.\nWhat are the fixed effects? What are the random effects?\n\n\n\nEx. 1 from Section 7.10.1 in BMLR"
  },
  {
    "objectID": "slides/07_correlated_data_ch7_o.html#data-teratogen-and-rat-pups",
    "href": "slides/07_correlated_data_ch7_o.html#data-teratogen-and-rat-pups",
    "title": "Correlated Data",
    "section": "Data: Teratogen and rat pups",
    "text": "Data: Teratogen and rat pups\nToday’s data are simulated results of an experiment with 24 dams (mother rats) randomly divided into four groups that received different doses of teratogen, a substance that could potentially cause harm to developing fetuses. The four groups are\n\nHigh dose (3 mg)\nMedium dose (2 mg)\nLow dose (1 mg)\nNo dose (Control)\n\n. . .\nEach dam produced 10 rat pups and the presence of a deformity was noted.\n. . .\nGoal: Understand the association between teratogen exposure and the probability a pup is born with a deformity."
  },
  {
    "objectID": "slides/07_correlated_data_ch7_o.html#scenario-1-no-dose-effect-1",
    "href": "slides/07_correlated_data_ch7_o.html#scenario-1-no-dose-effect-1",
    "title": "Correlated Data",
    "section": "Scenario 1: No dose effect",
    "text": "Scenario 1: No dose effect\nAssume dose has no effect on, \\(p\\), the probability of a pup born with a deformity.\n\nScenario 1a.: \\(p = 0.5\\) for each dam\nScenario 1b.: \\(p \\sim Beta(0.5, 0.5)\\) (expected value = 0.5)\n\n. . ."
  },
  {
    "objectID": "slides/07_correlated_data_ch7_o.html#scenario-1-no-dose-effect-2",
    "href": "slides/07_correlated_data_ch7_o.html#scenario-1-no-dose-effect-2",
    "title": "Correlated Data",
    "section": "Scenario 1: No dose effect",
    "text": "Scenario 1: No dose effect\n\nOuputCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntheoretical_pi &lt;- tibble(x = 1:250000,\n                         p1 = rbeta(x, 0.5, 0.5))\n\ntibble(x = 1:24, pi_1b) |&gt;\n  ggplot() +\n    geom_histogram(bins = 5, aes(x = pi_1b, y = ..density..),\n                   color = \"black\", fill = \"blue\", alpha = 0.2) + \n    coord_cartesian(xlim = c(0,1)) +\n    geom_density(data = theoretical_pi, aes(x = p1), \n                  linetype = 3, color = \"blue\", lwd = 2) +\n    geom_vline(xintercept = 0.5, color = \"red\", lwd = 2) +\n    labs(title = \"Probability of deformity\", \n         subtitle = \"Red = Scenario 1a, Blue dashed line  = Scenario 1b\", \n         x = \"Probability of Deformity\")\n\n\n\n\n\n\nFrom Figure 7.1 in BMLR"
  },
  {
    "objectID": "slides/07_correlated_data_ch7_o.html#questions",
    "href": "slides/07_correlated_data_ch7_o.html#questions",
    "title": "Correlated Data",
    "section": "Questions",
    "text": "Questions\n\nWould you expect the number of pups with a deformity for dams in Scenario 1a to follow a distribution similar to the binomial distribution with \\(n=10\\) and \\(p=0.5\\)? Why or why not?\nWould you expect the number of pups with a deformity for dams in Scenario 1b to follow a distribution similar to the binomial distribution with \\(n=10\\) and \\(p=0.5\\)? Why or why not?\nWhich scenario do you think is more realistic - Scenario 1a or 1b?"
  },
  {
    "objectID": "slides/07_correlated_data_ch7_o.html#investigating",
    "href": "slides/07_correlated_data_ch7_o.html#investigating",
    "title": "Correlated Data",
    "section": "Investigating",
    "text": "Investigating\n\nOutputCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmean_1a\nsd_1a\nmean_1b\nsd_1b\n\n\n\n\n5.166667\n1.493949\n5.666667\n4.103727\n\n\n\n\n\n\n\n\n\n\npi_1a &lt;- rep(0.5, 24)\ncount_1a &lt;- rbinom(24, 10, pi_1a)\n\npi_1b &lt;- rbeta(24,.5,.5)  \ncount_1b &lt;- rbinom(24, 10, pi_1b)  \n\n\nscenario_1 &lt;- \n  tibble(pi_1a, count_1a, pi_1b, count_1b) |&gt;\n  mutate(phat_1a = count_1a / 10, \n         phat_1b = count_1b / 10)\n\nhist_1a &lt;- ggplot(data = scenario_1, aes(x = count_1a)) + \n  geom_histogram(bins = 5, color = \"black\", fill = \"steelblue\") +\n  coord_cartesian(xlim = c(0, 10)) +\n  labs(title = \"Scenario 1a: Binomial, p = 0.5\",\n       x = \"Count of deformed pups per dam\")\n\nhist_1b &lt;- ggplot(data = scenario_1, aes(x = count_1b)) + \n  geom_histogram(bins = 5, color = \"black\", fill = \"steelblue\") +\n  coord_cartesian(xlim = c(0, 10)) +\n  labs(title = \"Scenario 1b: Binomial, p ~ Beta(0.5, 0.5)\",\n       x = \"Count of deformed pups per dam\")\n\nhist_1a / hist_1b\n\n\nscenario_1 |&gt; \n  summarise(mean_1a = mean(count_1a), sd_1a = sd(count_1a),\n            mean_1b = mean(count_1b), sd_1b = sd(count_1b) ) |&gt;\n  kable()\n\n\n\n\n. . .\nLet’s take a look at a binomial and quasibinomial model for Scenarios 1a and 1b."
  },
  {
    "objectID": "slides/07_correlated_data_ch7_o.html#scenario-2-dose-effect-1",
    "href": "slides/07_correlated_data_ch7_o.html#scenario-2-dose-effect-1",
    "title": "Correlated Data",
    "section": "Scenario 2: Dose effect",
    "text": "Scenario 2: Dose effect\nNow we will consider the effect of the dose of teratogen on the probability of a pup born with a deformity. The 24 pups have been randomly divided into four groups:\n\nHigh dose (dose = 3)\nMedium dose (dose = 2)\nLow dose (dose = 1)\nNo dose (dose = 0)\n\n. . .\nWe will assume the true relationship between \\(p\\) and dose is the following:\n\\[\\log\\Big(\\frac{p}{1-p}\\Big) = -2 + 1.33 ~ dose\\]"
  },
  {
    "objectID": "slides/07_correlated_data_ch7_o.html#scenario-2",
    "href": "slides/07_correlated_data_ch7_o.html#scenario-2",
    "title": "Correlated Data",
    "section": "Scenario 2",
    "text": "Scenario 2\nScenario 2a.\n\\[p = \\frac{e^{-2 + 1.33 ~ dose}}{1 + e^{-2 + 1.33 ~ dose}}\\]\n. . .\nScenario 2b.:\n\\[p \\sim Beta\\Big(\\frac{2p}{(1-p)}, 2\\Big)\\]\nOn average, dams who receive dose \\(x\\) have the same probability of deformed pup as dams with dose \\(x\\) under Scenario 2a."
  },
  {
    "objectID": "slides/07_correlated_data_ch7_o.html#distributions-under-scenario-2",
    "href": "slides/07_correlated_data_ch7_o.html#distributions-under-scenario-2",
    "title": "Correlated Data",
    "section": "Distributions under Scenario 2",
    "text": "Distributions under Scenario 2\n\n\n\n\n\n\n\n\n\n\n\nReplicated from Figure 7.3 in BMLR"
  },
  {
    "objectID": "slides/07_correlated_data_ch7_o.html#summary-stats-under-scen.2",
    "href": "slides/07_correlated_data_ch7_o.html#summary-stats-under-scen.2",
    "title": "Correlated Data",
    "section": "Summary stats under Scen.2",
    "text": "Summary stats under Scen.2\n\nOutputCode\n\n\n\n\n\n\n\nmean_2a\nsd_2a\nmean_2b\nsd_2b\n\n\n\n\n4.791667\n3.202976\n4.666667\n3.583375\n\n\n\n\n\n\n\n\nSummary statistics of Scenario 2 by dose.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nScenario 2a\n\n\nScenario 2b\n\n\n\nDosage\nMean p\nSD p\nMean Count\nSD Count\nMean p\nSD p\nMean Count\nSD Count\n\n\n\n\n0\n0.119\n0\n1.333\n1.366\n0.061\n0.069\n0.500\n0.837\n\n\n1\n0.339\n0\n3.167\n1.835\n0.239\n0.208\n3.500\n2.881\n\n\n2\n0.661\n0\n5.833\n1.472\n0.615\n0.195\n5.833\n1.941\n\n\n3\n0.881\n0\n8.833\n1.169\n0.872\n0.079\n8.833\n1.169\n\n\n\n\n\n\n\n\n\n\nscenario_2 |&gt; \n  summarise(mean_2a = mean(count_2a), sd_2a = sd(count_2a),\n            mean_2b = mean(count_2b), sd_2b = sd(count_2b) )\n\n\nscenario2Tab &lt;- scenario_2 |&gt;\n                  group_by(dose) |&gt;\n                  summarise(mean_2a_pi = round(mean(pi_2a),3), sd_2a_pi = round(sd(pi_2a),3),\n                            mean_2a_cnt = round(mean(count_2a),3), sd_2a_cnt = round(sd(count_2a),3),\n                            mean_2b_pi = round(mean(pi_2b),3), sd_2b_pi = round(sd(pi_2b),3),\n                            mean_2b_cnt = round(mean(count_2b),3), sd_2b_cnt = round(sd(count_2b),3)) |&gt;\n                  as.data.frame()\ncolnames(scenario2Tab) &lt;- c(\"Dosage\",\"Mean p\", \"SD p\",\n    \"Mean Count\", \"SD Count\", \"Mean p\", \"SD p\",\n    \"Mean Count\", \"SD Count\")\nkable(scenario2Tab, booktabs = T, \n    caption=\"Summary statistics of Scenario 2 by dose.\") |&gt;\n    add_header_above(c(\" \" = 1, \"Scenario 2a\" = 4, \n                       \"Scenario 2b\" = 4)) |&gt;\n    kable_styling(latex_options = \"scale_down\") |&gt;\n    column_spec(c(4:5,8:9), width = \"1cm\")\n\n\n\n\n\n\nFrom Table 7.2 in BMLR"
  },
  {
    "objectID": "slides/07_correlated_data_ch7_o.html#questions-1",
    "href": "slides/07_correlated_data_ch7_o.html#questions-1",
    "title": "Correlated Data",
    "section": "Questions",
    "text": "Questions\n\nIn Scenario 2a, dams produced 4.79 deformed pups on average, with standard deviation 3.20. Scenario 2b saw an average of 4.67 with standard deviation 3.58. Why are comparisons by dose more meaningful than these overall comparisons?\nWe will use binomial and quasibinomial regression to model the relationship between dose and probability of pup born with a deformity. What can you say about the center and the width of the confidence intervals under Scenarios 2a and 2b?\n\n\n\nWhich will be similar and why?\n\nWhich will be different and how?"
  },
  {
    "objectID": "slides/07_correlated_data_ch7_o.html#scenario-2-estimated-odds-ratio",
    "href": "slides/07_correlated_data_ch7_o.html#scenario-2-estimated-odds-ratio",
    "title": "Correlated Data",
    "section": "Scenario 2: Estimated odds ratio",
    "text": "Scenario 2: Estimated odds ratio\nThe estimated effect of dose and the 95% CI from the binomial and quasibinomial models are below:\n. . .\nScenario 2a\n\n\n\n\nOdds Ratio\n95% CI\n\n\n\n\nBinomial\n3.536\n(2.604, 4.958)\n\n\nQuasibinomial\n3.536\n(2.512, 5.186)\n\n\n\nScenario 2b\n\n\n\n\nOdds Ratio\n95% CI\n\n\n\n\nBinomial\n4.311\n(3.086, 6.271)\n\n\nQuasibinomial\n4.311\n(2.735, 7.352)"
  },
  {
    "objectID": "slides/07_correlated_data_ch7_o.html#questions-2",
    "href": "slides/07_correlated_data_ch7_o.html#questions-2",
    "title": "Correlated Data",
    "section": "Questions",
    "text": "Questions\n\nDescribe how the quasibinomial analysis of Scenario 2b differs from the binomial analysis of the same simulated data. Do confidence intervals contain the true model parameters? Is this what you expected? Why?\nWhy are differences between quasibinomial and binomial models of Scenario 2a less noticeable than the differences in Scenario 2b?"
  },
  {
    "objectID": "slides/07_correlated_data_ch7_o.html#summary",
    "href": "slides/07_correlated_data_ch7_o.html#summary",
    "title": "Correlated Data",
    "section": "Summary",
    "text": "Summary\n\nThe structure of the data set may imply correlation between observations.\nCorrelated observations provide less information than independent observations; we need to account for this reduction in information.\nFailing to account for this reduction could result in underestimating standard error, thus resulting in overstating significance and the precision of the estimates.\nWe showed how we can account for this by incorporating the dispersion parameter or a random effect."
  },
  {
    "objectID": "slides/07_correlated_data_ch7_o.html#acknowledgements",
    "href": "slides/07_correlated_data_ch7_o.html#acknowledgements",
    "title": "Correlated Data",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThese slides are based on content in\n\nBMLR: Chapter 7 - Logistic Regression\nInitial versions of the slides are by Dr. Maria Tackett, Duke University"
  },
  {
    "objectID": "slides/05_glm_ch5.html#learning-goals",
    "href": "slides/05_glm_ch5.html#learning-goals",
    "title": "Generalized Linear Models (GLMs)",
    "section": "Learning goals",
    "text": "Learning goals\n\nIdentify the components common to all generalized linear models\nFind the canonical link based on the distribution of the response variable\nExplain how coefficients are estimated using iteratively reweighted least squares (IWLS)"
  },
  {
    "objectID": "slides/05_glm_ch5.html#many-models-one-family",
    "href": "slides/05_glm_ch5.html#many-models-one-family",
    "title": "Generalized Linear Models (GLMs)",
    "section": "Many models; one family",
    "text": "Many models; one family\nWe have studied models for a variety of response variables\n\nLeast squares (Normal)\nLogistic (Bernoulli, Binomial, Multinomial)\nLog-linear (Poisson, Negative Binomial)\n\n\nThese models are all examples of generalized linear models.\nGLMs have a similar structure for their likelihoods, MLEs, variances, so we can use a generalized approach to find the model estimates and associated uncertainty."
  },
  {
    "objectID": "slides/05_glm_ch5.html#components-of-a-glm",
    "href": "slides/05_glm_ch5.html#components-of-a-glm",
    "title": "Generalized Linear Models (GLMs)",
    "section": "Components of a GLM",
    "text": "Components of a GLM\nNelder and Wdderburn (1972) defines a broad class of models called generalized linear models that generalizes multiple linear regression. GLMs are characterized by three components:\n\n1️⃣ Response variable with parameter \\(\\theta\\) whose probability function can be written in exponential family form (random component)\n2️⃣ A linear combination of predictors, \\(\\eta = \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_p x_p\\) (systematic component)\n3️⃣ A link function \\(g(\\theta)\\) that connects \\(\\theta\\) to \\(\\eta\\)\n\n\n\nNelder, J. A., & Wedderburn, R. W. (1972). Generalized linear models. Journal of the Royal Statistical Society: Series A (General), 135(3), 370-384."
  },
  {
    "objectID": "slides/05_glm_ch5.html#exponential-family-form",
    "href": "slides/05_glm_ch5.html#exponential-family-form",
    "title": "Generalized Linear Models (GLMs)",
    "section": "Exponential family form",
    "text": "Exponential family form\nSuppose a probability (mass or density) function has a parameter \\(\\theta\\). It is said to have a one-parameter exponential family form if\n\n✅ The support (set of possible values) does not depend on \\(\\theta\\), and\n\n\n✅ The probability function can be written in the following form\n\\(f(y;\\theta) = e^{[a(y)b(\\theta) + c(\\theta) + d(y)]}\\)\n\n\nUsing this form:\n\\[E(Y) = -\\frac{c'(\\theta)}{b'(\\theta)} \\hspace{20mm} Var(Y) = \\frac{b''(\\theta)c'(\\theta) - c''(\\theta)b'(\\theta)}{[b'(\\theta)]^3}\\]"
  },
  {
    "objectID": "slides/05_glm_ch5.html#poisson-in-exponential-family-form",
    "href": "slides/05_glm_ch5.html#poisson-in-exponential-family-form",
    "title": "Generalized Linear Models (GLMs)",
    "section": "Poisson in exponential family form",
    "text": "Poisson in exponential family form\n\\[P(Y = y) = \\frac{e^{-\\lambda}\\lambda^y}{y!} \\hspace{10mm} y = 0, 1, 2, \\ldots, \\infty\\]\n\n\\[\\begin{aligned}P(Y = y) &= e^{-\\lambda}e^{y\\log(\\lambda)}e^{-\\log(y!)}\\\\\n& = e^{y\\log(\\lambda) - \\lambda - \\log(y!)}\\end{aligned}\\]\n\n\nRecall the form: \\(f(y;\\theta) = e^{[a(y)b(\\theta) + c(\\theta) + d(y)]}\\), where the parameter \\(\\theta = \\lambda\\) for the Poisson distribution\n\n\n\n\\(a(y) = y\\)\n\\(b(\\lambda) = \\log(\\lambda)\\)\n\n\n\n\\(c(\\lambda) = -\\lambda\\)\n\\(d(y) = -\\log(y!)\\)"
  },
  {
    "objectID": "slides/05_glm_ch5.html#poisson-in-exponential-family-form-1",
    "href": "slides/05_glm_ch5.html#poisson-in-exponential-family-form-1",
    "title": "Generalized Linear Models (GLMs)",
    "section": "Poisson in exponential family form",
    "text": "Poisson in exponential family form\n\nThe support for the Poisson distribution is \\(y = 0, 1, 2, \\ldots, \\infty\\). This does not depend on the parameter \\(\\lambda\\).\nThe probability mass function can be written in the form \\(f(y;\\theta) = e^{[a(y)b(\\theta) + c(\\theta) + d(y)]}\\)\n\n\nThe Poisson distribution can be written in one-parameter exponential family form."
  },
  {
    "objectID": "slides/05_glm_ch5.html#canonical-link",
    "href": "slides/05_glm_ch5.html#canonical-link",
    "title": "Generalized Linear Models (GLMs)",
    "section": "Canonical link",
    "text": "Canonical link\nSuppose there is a response variable \\(Y\\) from a distribution with parameter \\(\\theta\\) and a set of predictors that can be written as a linear combination \\(\\eta = \\sum_{j=1}^{p}\\beta_jx_j = \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_p x_p\\)\n\n(There does not have to be an intercept but generally we also include \\(\\beta_0\\))\n\n\nA link function, \\(g()\\), is a monotonic and differentiable function that connects \\(\\theta\\) to \\(\\eta\\)\n\n\nThe canonical link is a link function such that \\(g(\\theta) = \\eta\\)\n\nWhen working with a member of the one-parameter exponential family, the canonical link is \\(b(\\theta)\\)"
  },
  {
    "objectID": "slides/05_glm_ch5.html#canonical-link-for-poisson",
    "href": "slides/05_glm_ch5.html#canonical-link-for-poisson",
    "title": "Generalized Linear Models (GLMs)",
    "section": "Canonical link for Poisson",
    "text": "Canonical link for Poisson\nRecall\n\\[P(Y = y) = e^{y\\log(\\lambda) - \\lambda - \\log(y!)}\\]\nthen the canonical link is \\(b(\\lambda) = \\log(\\lambda)\\)"
  },
  {
    "objectID": "slides/05_glm_ch5.html#glm-framework-poisson-response-variable",
    "href": "slides/05_glm_ch5.html#glm-framework-poisson-response-variable",
    "title": "Generalized Linear Models (GLMs)",
    "section": "GLM framework: Poisson response variable",
    "text": "GLM framework: Poisson response variable\n1️⃣ Response variable with parameter \\(\\theta\\) whose probability function can be written in exponential family form\n\\[P(Y = y) = e^{y\\log(\\lambda) - \\lambda - \\log(y!)}\\]\n\n2️⃣ A linear combination of predictors, \\(\\eta = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_p x_p\\)\n\n\n3️⃣ A function \\(g(\\lambda)\\) that connects \\(\\lambda\\) and \\(\\eta\\)\n\\[\\log(\\lambda) = \\eta =  \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_p x_p\\]"
  },
  {
    "objectID": "slides/05_glm_ch5.html#activity-identifying-canonical-link",
    "href": "slides/05_glm_ch5.html#activity-identifying-canonical-link",
    "title": "Generalized Linear Models (GLMs)",
    "section": "Activity: Identifying canonical link",
    "text": "Activity: Identifying canonical link\nFor the distribution\n\nDescribe an example of a setting where this random variable may be used.\nIdentify the parameter.\nWrite the pmf or pdf in one-parameter exponential form.\nIdentify the canonical link function\nOne person from each group: Write your response on the board."
  },
  {
    "objectID": "slides/05_glm_ch5.html#activity",
    "href": "slides/05_glm_ch5.html#activity",
    "title": "Generalized Linear Models (GLMs)",
    "section": "Activity",
    "text": "Activity\nDistributions\n\nBinary\nExponential\nNegative binomial (with fixed \\(r\\))\nGeometric\nNormal (with fixed \\(\\sigma\\))\n\n\nIf your group finishes early, try identifying the canonical link for the other distributions.\nSee BMLR - Section 3.6 for details on the distributions."
  },
  {
    "objectID": "slides/05_glm_ch5.html#data-noisy-miners",
    "href": "slides/05_glm_ch5.html#data-noisy-miners",
    "title": "Generalized Linear Models (GLMs)",
    "section": "Data: Noisy Miners",
    "text": "Data: Noisy Miners\nThe dataset nminer contains information about the number of noisy miners (small Australian bird) detected in two woodland patches within the Wimmera Plains of Victoria, Australia. It was obtained from the GLMsdata R package. We will use the following variables:\n\nMinerab: The number of noisy miners (abundance) observed in three 20 minute surveys\nEucs: The number of eucalyptus trees in each 2 hectare area (about 4.94 acres)"
  },
  {
    "objectID": "slides/05_glm_ch5.html#noisy-miner-model",
    "href": "slides/05_glm_ch5.html#noisy-miner-model",
    "title": "Generalized Linear Models (GLMs)",
    "section": "Noisy Miner Model",
    "text": "Noisy Miner Model\n\n\n\n\n\nEucs\nMinerab\n\n\n\n\n2\n0\n\n\n10\n0\n\n\n16\n3\n\n\n20\n2\n\n\n19\n8\n\n\n\n\n\n\nOur goal is to use a Poisson regression model to predict the number of noisy miners observed in three 20 minute surveys based on the number of eucalyptus trees.\n\\[\\log(\\lambda_{Minearab}) = \\beta_0 + \\beta_1 ~ Euc\\]\n\n\nWhat are the best estimates of \\(\\beta_0\\) and \\(\\beta_1\\)?"
  },
  {
    "objectID": "slides/05_glm_ch5.html#iteratively-reweighted-least-squares-iwls-1",
    "href": "slides/05_glm_ch5.html#iteratively-reweighted-least-squares-iwls-1",
    "title": "Generalized Linear Models (GLMs)",
    "section": "Iteratively reweighted least squares (IWLS)",
    "text": "Iteratively reweighted least squares (IWLS)\n\nThe estimates of \\(\\beta_0\\) and \\(\\beta_1\\) are found using maximum likelihood estimation.\nIteratively reweighted least-squares (IWLS) is used to find the MLEs\n\nNelder and Wedderburn (1972) show that under certain specifications of the weights and a modified response variable, the estimates found using IWLS are equivalent to the MLEs."
  },
  {
    "objectID": "slides/05_glm_ch5.html#iwls-set-up",
    "href": "slides/05_glm_ch5.html#iwls-set-up",
    "title": "Generalized Linear Models (GLMs)",
    "section": "IWLS Set up",
    "text": "IWLS Set up\nWorking response: Modified response variable at each step of the iteration.\n\\[z_i = g(\\theta) + g'(\\theta)(y_i - \\theta_i)\\]\nFor Poisson regression, this is\n\\[z_i = \\log(\\lambda) + \\frac{(y_i - \\lambda_i)}{\\lambda_i}\\]\n\nWorking Weights: Weights applied to the observations at each step of the iteration\n\\[W_i = \\frac{\\theta^2}{Var(Y)} \\hspace{5mm} \\Rightarrow \\hspace{5mm}  W_i = \\frac{\\lambda^2}{\\lambda} =  \\lambda \\text{ for Poisson regression}\\]"
  },
  {
    "objectID": "slides/05_glm_ch5.html#iwls-procedure",
    "href": "slides/05_glm_ch5.html#iwls-procedure",
    "title": "Generalized Linear Models (GLMs)",
    "section": "IWLS procedure",
    "text": "IWLS procedure\n\nFind initial starting values \\(\\hat{\\theta}_i\\).\nCalculate the working response values \\(z_i\\).\nCalculate the working weights \\(W_i\\).\nFind the coefficient estimates of the weighted least squares model.\n\n\n\\(z_i = \\beta_0 + \\beta_1 x \\hspace{5mm} \\text{ with weights }W_i\\)\nThe estimates \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) are the estimates for the model coefficients.\n\n\nUse \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) to calculate updated values of \\(\\hat{\\theta}_i\\) and repeat steps 2 - 4 until convergence."
  },
  {
    "objectID": "slides/05_glm_ch5.html#demo-in-ch5_iwls.r-in-server-class-files",
    "href": "slides/05_glm_ch5.html#demo-in-ch5_iwls.r-in-server-class-files",
    "title": "Generalized Linear Models (GLMs)",
    "section": "Demo in ch5_iwls.R in server class files",
    "text": "Demo in ch5_iwls.R in server class files"
  },
  {
    "objectID": "slides/05_glm_ch5.html#acknowledgements",
    "href": "slides/05_glm_ch5.html#acknowledgements",
    "title": "Generalized Linear Models (GLMs)",
    "section": "Acknowledgements",
    "text": "Acknowledgements\n\nThese slides are based on content in BMLR: Chapter 4\nInitial versions of the slides are by Dr. Maria Tackett, Duke University\nBMLR: Chapter 5 - Generalized Linear Models: A Unifying Theory\nNelder, J. A., & Wedderburn, R. W. (1972). Generalized linear models. Journal of the Royal Statistical Society: Series A (General), 135(3), 370-384.\nGeneralized Linear Models with Examples in R\n\nChapter 5 - Generalized Linear Models: Structure\nChapter 6 - Generalized Linear Models: Estimation"
  },
  {
    "objectID": "slides/05_glm_ch5_o.html",
    "href": "slides/05_glm_ch5_o.html",
    "title": "Generalized Linear Models (GLMs)",
    "section": "",
    "text": "Identify the components common to all generalized linear models\nFind the canonical link based on the distribution of the response variable\nExplain how coefficients are estimated using iteratively reweighted least squares (IWLS)"
  },
  {
    "objectID": "slides/05_glm_ch5_o.html#learning-goals",
    "href": "slides/05_glm_ch5_o.html#learning-goals",
    "title": "Generalized Linear Models (GLMs)",
    "section": "",
    "text": "Identify the components common to all generalized linear models\nFind the canonical link based on the distribution of the response variable\nExplain how coefficients are estimated using iteratively reweighted least squares (IWLS)"
  },
  {
    "objectID": "slides/05_glm_ch5_o.html#many-models-one-family",
    "href": "slides/05_glm_ch5_o.html#many-models-one-family",
    "title": "Generalized Linear Models (GLMs)",
    "section": "Many models; one family",
    "text": "Many models; one family\nWe have studied models for a variety of response variables\n\nLeast squares (Normal)\nLogistic (Bernoulli, Binomial, Multinomial)\nLog-linear (Poisson, Negative Binomial)\n\n. . .\nThese models are all examples of generalized linear models.\nGLMs have a similar structure for their likelihoods, MLEs, variances, so we can use a generalized approach to find the model estimates and associated uncertainty."
  },
  {
    "objectID": "slides/05_glm_ch5_o.html#components-of-a-glm",
    "href": "slides/05_glm_ch5_o.html#components-of-a-glm",
    "title": "Generalized Linear Models (GLMs)",
    "section": "Components of a GLM",
    "text": "Components of a GLM\nNelder and Wdderburn (1972) defines a broad class of models called generalized linear models that generalizes multiple linear regression. GLMs are characterized by three components:\n. . .\n1️⃣ Response variable with parameter \\(\\theta\\) whose probability function can be written in exponential family form (random component)\n2️⃣ A linear combination of predictors, \\(\\eta = \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_p x_p\\) (systematic component)\n3️⃣ A link function \\(g(\\theta)\\) that connects \\(\\theta\\) to \\(\\eta\\)\n\n\nNelder, J. A., & Wedderburn, R. W. (1972). Generalized linear models. Journal of the Royal Statistical Society: Series A (General), 135(3), 370-384."
  },
  {
    "objectID": "slides/05_glm_ch5_o.html#exponential-family-form",
    "href": "slides/05_glm_ch5_o.html#exponential-family-form",
    "title": "Generalized Linear Models (GLMs)",
    "section": "Exponential family form",
    "text": "Exponential family form\nSuppose a probability (mass or density) function has a parameter \\(\\theta\\). It is said to have a one-parameter exponential family form if\n. . .\n✅ The support (set of possible values) does not depend on \\(\\theta\\), and\n. . .\n✅ The probability function can be written in the following form\n\\(f(y;\\theta) = e^{[a(y)b(\\theta) + c(\\theta) + d(y)]}\\)\n. . .\nUsing this form:\n\\[E(Y) = -\\frac{c'(\\theta)}{b'(\\theta)} \\hspace{20mm} Var(Y) = \\frac{b''(\\theta)c'(\\theta) - c''(\\theta)b'(\\theta)}{[b'(\\theta)]^3}\\]"
  },
  {
    "objectID": "slides/05_glm_ch5_o.html#poisson-in-exponential-family-form",
    "href": "slides/05_glm_ch5_o.html#poisson-in-exponential-family-form",
    "title": "Generalized Linear Models (GLMs)",
    "section": "Poisson in exponential family form",
    "text": "Poisson in exponential family form\n\\[P(Y = y) = \\frac{e^{-\\lambda}\\lambda^y}{y!} \\hspace{10mm} y = 0, 1, 2, \\ldots, \\infty\\]\n. . .\n\\[\\begin{aligned}P(Y = y) &= e^{-\\lambda}e^{y\\log(\\lambda)}e^{-\\log(y!)}\\\\\n& = e^{y\\log(\\lambda) - \\lambda - \\log(y!)}\\end{aligned}\\]\n. . .\nRecall the form: \\(f(y;\\theta) = e^{[a(y)b(\\theta) + c(\\theta) + d(y)]}\\), where the parameter \\(\\theta = \\lambda\\) for the Poisson distribution\n\n\n\n\\(a(y) = y\\)\n\\(b(\\lambda) = \\log(\\lambda)\\)\n\n\n\n\\(c(\\lambda) = -\\lambda\\)\n\\(d(y) = -\\log(y!)\\)"
  },
  {
    "objectID": "slides/05_glm_ch5_o.html#poisson-in-exponential-family-form-1",
    "href": "slides/05_glm_ch5_o.html#poisson-in-exponential-family-form-1",
    "title": "Generalized Linear Models (GLMs)",
    "section": "Poisson in exponential family form",
    "text": "Poisson in exponential family form\n\nThe support for the Poisson distribution is \\(y = 0, 1, 2, \\ldots, \\infty\\). This does not depend on the parameter \\(\\lambda\\).\nThe probability mass function can be written in the form \\(f(y;\\theta) = e^{[a(y)b(\\theta) + c(\\theta) + d(y)]}\\)\n\n. . .\nThe Poisson distribution can be written in one-parameter exponential family form."
  },
  {
    "objectID": "slides/05_glm_ch5_o.html#canonical-link",
    "href": "slides/05_glm_ch5_o.html#canonical-link",
    "title": "Generalized Linear Models (GLMs)",
    "section": "Canonical link",
    "text": "Canonical link\nSuppose there is a response variable \\(Y\\) from a distribution with parameter \\(\\theta\\) and a set of predictors that can be written as a linear combination \\(\\eta = \\sum_{j=1}^{p}\\beta_jx_j = \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_p x_p\\)\n\n(There does not have to be an intercept but generally we also include \\(\\beta_0\\))\n\n. . .\nA link function, \\(g()\\), is a monotonic and differentiable function that connects \\(\\theta\\) to \\(\\eta\\)\n. . .\nThe canonical link is a link function such that \\(g(\\theta) = \\eta\\)\n\nWhen working with a member of the one-parameter exponential family, the canonical link is \\(b(\\theta)\\)"
  },
  {
    "objectID": "slides/05_glm_ch5_o.html#canonical-link-for-poisson",
    "href": "slides/05_glm_ch5_o.html#canonical-link-for-poisson",
    "title": "Generalized Linear Models (GLMs)",
    "section": "Canonical link for Poisson",
    "text": "Canonical link for Poisson\nRecall\n\\[P(Y = y) = e^{y\\log(\\lambda) - \\lambda - \\log(y!)}\\]\nthen the canonical link is \\(b(\\lambda) = \\log(\\lambda)\\)"
  },
  {
    "objectID": "slides/05_glm_ch5_o.html#glm-framework-poisson-response-variable",
    "href": "slides/05_glm_ch5_o.html#glm-framework-poisson-response-variable",
    "title": "Generalized Linear Models (GLMs)",
    "section": "GLM framework: Poisson response variable",
    "text": "GLM framework: Poisson response variable\n1️⃣ Response variable with parameter \\(\\theta\\) whose probability function can be written in exponential family form\n\\[P(Y = y) = e^{y\\log(\\lambda) - \\lambda - \\log(y!)}\\]\n. . .\n2️⃣ A linear combination of predictors, \\(\\eta = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_p x_p\\)\n. . .\n3️⃣ A function \\(g(\\lambda)\\) that connects \\(\\lambda\\) and \\(\\eta\\)\n\\[\\log(\\lambda) = \\eta =  \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_p x_p\\]"
  },
  {
    "objectID": "slides/05_glm_ch5_o.html#activity-identifying-canonical-link",
    "href": "slides/05_glm_ch5_o.html#activity-identifying-canonical-link",
    "title": "Generalized Linear Models (GLMs)",
    "section": "Activity: Identifying canonical link",
    "text": "Activity: Identifying canonical link\nFor the distribution\n\nDescribe an example of a setting where this random variable may be used.\nIdentify the parameter.\nWrite the pmf or pdf in one-parameter exponential form.\nIdentify the canonical link function\nOne person from each group: Write your response on the board."
  },
  {
    "objectID": "slides/05_glm_ch5_o.html#activity",
    "href": "slides/05_glm_ch5_o.html#activity",
    "title": "Generalized Linear Models (GLMs)",
    "section": "Activity",
    "text": "Activity\nDistributions\n\nBinary\nExponential\nNegative binomial (with fixed \\(r\\))\nGeometric\nNormal (with fixed \\(\\sigma\\))\n\n\nIf your group finishes early, try identifying the canonical link for the other distributions.\nSee BMLR - Section 3.6 for details on the distributions."
  },
  {
    "objectID": "slides/05_glm_ch5_o.html#data-noisy-miners",
    "href": "slides/05_glm_ch5_o.html#data-noisy-miners",
    "title": "Generalized Linear Models (GLMs)",
    "section": "Data: Noisy Miners",
    "text": "Data: Noisy Miners\nThe dataset nminer contains information about the number of noisy miners (small Australian bird) detected in two woodland patches within the Wimmera Plains of Victoria, Australia. It was obtained from the GLMsdata R package. We will use the following variables:\n\nMinerab: The number of noisy miners (abundance) observed in three 20 minute surveys\nEucs: The number of eucalyptus trees in each 2 hectare area (about 4.94 acres)"
  },
  {
    "objectID": "slides/05_glm_ch5_o.html#noisy-miner-model",
    "href": "slides/05_glm_ch5_o.html#noisy-miner-model",
    "title": "Generalized Linear Models (GLMs)",
    "section": "Noisy Miner Model",
    "text": "Noisy Miner Model\n\n\n\n\n\nEucs\nMinerab\n\n\n\n\n2\n0\n\n\n10\n0\n\n\n16\n3\n\n\n20\n2\n\n\n19\n8\n\n\n\n\n\n. . .\nOur goal is to use a Poisson regression model to predict the number of noisy miners observed in three 20 minute surveys based on the number of eucalyptus trees.\n\\[\\log(\\lambda_{Minearab}) = \\beta_0 + \\beta_1 ~ Euc\\]\n. . .\nWhat are the best estimates of \\(\\beta_0\\) and \\(\\beta_1\\)?"
  },
  {
    "objectID": "slides/05_glm_ch5_o.html#iteratively-reweighted-least-squares-iwls-1",
    "href": "slides/05_glm_ch5_o.html#iteratively-reweighted-least-squares-iwls-1",
    "title": "Generalized Linear Models (GLMs)",
    "section": "Iteratively reweighted least squares (IWLS)",
    "text": "Iteratively reweighted least squares (IWLS)\n\nThe estimates of \\(\\beta_0\\) and \\(\\beta_1\\) are found using maximum likelihood estimation.\nIteratively reweighted least-squares (IWLS) is used to find the MLEs\n\nNelder and Wedderburn (1972) show that under certain specifications of the weights and a modified response variable, the estimates found using IWLS are equivalent to the MLEs."
  },
  {
    "objectID": "slides/05_glm_ch5_o.html#iwls-set-up",
    "href": "slides/05_glm_ch5_o.html#iwls-set-up",
    "title": "Generalized Linear Models (GLMs)",
    "section": "IWLS Set up",
    "text": "IWLS Set up\nWorking response: Modified response variable at each step of the iteration.\n\\[z_i = g(\\theta) + g'(\\theta)(y_i - \\theta_i)\\]\nFor Poisson regression, this is\n\\[z_i = \\log(\\lambda) + \\frac{(y_i - \\lambda_i)}{\\lambda_i}\\]\n. . .\nWorking Weights: Weights applied to the observations at each step of the iteration\n\\[W_i = \\frac{\\theta^2}{Var(Y)} \\hspace{5mm} \\Rightarrow \\hspace{5mm}  W_i = \\frac{\\lambda^2}{\\lambda} =  \\lambda \\text{ for Poisson regression}\\]"
  },
  {
    "objectID": "slides/05_glm_ch5_o.html#iwls-procedure",
    "href": "slides/05_glm_ch5_o.html#iwls-procedure",
    "title": "Generalized Linear Models (GLMs)",
    "section": "IWLS procedure",
    "text": "IWLS procedure\n\nFind initial starting values \\(\\hat{\\theta}_i\\).\nCalculate the working response values \\(z_i\\).\nCalculate the working weights \\(W_i\\).\nFind the coefficient estimates of the weighted least squares model.\n\n. . .\n\\(z_i = \\beta_0 + \\beta_1 x \\hspace{5mm} \\text{ with weights }W_i\\)\nThe estimates \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) are the estimates for the model coefficients.\n. . .\nUse \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) to calculate updated values of \\(\\hat{\\theta}_i\\) and repeat steps 2 - 4 until convergence.\n. . ."
  },
  {
    "objectID": "slides/05_glm_ch5_o.html#demo-in-ch5_iwls.r-in-server-class-files",
    "href": "slides/05_glm_ch5_o.html#demo-in-ch5_iwls.r-in-server-class-files",
    "title": "Generalized Linear Models (GLMs)",
    "section": "Demo in ch5_iwls.R in server class files",
    "text": "Demo in ch5_iwls.R in server class files"
  },
  {
    "objectID": "slides/05_glm_ch5_o.html#acknowledgements",
    "href": "slides/05_glm_ch5_o.html#acknowledgements",
    "title": "Generalized Linear Models (GLMs)",
    "section": "Acknowledgements",
    "text": "Acknowledgements\n\nThese slides are based on content in BMLR: Chapter 4\nInitial versions of the slides are by Dr. Maria Tackett, Duke University\nBMLR: Chapter 5 - Generalized Linear Models: A Unifying Theory\nNelder, J. A., & Wedderburn, R. W. (1972). Generalized linear models. Journal of the Royal Statistical Society: Series A (General), 135(3), 370-384.\nGeneralized Linear Models with Examples in R\n\nChapter 5 - Generalized Linear Models: Structure\nChapter 6 - Generalized Linear Models: Estimation"
  },
  {
    "objectID": "slides/03_distribution_ch3.html#learning-objectives",
    "href": "slides/03_distribution_ch3.html#learning-objectives",
    "title": "Distribution Theory",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nWrite definitions of non-normal random variables in the context of an application.\nIdentify possible values for each random variable.\nIdentify how changing values for a parameter affects the characteristics of the distribution.\nRecognize a form of the probability density function for each distribution.\nIdentify the mean and variance for each distribution.\nMatch the response for a study to a plausible random variable and provide reasons for ruling out other random variables.\nMatch a histogram of sample data to plausible distributions.\nCreate a mixture of distributions and evaluate the shape, mean, and variance."
  },
  {
    "objectID": "slides/03_distribution_ch3.html#libraries-needed-none-new",
    "href": "slides/03_distribution_ch3.html#libraries-needed-none-new",
    "title": "Distribution Theory",
    "section": "Libraries Needed (none new?)",
    "text": "Libraries Needed (none new?)\n\n# Packages required for Chapter 3\nlibrary(gridExtra)  \nlibrary(knitr) \nlibrary(kableExtra)\nlibrary(tidyverse)"
  },
  {
    "objectID": "slides/03_distribution_ch3.html#introduction",
    "href": "slides/03_distribution_ch3.html#introduction",
    "title": "Distribution Theory",
    "section": "Introduction",
    "text": "Introduction\n\nWhat if it is not plausible that a response is normally distributed?\nYou may want to construct a model to predict whether a prospective student will enroll at a school or model the lifetimes of patients following a particular surgery.\n\nIn the first case you have a binary response (enrolls (1) or does not enroll (0)), and in the second case you are likely to have very skewed data with many similar values and a few hardy souls with extremely long survival.\nThese responses are not expected to be normally distributed; other distributions will be needed to describe and model binary or lifetime data.\nNon-normal responses are encountered in a large number of situations. Luckily, there are quite a few possibilities for models."
  },
  {
    "objectID": "slides/03_distribution_ch3.html#discrete-random-variables-1",
    "href": "slides/03_distribution_ch3.html#discrete-random-variables-1",
    "title": "Distribution Theory",
    "section": "Discrete Random Variables",
    "text": "Discrete Random Variables\n\nA discrete random variable has a countable number of possible values\n\nEx: we may want to measure the number of people in a household\nEx: model the number of crimes committed on a college campus.\n\nWith discrete random variables, the associated probabilities can be calculated for each possible value using a probability mass function (pmf). \nA pmf is a function that calculates \\(P(Y=y)\\), given each variable’s parameters."
  },
  {
    "objectID": "slides/03_distribution_ch3.html#binary-random-variable",
    "href": "slides/03_distribution_ch3.html#binary-random-variable",
    "title": "Distribution Theory",
    "section": "Binary Random Variable",
    "text": "Binary Random Variable\n\nConsider the event of flipping a (possibly unfair) coin.\nIf the coin lands heads, let’s consider this a success and record \\(Y = 1\\).\nA series of these events is a Bernoulli process, independent trials that take on one of two values (e.g., 0 or 1).\nThese values are often referred to as a failure and a success\n\nthe probability of success is identical for each trial."
  },
  {
    "objectID": "slides/03_distribution_ch3.html#binary-random-variable-1",
    "href": "slides/03_distribution_ch3.html#binary-random-variable-1",
    "title": "Distribution Theory",
    "section": "Binary Random Variable",
    "text": "Binary Random Variable\n\nSuppose we only flip the coin once, so we only have one parameter, the probability of flipping heads, \\(p\\).\nIf we know this value, we can express \\(P(Y=1) = p\\) and \\(P(Y=0) = 1-p\\).\nIn general, if we have a Bernoulli process with only one trial, we have a binary distribution (also called a Bernoulli distribution) where\n\n\n\\[\\begin{equation*}\nP(Y = y) = p^y(1-p)^{1-y} \\quad \\textrm{for} \\quad y = 0, 1.\n\\end{equation*}\\]\n\nIf \\(Y \\sim \\textrm{Binary}(p)\\), then \\(Y\\) has mean \\(\\operatorname{E}(Y) = p\\) and standard deviation \\(\\operatorname{SD}(Y) = \\sqrt{p(1-p)}\\)."
  },
  {
    "objectID": "slides/03_distribution_ch3.html#example-1",
    "href": "slides/03_distribution_ch3.html#example-1",
    "title": "Distribution Theory",
    "section": "Example 1",
    "text": "Example 1\n\nYour playlist of 200 songs has 5 which you cannot stand. What is the probability that when you hit shuffle, a song you tolerate comes on?\nWe want to understand the example and be able to translate that into notation/model it with a distribution\nAssuming all songs have equal odds of playing, we can calculate \\(p = \\frac{200-5}{200} = 0.975\\)\n\nso there is a 97.5% chance of a song you tolerate playing, since \\(P(Y=1)=.975^1*(1-.975)^0\\)."
  },
  {
    "objectID": "slides/03_distribution_ch3.html#binomial-random-variable-12",
    "href": "slides/03_distribution_ch3.html#binomial-random-variable-12",
    "title": "Distribution Theory",
    "section": "Binomial Random Variable (1/2)",
    "text": "Binomial Random Variable (1/2)\n\nDoes anybody know how this relates to a Bernoulli random variable?\nWe can extend our knowledge of binary random variables.\nSuppose we flipped an unfair coin \\(n\\) times and recorded \\(Y\\), the number of heads after \\(n\\) flips.\nIf we consider a case where \\(p = 0.25\\) and \\(n = 4\\), then here \\(P(Y=0)\\) represents the probability of no successes in 4 trials\n\n4 consecutive failures.\nThe probability of 4 consecutive failures is \\(P(Y = 0) = P(TTTT) = (1-p)^4 = 0.75^4\\)."
  },
  {
    "objectID": "slides/03_distribution_ch3.html#binomial-random-variable-22",
    "href": "slides/03_distribution_ch3.html#binomial-random-variable-22",
    "title": "Distribution Theory",
    "section": "Binomial Random Variable (2/2)",
    "text": "Binomial Random Variable (2/2)\n\nNext we consider \\(P(Y = 1)\\), and are interested in the probability of exactly 1 success anywhere among the 4 trials.\nHow many different ways can we get exactly 1 success in the four trials?\nTo find the probability of this what else do we need to count?\nWhat is the probability?\nThere are \\(\\binom{4}{1} = 4\\) ways to have exactly 1 success in 4 trials, \\(P(Y = 1) = \\binom{4}{1}p^1(1-p)^{4-1} = (4)(0.25)(0.75)^3\\)."
  },
  {
    "objectID": "slides/03_distribution_ch3.html#binomial-distribution",
    "href": "slides/03_distribution_ch3.html#binomial-distribution",
    "title": "Distribution Theory",
    "section": "Binomial Distribution",
    "text": "Binomial Distribution\n\nIn general, if we carry out a sequence of \\(n\\) Bernoulli trials (with probability of success \\(p\\)) and record \\(Y\\), the total number of successes, then \\(Y\\) follows a binomial distribution, where\n\n\n\\[\\begin{equation}\nP(Y=y) = \\binom{n}{y} p^y (1-p)^{n-y} \\quad \\textrm{for} \\quad y = 0, 1, \\ldots, n.\n\\end{equation}\\]\n\n\n\nIf \\(Y \\sim \\textrm{Binomial}(n,p)\\), then \\(\\operatorname{E}(Y) = np\\) and \\(\\operatorname{SD}(Y) = \\sqrt{np(1-p)}\\).\n\\(E(y)\\) is the expected value of \\(Y\\). If we repeated the experiment many times we would expect on average \\(n*p\\) successes."
  },
  {
    "objectID": "slides/03_distribution_ch3.html#binomial-distribution-graphs",
    "href": "slides/03_distribution_ch3.html#binomial-distribution-graphs",
    "title": "Distribution Theory",
    "section": "Binomial Distribution Graphs",
    "text": "Binomial Distribution Graphs\n\nTypical shapes of a binomial distribution\nI will show you these in R"
  },
  {
    "objectID": "slides/03_distribution_ch3.html#binomial-distribution-graphs-1",
    "href": "slides/03_distribution_ch3.html#binomial-distribution-graphs-1",
    "title": "Distribution Theory",
    "section": "Binomial Distribution Graphs",
    "text": "Binomial Distribution Graphs\n\nOn the left side \\(n\\) remains constant.\nWe see that as \\(p\\) increases, the center of the distribution (\\(\\operatorname{E}(Y) = np\\)) shifts right.\nOn the right, \\(p\\) is held constant.\nAs \\(n\\) increases, the distribution becomes less skewed."
  },
  {
    "objectID": "slides/03_distribution_ch3.html#binomial-distribution-vs-bernoulli",
    "href": "slides/03_distribution_ch3.html#binomial-distribution-vs-bernoulli",
    "title": "Distribution Theory",
    "section": "Binomial Distribution vs Bernoulli",
    "text": "Binomial Distribution vs Bernoulli\n\nNote that if \\(n=1\\),\n\n\n\\[\\begin{align*}\nP(Y=y) &= \\binom{1}{y} p^y(1-p)^{1-y} \\\\\n        &= p^y(1-p)^{1-y}\\quad \\textrm{for}\\quad y = 0, 1,\n\\end{align*}\\] a Bernoulli distribution! - In fact, Bernoulli random variables are a special case of binomial random variables where \\(n=1\\)."
  },
  {
    "objectID": "slides/03_distribution_ch3.html#graphingcalling-in-r",
    "href": "slides/03_distribution_ch3.html#graphingcalling-in-r",
    "title": "Distribution Theory",
    "section": "Graphing/calling in R",
    "text": "Graphing/calling in R\n\nIn R we can use the function dbinom(y, n, p), which outputs the probability of \\(y\\) successes given \\(n\\) trials with probability \\(p\\), i.e., \\(P(Y=y)\\) for \\(Y \\sim \\textrm{Binomial}(n,p)\\).\nLets try it"
  },
  {
    "objectID": "slides/03_distribution_ch3.html#example-2-12",
    "href": "slides/03_distribution_ch3.html#example-2-12",
    "title": "Distribution Theory",
    "section": "Example 2 (1/2)",
    "text": "Example 2 (1/2)\n\nWhile taking a multiple choice test, a student encountered 10 problems where she ended up completely guessing, randomly selecting one of the four options.\nWhat is the chance that she got exactly 2 of the 10 correct?\nWhat assumption do we need to make about the questions?\nHow would we do this without using a distribution?"
  },
  {
    "objectID": "slides/03_distribution_ch3.html#example-2-22",
    "href": "slides/03_distribution_ch3.html#example-2-22",
    "title": "Distribution Theory",
    "section": "Example 2 (2/2)",
    "text": "Example 2 (2/2)\n\nKnowing that the student randomly selected her answers, we assume she has a 25% chance of a correct response.\nn factorial is denoted \\(n!\\) and \\(n!=n\\cdot(n-1)\\cdot...\\cdot 3\\cdot 2\\cdot 1\\)\nHere we used a combination \\({n \\choose p}=\\frac{n!}{p!(n-p)!}\\) read n choose p\nThus, \\(P(Y=2) = {10 \\choose 2}(.25)^2(.75)^8 = 0.282\\).\n\n\n\nWe can use R to verify this:\n\n\n\n\ndbinom(2, size = 10, prob = .25)\n\n[1] 0.2815676\n\n\nTherefore, there is a 28% chance of exactly 2 correct answers out of 10."
  },
  {
    "objectID": "slides/03_distribution_ch3.html#negative-binomial-random-variable",
    "href": "slides/03_distribution_ch3.html#negative-binomial-random-variable",
    "title": "Distribution Theory",
    "section": "Negative Binomial Random Variable",
    "text": "Negative Binomial Random Variable\n\nWhat if we were to carry out multiple independent and identical Bernoulli trails until the \\(r\\)th success occurs?\nIf we model \\(Y\\), the number of failures before the \\(r\\)th success, then \\(Y\\) follows a negative binomial distribution where\n\n\n\\[\\begin{equation}\nP(Y=y) = \\binom{y + r - 1}{r-1} (1-p)^{y}(p)^r \\quad \\textrm{for}\\quad y = 0, 1, \\ldots, \\infty.\n\\end{equation}\\]"
  },
  {
    "objectID": "slides/03_distribution_ch3.html#negative-binomial-rv",
    "href": "slides/03_distribution_ch3.html#negative-binomial-rv",
    "title": "Distribution Theory",
    "section": "Negative Binomial RV",
    "text": "Negative Binomial RV\n\nIf \\(Y \\sim \\textrm{Negative Binomial}(r, p)\\) then \\(\\operatorname{E}(Y) = \\frac{r(1-p)}{p}\\) and \\(\\operatorname{SD}(Y) = \\sqrt{\\frac{r(1-p)}{p^2}}\\)."
  },
  {
    "objectID": "slides/03_distribution_ch3.html#negative-binomial-rv-1",
    "href": "slides/03_distribution_ch3.html#negative-binomial-rv-1",
    "title": "Distribution Theory",
    "section": "Negative Binomial RV",
    "text": "Negative Binomial RV\n\nPlotCode\n\n\n\n\n\n\n\nNegative Binomial\n\n\n\n\n\nNotice how centers shift right as \\(r\\) increases, and left as \\(p\\) increases.\n\n\n\n\n#negativeBinomialPlots\nplotNBinom &lt;- function(p, r){\n  ynb &lt;- 0:10000\n  nbd &lt;- tibble(x = rnbinom(ynb, r, p))\n  #breaks &lt;- pretty(range(nbd$x), n = nclass.FD(nbd$x), min.n = 1)  # pretty binning\n  #bwidth &lt;- breaks[2] - breaks[1]\n  ggplot(nbd, aes(x = x)) +\n    geom_histogram(aes(y=..count../sum(..count..)), binwidth = .25) +\n    labs(title = paste(\"p = \", p, \", r = \", r),\n         x = \"number of failures\",\n         y = \"probability\") +\n    xlim(-1,30)\n}\n\nNBin1 &lt;- plotNBinom(0.35, 3)\nNBin2 &lt;- plotNBinom(0.35, 5)\nNBin3 &lt;- plotNBinom(0.70, 5)\ngrid.arrange(NBin1, NBin2, NBin3, ncol = 1)"
  },
  {
    "objectID": "slides/03_distribution_ch3.html#negative-binomial-rv-2",
    "href": "slides/03_distribution_ch3.html#negative-binomial-rv-2",
    "title": "Distribution Theory",
    "section": "Negative Binomial RV",
    "text": "Negative Binomial RV\n\nNote that if we set \\(r=1\\), then\n\n\n\\[\\begin{align*}\nP(Y=y) &= \\binom{y}{0} (1-p)^yp \\\\\n        &= (1-p)^yp \\quad \\textrm{for} \\quad y = 0, 1, \\ldots, \\infty,\n\\end{align*}\\] which is the probability mass function of a geometric random variable!\n\nThus, a geometric random variable is, in fact, a special case of a negative binomial random variable.\nWhile negative binomial random variables typically are expressed as above using binomial coefficients (expressions such as \\(\\binom{x}{y}\\)), we can generalize our definition to allow non-integer values of \\(r\\).\nR function dnbinom(y, r, p) for the probability of \\(y\\) failures before the \\(r\\)th success given probability \\(p\\)."
  },
  {
    "objectID": "slides/03_distribution_ch3.html#poisson-random-variable",
    "href": "slides/03_distribution_ch3.html#poisson-random-variable",
    "title": "Distribution Theory",
    "section": "Poisson Random Variable",
    "text": "Poisson Random Variable\n\nSometimes, random variables are based on a Poisson process. \nIn a Poisson process, we are counting the number of events per unit of time or space and the number of events depends only on the length or size of the interval.\nWe can then model \\(Y\\), the number of events in one of these sections with the Poisson distribution, where\n\n\n\\[\\begin{equation}\nP(Y=y) = \\frac{e^{-\\lambda}\\lambda^y}{y!} \\quad \\textrm{for} \\quad y = 0, 1, \\ldots, \\infty,\n\\end{equation}\\] where \\(\\lambda\\) is the mean or expected count in the unit of time or space of interest. - This probability mass function has \\(\\operatorname{E}(Y) = \\lambda\\) and \\(\\operatorname{SD}(Y) = \\sqrt{\\lambda}\\)."
  },
  {
    "objectID": "slides/03_distribution_ch3.html#poisson-distribution-graphs",
    "href": "slides/03_distribution_ch3.html#poisson-distribution-graphs",
    "title": "Distribution Theory",
    "section": "Poisson Distribution Graphs",
    "text": "Poisson Distribution Graphs\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\nNotice how distributions become more symmetric as \\(\\lambda\\) increases.\nIf we wish to use R, dpois(y, lambda) outputs the probability of \\(y\\) events given \\(\\lambda\\).\n\n\n\n\n#poissonPlots\nplotPois &lt;- function(lam){\nyp = 0:10000 # possible values\npd &lt;- data.frame(x=rpois(yp, lam))  # generate random deviates\nbreaks &lt;- pretty(range(pd$x), n = nclass.FD(pd$x), min.n = 1)  # pretty binning\n#bwidth &lt;- breaks[2] - breaks[1]\nggplot(pd, aes(x = x)) + geom_histogram(aes(y=..count../sum(..count..)), binwidth = .25) +\n  xlab(\"number of events\") + ylab(\"probability\") + \n  labs(title=paste(\"Poisson lambda = \", lam)) + xlim(-1,13)\n}\n\nPois1 &lt;- plotPois(0.5)\nPois2 &lt;- plotPois(1)\nPois3 &lt;- plotPois(5) + scale_y_continuous(breaks = c(0, 0.1))\ngrid.arrange(Pois1,Pois2,Pois3,ncol=1)"
  },
  {
    "objectID": "slides/03_distribution_ch3.html#continuous-random-variables-1",
    "href": "slides/03_distribution_ch3.html#continuous-random-variables-1",
    "title": "Distribution Theory",
    "section": "Continuous Random Variables",
    "text": "Continuous Random Variables\n\nA continuous random variable can take on an uncountably infinite number of values.\nWith continuous random variables, we define probabilities using probability density functions (pdfs). \nProbabilities are calculated by computing the area under the density curve over the interval of interest. So, given a pdf, \\(f(y)\\), we can compute\n\n\n\\[\\begin{align*}\nP(a \\le Y \\le b) = \\int_a^b f(y)dy.\n\\end{align*}\\]"
  },
  {
    "objectID": "slides/03_distribution_ch3.html#continuous-random-variables-2",
    "href": "slides/03_distribution_ch3.html#continuous-random-variables-2",
    "title": "Distribution Theory",
    "section": "Continuous Random Variables",
    "text": "Continuous Random Variables\nA few properties of continuous random variables:\n\nThe area under their density curve is 1. (\\(\\int_{-\\infty}^{\\infty} f(y)dy = 1\\)).\n\nFor any value \\(y\\), \\(P(Y = y) =  \\int_y^y f(y)dy = 0\\). Why?\nBecause of the above property, \\(P(y &lt; Y) = P(y \\le Y)\\). We will typically use the first notation rather than the second, but both are equally valid."
  },
  {
    "objectID": "slides/03_distribution_ch3.html#exponential-random-variable",
    "href": "slides/03_distribution_ch3.html#exponential-random-variable",
    "title": "Distribution Theory",
    "section": "Exponential Random Variable",
    "text": "Exponential Random Variable\n\nSuppose we have a Poisson process with rate \\(\\lambda\\), and we wish to model the wait time \\(Y\\) until the first event.\nWe could model \\(Y\\) using an exponential distribution, where\n\n\n\\[\\begin{equation}\nf(y) = \\lambda e^{-\\lambda y} \\quad \\textrm{for} \\quad y &gt; 0,\n\\end{equation}\\] - \\(\\operatorname{E}(Y) = 1/\\lambda\\) and \\(\\operatorname{SD}(Y) = 1/\\lambda\\)."
  },
  {
    "objectID": "slides/03_distribution_ch3.html#exponential-distribution",
    "href": "slides/03_distribution_ch3.html#exponential-distribution",
    "title": "Distribution Theory",
    "section": "Exponential Distribution",
    "text": "Exponential Distribution\nExponential distributions with \\(\\lambda = 0.5, 1,\\) and \\(5\\).\n\nPlotCode\n\n\n\n\n\n\n\n\n\nExponential Distribution\n\n\n\n\n\n\nAs \\(\\lambda\\) increases, \\(\\operatorname{E}(Y)\\) tends towards 0, and distributions “die off” quicker.\n\n\n\n\n\n#exponentialPlots\nx=seq(0,4,by=0.01)  # possible values\nprobex1 &lt;- dexp(x,.5)  # P(Y=y)\nprobex2 &lt;- dexp(x,1)\nprobex3 &lt;- dexp(x,5)\nExpdf &lt;- tibble(x,probex1, probex2, probex3) %&gt;%\n  rename(x = x,\n         `0.5` = probex1,\n         `1` = probex2,\n         `5` = probex3) %&gt;%\n  gather(2:4, key = \"Lambda\", value = \"value\") %&gt;%\n  mutate(Lambda = factor(Lambda, levels = c(\"0.5\", \"1\", \"5\")))\nggplot(data = Expdf, aes(x = x, y = value, color = Lambda)) +\n  geom_line(aes(linetype = Lambda)) +\n  xlab(\"values\") + ylab(\"density\") + \n  labs(title = \"Exponential Distributions\") + \n  xlim(0,4) + ylim(0,3)\n\n\n\n\n\n\n\n\n\nTo use R, pexp(y, lambda) outputs the probability \\(P(Y &lt; y)\\) given \\(\\lambda\\)."
  },
  {
    "objectID": "slides/03_distribution_ch3.html#gamma-random-variable",
    "href": "slides/03_distribution_ch3.html#gamma-random-variable",
    "title": "Distribution Theory",
    "section": "Gamma Random Variable",
    "text": "Gamma Random Variable\n\nOnce again consider a Poisson process.\nWhen discussing exponential random variables, we modeled the wait time before one event occurred.\nIf \\(Y\\) represents the wait time before \\(r\\) events occur in a Poisson process with rate \\(\\lambda\\), \\(Y\\) follows a gamma distribution where\n\n\n\\[\\begin{equation}\nf(y) = \\frac{\\lambda^r}{\\Gamma(r)} y^{r-1} e^{-\\lambda y}\\quad \\textrm{for} \\quad y &gt;0.\n\\end{equation}\\]\n\nIf \\(Y \\sim \\textrm{Gamma}(r, \\lambda)\\) then \\(\\operatorname{E}(Y) = r/\\lambda\\) and \\(\\operatorname{SD}(Y) = \\sqrt{r/\\lambda^2}\\).\nNote: \\(\\Gamma(r)=(r-1)!\\) (There is more to it)"
  },
  {
    "objectID": "slides/03_distribution_ch3.html#gamma-distribution",
    "href": "slides/03_distribution_ch3.html#gamma-distribution",
    "title": "Distribution Theory",
    "section": "Gamma Distribution",
    "text": "Gamma Distribution\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\nMeans increase as \\(r\\) increases, but decrease as \\(\\lambda\\) increases.\n\n\n\n\nx &lt;- seq(0, 7, by = 0.01)\n`r = 1, lambda = 1` &lt;- dgamma(x, 1, rate = 1)\n`r = 2, lambda = 1` &lt;- dgamma(x, 2, rate = 1) \n`r = 5, lambda = 5` &lt;- dgamma(x, 5, rate = 5)\n`r = 5, lambda = 7` &lt;- dgamma(x, 5, rate = 7)\n\ngammaDf &lt;- tibble(x, `r = 1, lambda = 1`, `r = 2, lambda = 1`, `r = 5, lambda = 5`, `r = 5, lambda = 7`) %&gt;%\n  gather(2:5, key = \"Distribution\", value = \"value\") %&gt;%\n  mutate(Distribution = factor(Distribution, \n                               levels = c(\"r = 2, lambda = 1\", \n                                          \"r = 1, lambda = 1\", \n                                          \"r = 5, lambda = 5\", \n                                          \"r = 5, lambda = 7\")))\n\nggplot(data = gammaDf, aes(x = x, y = value, \n                           color = Distribution)) +\n  geom_line(aes(linetype = Distribution)) +\n  xlab(\"values\") + ylab(\"density\") + \n  labs(title = \"Gamma Distributions\") +\n  theme(legend.title = element_blank())"
  },
  {
    "objectID": "slides/03_distribution_ch3.html#gamma-vs-others",
    "href": "slides/03_distribution_ch3.html#gamma-vs-others",
    "title": "Distribution Theory",
    "section": "Gamma vs Others",
    "text": "Gamma vs Others\n\nNote that if we let \\(r = 1\\), we have the following pdf,\n\n\n\\[\\begin{align*}\nf(y) &= \\frac{\\lambda}{\\Gamma(1)} y^{1-1} e^{-\\lambda y} \\\\\n      &= \\lambda e^{-\\lambda y} \\quad \\textrm{for} \\quad y &gt; 0,\n\\end{align*}\\] an exponential distribution. - Just as how the geometric distribution was a special case of the negative binomial, exponential distributions are in fact a special case of gamma distributions!\n\nJust like negative binomial, the pdf of a gamma distribution is defined for all real, non-negative \\(r\\).\nIn R, pgamma(y, r, lambda) outputs the probability \\(P(Y &lt; y)\\) given \\(r\\) and \\(\\lambda\\)."
  },
  {
    "objectID": "slides/03_distribution_ch3.html#distributions-used-in-testing",
    "href": "slides/03_distribution_ch3.html#distributions-used-in-testing",
    "title": "Distribution Theory",
    "section": "Distributions Used in Testing",
    "text": "Distributions Used in Testing\n\nWe have spent most of this chapter discussing probability distributions that may come in handy when modeling.\nThe following distributions, while rarely used in modeling, prove useful in hypothesis testing as certain commonly used test statistics follow these distributions.\n\\(\\chi^2\\) distribution (requires a degree of freedom)\nStudent \\(t\\) distribution\n\\(F\\) distribution (need 2 different degrees of freedom)\nSince we have used these In the past, we will leave their definitions to be referenced if needed"
  },
  {
    "objectID": "slides/03_distribution_ch3.html#distribution-table",
    "href": "slides/03_distribution_ch3.html#distribution-table",
    "title": "Distribution Theory",
    "section": "Distribution Table!",
    "text": "Distribution Table!\n\nWould not fit on a slide\nClick HERE\nThe “web” of distributions: https://www.acsu.buffalo.edu/~adamcunn/probability/poisson.html"
  },
  {
    "objectID": "slides/03_distribution_ch3.html#acknowledgements",
    "href": "slides/03_distribution_ch3.html#acknowledgements",
    "title": "Distribution Theory",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThese slides are based on content in BMLR: Chapter 1 - Review of Multiple Linear Regression"
  },
  {
    "objectID": "slides/03_distribution_ch3_o.html",
    "href": "slides/03_distribution_ch3_o.html",
    "title": "Distribution Theory",
    "section": "",
    "text": "Write definitions of non-normal random variables in the context of an application.\nIdentify possible values for each random variable.\nIdentify how changing values for a parameter affects the characteristics of the distribution.\nRecognize a form of the probability density function for each distribution.\nIdentify the mean and variance for each distribution.\nMatch the response for a study to a plausible random variable and provide reasons for ruling out other random variables.\nMatch a histogram of sample data to plausible distributions.\nCreate a mixture of distributions and evaluate the shape, mean, and variance."
  },
  {
    "objectID": "slides/03_distribution_ch3_o.html#learning-objectives",
    "href": "slides/03_distribution_ch3_o.html#learning-objectives",
    "title": "Distribution Theory",
    "section": "",
    "text": "Write definitions of non-normal random variables in the context of an application.\nIdentify possible values for each random variable.\nIdentify how changing values for a parameter affects the characteristics of the distribution.\nRecognize a form of the probability density function for each distribution.\nIdentify the mean and variance for each distribution.\nMatch the response for a study to a plausible random variable and provide reasons for ruling out other random variables.\nMatch a histogram of sample data to plausible distributions.\nCreate a mixture of distributions and evaluate the shape, mean, and variance."
  },
  {
    "objectID": "slides/03_distribution_ch3_o.html#libraries-needed-none-new",
    "href": "slides/03_distribution_ch3_o.html#libraries-needed-none-new",
    "title": "Distribution Theory",
    "section": "Libraries Needed (none new?)",
    "text": "Libraries Needed (none new?)\n\n# Packages required for Chapter 3\nlibrary(gridExtra)  \nlibrary(knitr) \nlibrary(kableExtra)\nlibrary(tidyverse)"
  },
  {
    "objectID": "slides/03_distribution_ch3_o.html#introduction",
    "href": "slides/03_distribution_ch3_o.html#introduction",
    "title": "Distribution Theory",
    "section": "Introduction",
    "text": "Introduction\n\nWhat if it is not plausible that a response is normally distributed?\nYou may want to construct a model to predict whether a prospective student will enroll at a school or model the lifetimes of patients following a particular surgery.\n\nIn the first case you have a binary response (enrolls (1) or does not enroll (0)), and in the second case you are likely to have very skewed data with many similar values and a few hardy souls with extremely long survival.\nThese responses are not expected to be normally distributed; other distributions will be needed to describe and model binary or lifetime data.\nNon-normal responses are encountered in a large number of situations. Luckily, there are quite a few possibilities for models."
  },
  {
    "objectID": "slides/03_distribution_ch3_o.html#discrete-random-variables-1",
    "href": "slides/03_distribution_ch3_o.html#discrete-random-variables-1",
    "title": "Distribution Theory",
    "section": "Discrete Random Variables",
    "text": "Discrete Random Variables\n\nA discrete random variable has a countable number of possible values\n\nEx: we may want to measure the number of people in a household\nEx: model the number of crimes committed on a college campus.\n\nWith discrete random variables, the associated probabilities can be calculated for each possible value using a probability mass function (pmf). \nA pmf is a function that calculates \\(P(Y=y)\\), given each variable’s parameters."
  },
  {
    "objectID": "slides/03_distribution_ch3_o.html#binary-random-variable",
    "href": "slides/03_distribution_ch3_o.html#binary-random-variable",
    "title": "Distribution Theory",
    "section": "Binary Random Variable",
    "text": "Binary Random Variable\n\nConsider the event of flipping a (possibly unfair) coin.\nIf the coin lands heads, let’s consider this a success and record \\(Y = 1\\).\nA series of these events is a Bernoulli process, independent trials that take on one of two values (e.g., 0 or 1).\nThese values are often referred to as a failure and a success\n\nthe probability of success is identical for each trial."
  },
  {
    "objectID": "slides/03_distribution_ch3_o.html#binary-random-variable-1",
    "href": "slides/03_distribution_ch3_o.html#binary-random-variable-1",
    "title": "Distribution Theory",
    "section": "Binary Random Variable",
    "text": "Binary Random Variable\n\nSuppose we only flip the coin once, so we only have one parameter, the probability of flipping heads, \\(p\\).\nIf we know this value, we can express \\(P(Y=1) = p\\) and \\(P(Y=0) = 1-p\\).\nIn general, if we have a Bernoulli process with only one trial, we have a binary distribution (also called a Bernoulli distribution) where\n\n. . .\n\\[\\begin{equation*}\nP(Y = y) = p^y(1-p)^{1-y} \\quad \\textrm{for} \\quad y = 0, 1.\n\\end{equation*}\\]\n\nIf \\(Y \\sim \\textrm{Binary}(p)\\), then \\(Y\\) has mean \\(\\operatorname{E}(Y) = p\\) and standard deviation \\(\\operatorname{SD}(Y) = \\sqrt{p(1-p)}\\)."
  },
  {
    "objectID": "slides/03_distribution_ch3_o.html#example-1",
    "href": "slides/03_distribution_ch3_o.html#example-1",
    "title": "Distribution Theory",
    "section": "Example 1",
    "text": "Example 1\n\nYour playlist of 200 songs has 5 which you cannot stand. What is the probability that when you hit shuffle, a song you tolerate comes on?\nWe want to understand the example and be able to translate that into notation/model it with a distribution\nAssuming all songs have equal odds of playing, we can calculate \\(p = \\frac{200-5}{200} = 0.975\\)\n\nso there is a 97.5% chance of a song you tolerate playing, since \\(P(Y=1)=.975^1*(1-.975)^0\\)."
  },
  {
    "objectID": "slides/03_distribution_ch3_o.html#binomial-random-variable-12",
    "href": "slides/03_distribution_ch3_o.html#binomial-random-variable-12",
    "title": "Distribution Theory",
    "section": "Binomial Random Variable (1/2)",
    "text": "Binomial Random Variable (1/2)\n\nDoes anybody know how this relates to a Bernoulli random variable?\nWe can extend our knowledge of binary random variables.\nSuppose we flipped an unfair coin \\(n\\) times and recorded \\(Y\\), the number of heads after \\(n\\) flips.\nIf we consider a case where \\(p = 0.25\\) and \\(n = 4\\), then here \\(P(Y=0)\\) represents the probability of no successes in 4 trials\n\n4 consecutive failures.\nThe probability of 4 consecutive failures is \\(P(Y = 0) = P(TTTT) = (1-p)^4 = 0.75^4\\)."
  },
  {
    "objectID": "slides/03_distribution_ch3_o.html#binomial-random-variable-22",
    "href": "slides/03_distribution_ch3_o.html#binomial-random-variable-22",
    "title": "Distribution Theory",
    "section": "Binomial Random Variable (2/2)",
    "text": "Binomial Random Variable (2/2)\n\nNext we consider \\(P(Y = 1)\\), and are interested in the probability of exactly 1 success anywhere among the 4 trials.\nHow many different ways can we get exactly 1 success in the four trials?\nTo find the probability of this what else do we need to count?\nWhat is the probability?\nThere are \\(\\binom{4}{1} = 4\\) ways to have exactly 1 success in 4 trials, \\(P(Y = 1) = \\binom{4}{1}p^1(1-p)^{4-1} = (4)(0.25)(0.75)^3\\)."
  },
  {
    "objectID": "slides/03_distribution_ch3_o.html#binomial-distribution",
    "href": "slides/03_distribution_ch3_o.html#binomial-distribution",
    "title": "Distribution Theory",
    "section": "Binomial Distribution",
    "text": "Binomial Distribution\n\nIn general, if we carry out a sequence of \\(n\\) Bernoulli trials (with probability of success \\(p\\)) and record \\(Y\\), the total number of successes, then \\(Y\\) follows a binomial distribution, where\n\n. . .\n\\[\\begin{equation}\nP(Y=y) = \\binom{n}{y} p^y (1-p)^{n-y} \\quad \\textrm{for} \\quad y = 0, 1, \\ldots, n.\n\\end{equation}\\]\n. . .\n\nIf \\(Y \\sim \\textrm{Binomial}(n,p)\\), then \\(\\operatorname{E}(Y) = np\\) and \\(\\operatorname{SD}(Y) = \\sqrt{np(1-p)}\\).\n\\(E(y)\\) is the expected value of \\(Y\\). If we repeated the experiment many times we would expect on average \\(n*p\\) successes."
  },
  {
    "objectID": "slides/03_distribution_ch3_o.html#binomial-distribution-graphs",
    "href": "slides/03_distribution_ch3_o.html#binomial-distribution-graphs",
    "title": "Distribution Theory",
    "section": "Binomial Distribution Graphs",
    "text": "Binomial Distribution Graphs\n\nTypical shapes of a binomial distribution\nI will show you these in R"
  },
  {
    "objectID": "slides/03_distribution_ch3_o.html#binomial-distribution-graphs-1",
    "href": "slides/03_distribution_ch3_o.html#binomial-distribution-graphs-1",
    "title": "Distribution Theory",
    "section": "Binomial Distribution Graphs",
    "text": "Binomial Distribution Graphs\n\nOn the left side \\(n\\) remains constant.\nWe see that as \\(p\\) increases, the center of the distribution (\\(\\operatorname{E}(Y) = np\\)) shifts right.\nOn the right, \\(p\\) is held constant.\nAs \\(n\\) increases, the distribution becomes less skewed."
  },
  {
    "objectID": "slides/03_distribution_ch3_o.html#binomial-distribution-vs-bernoulli",
    "href": "slides/03_distribution_ch3_o.html#binomial-distribution-vs-bernoulli",
    "title": "Distribution Theory",
    "section": "Binomial Distribution vs Bernoulli",
    "text": "Binomial Distribution vs Bernoulli\n\nNote that if \\(n=1\\),\n\n. . .\n\\[\\begin{align*}\nP(Y=y) &= \\binom{1}{y} p^y(1-p)^{1-y} \\\\\n        &= p^y(1-p)^{1-y}\\quad \\textrm{for}\\quad y = 0, 1,\n\\end{align*}\\] a Bernoulli distribution! - In fact, Bernoulli random variables are a special case of binomial random variables where \\(n=1\\)."
  },
  {
    "objectID": "slides/03_distribution_ch3_o.html#graphingcalling-in-r",
    "href": "slides/03_distribution_ch3_o.html#graphingcalling-in-r",
    "title": "Distribution Theory",
    "section": "Graphing/calling in R",
    "text": "Graphing/calling in R\n\nIn R we can use the function dbinom(y, n, p), which outputs the probability of \\(y\\) successes given \\(n\\) trials with probability \\(p\\), i.e., \\(P(Y=y)\\) for \\(Y \\sim \\textrm{Binomial}(n,p)\\).\nLets try it"
  },
  {
    "objectID": "slides/03_distribution_ch3_o.html#example-2-12",
    "href": "slides/03_distribution_ch3_o.html#example-2-12",
    "title": "Distribution Theory",
    "section": "Example 2 (1/2)",
    "text": "Example 2 (1/2)\n\nWhile taking a multiple choice test, a student encountered 10 problems where she ended up completely guessing, randomly selecting one of the four options.\nWhat is the chance that she got exactly 2 of the 10 correct?\nWhat assumption do we need to make about the questions?\nHow would we do this without using a distribution?"
  },
  {
    "objectID": "slides/03_distribution_ch3_o.html#example-2-22",
    "href": "slides/03_distribution_ch3_o.html#example-2-22",
    "title": "Distribution Theory",
    "section": "Example 2 (2/2)",
    "text": "Example 2 (2/2)\n\nKnowing that the student randomly selected her answers, we assume she has a 25% chance of a correct response.\nn factorial is denoted \\(n!\\) and \\(n!=n\\cdot(n-1)\\cdot...\\cdot 3\\cdot 2\\cdot 1\\)\nHere we used a combination \\({n \\choose p}=\\frac{n!}{p!(n-p)!}\\) read n choose p\nThus, \\(P(Y=2) = {10 \\choose 2}(.25)^2(.75)^8 = 0.282\\).\n\n. . .\n\nWe can use R to verify this:\n\n. . .\n\ndbinom(2, size = 10, prob = .25)\n\n[1] 0.2815676\n\n\nTherefore, there is a 28% chance of exactly 2 correct answers out of 10."
  },
  {
    "objectID": "slides/03_distribution_ch3_o.html#negative-binomial-random-variable",
    "href": "slides/03_distribution_ch3_o.html#negative-binomial-random-variable",
    "title": "Distribution Theory",
    "section": "Negative Binomial Random Variable",
    "text": "Negative Binomial Random Variable\n\nWhat if we were to carry out multiple independent and identical Bernoulli trails until the \\(r\\)th success occurs?\nIf we model \\(Y\\), the number of failures before the \\(r\\)th success, then \\(Y\\) follows a negative binomial distribution where\n\n. . .\n\\[\\begin{equation}\nP(Y=y) = \\binom{y + r - 1}{r-1} (1-p)^{y}(p)^r \\quad \\textrm{for}\\quad y = 0, 1, \\ldots, \\infty.\n\\end{equation}\\]"
  },
  {
    "objectID": "slides/03_distribution_ch3_o.html#negative-binomial-rv",
    "href": "slides/03_distribution_ch3_o.html#negative-binomial-rv",
    "title": "Distribution Theory",
    "section": "Negative Binomial RV",
    "text": "Negative Binomial RV\n\nIf \\(Y \\sim \\textrm{Negative Binomial}(r, p)\\) then \\(\\operatorname{E}(Y) = \\frac{r(1-p)}{p}\\) and \\(\\operatorname{SD}(Y) = \\sqrt{\\frac{r(1-p)}{p^2}}\\)."
  },
  {
    "objectID": "slides/03_distribution_ch3_o.html#negative-binomial-rv-1",
    "href": "slides/03_distribution_ch3_o.html#negative-binomial-rv-1",
    "title": "Distribution Theory",
    "section": "Negative Binomial RV",
    "text": "Negative Binomial RV\n\nPlotCode\n\n\n\n\n\n\n\nNegative Binomial\n\n\n\n\n\nNotice how centers shift right as \\(r\\) increases, and left as \\(p\\) increases.\n\n\n\n\n#negativeBinomialPlots\nplotNBinom &lt;- function(p, r){\n  ynb &lt;- 0:10000\n  nbd &lt;- tibble(x = rnbinom(ynb, r, p))\n  #breaks &lt;- pretty(range(nbd$x), n = nclass.FD(nbd$x), min.n = 1)  # pretty binning\n  #bwidth &lt;- breaks[2] - breaks[1]\n  ggplot(nbd, aes(x = x)) +\n    geom_histogram(aes(y=..count../sum(..count..)), binwidth = .25) +\n    labs(title = paste(\"p = \", p, \", r = \", r),\n         x = \"number of failures\",\n         y = \"probability\") +\n    xlim(-1,30)\n}\n\nNBin1 &lt;- plotNBinom(0.35, 3)\nNBin2 &lt;- plotNBinom(0.35, 5)\nNBin3 &lt;- plotNBinom(0.70, 5)\ngrid.arrange(NBin1, NBin2, NBin3, ncol = 1)"
  },
  {
    "objectID": "slides/03_distribution_ch3_o.html#negative-binomial-rv-2",
    "href": "slides/03_distribution_ch3_o.html#negative-binomial-rv-2",
    "title": "Distribution Theory",
    "section": "Negative Binomial RV",
    "text": "Negative Binomial RV\n\nNote that if we set \\(r=1\\), then\n\n. . .\n\\[\\begin{align*}\nP(Y=y) &= \\binom{y}{0} (1-p)^yp \\\\\n        &= (1-p)^yp \\quad \\textrm{for} \\quad y = 0, 1, \\ldots, \\infty,\n\\end{align*}\\] which is the probability mass function of a geometric random variable!\n\nThus, a geometric random variable is, in fact, a special case of a negative binomial random variable.\nWhile negative binomial random variables typically are expressed as above using binomial coefficients (expressions such as \\(\\binom{x}{y}\\)), we can generalize our definition to allow non-integer values of \\(r\\).\nR function dnbinom(y, r, p) for the probability of \\(y\\) failures before the \\(r\\)th success given probability \\(p\\)."
  },
  {
    "objectID": "slides/03_distribution_ch3_o.html#poisson-random-variable",
    "href": "slides/03_distribution_ch3_o.html#poisson-random-variable",
    "title": "Distribution Theory",
    "section": "Poisson Random Variable",
    "text": "Poisson Random Variable\n\nSometimes, random variables are based on a Poisson process. \nIn a Poisson process, we are counting the number of events per unit of time or space and the number of events depends only on the length or size of the interval.\nWe can then model \\(Y\\), the number of events in one of these sections with the Poisson distribution, where\n\n. . .\n\\[\\begin{equation}\nP(Y=y) = \\frac{e^{-\\lambda}\\lambda^y}{y!} \\quad \\textrm{for} \\quad y = 0, 1, \\ldots, \\infty,\n\\end{equation}\\] where \\(\\lambda\\) is the mean or expected count in the unit of time or space of interest. - This probability mass function has \\(\\operatorname{E}(Y) = \\lambda\\) and \\(\\operatorname{SD}(Y) = \\sqrt{\\lambda}\\)."
  },
  {
    "objectID": "slides/03_distribution_ch3_o.html#poisson-distribution-graphs",
    "href": "slides/03_distribution_ch3_o.html#poisson-distribution-graphs",
    "title": "Distribution Theory",
    "section": "Poisson Distribution Graphs",
    "text": "Poisson Distribution Graphs\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\nNotice how distributions become more symmetric as \\(\\lambda\\) increases.\nIf we wish to use R, dpois(y, lambda) outputs the probability of \\(y\\) events given \\(\\lambda\\).\n\n\n\n\n#poissonPlots\nplotPois &lt;- function(lam){\nyp = 0:10000 # possible values\npd &lt;- data.frame(x=rpois(yp, lam))  # generate random deviates\nbreaks &lt;- pretty(range(pd$x), n = nclass.FD(pd$x), min.n = 1)  # pretty binning\n#bwidth &lt;- breaks[2] - breaks[1]\nggplot(pd, aes(x = x)) + geom_histogram(aes(y=..count../sum(..count..)), binwidth = .25) +\n  xlab(\"number of events\") + ylab(\"probability\") + \n  labs(title=paste(\"Poisson lambda = \", lam)) + xlim(-1,13)\n}\n\nPois1 &lt;- plotPois(0.5)\nPois2 &lt;- plotPois(1)\nPois3 &lt;- plotPois(5) + scale_y_continuous(breaks = c(0, 0.1))\ngrid.arrange(Pois1,Pois2,Pois3,ncol=1)"
  },
  {
    "objectID": "slides/03_distribution_ch3_o.html#continuous-random-variables-1",
    "href": "slides/03_distribution_ch3_o.html#continuous-random-variables-1",
    "title": "Distribution Theory",
    "section": "Continuous Random Variables",
    "text": "Continuous Random Variables\n\nA continuous random variable can take on an uncountably infinite number of values.\nWith continuous random variables, we define probabilities using probability density functions (pdfs). \nProbabilities are calculated by computing the area under the density curve over the interval of interest. So, given a pdf, \\(f(y)\\), we can compute\n\n. . .\n\\[\\begin{align*}\nP(a \\le Y \\le b) = \\int_a^b f(y)dy.\n\\end{align*}\\]"
  },
  {
    "objectID": "slides/03_distribution_ch3_o.html#continuous-random-variables-2",
    "href": "slides/03_distribution_ch3_o.html#continuous-random-variables-2",
    "title": "Distribution Theory",
    "section": "Continuous Random Variables",
    "text": "Continuous Random Variables\nA few properties of continuous random variables:\n\nThe area under their density curve is 1. (\\(\\int_{-\\infty}^{\\infty} f(y)dy = 1\\)).\n\nFor any value \\(y\\), \\(P(Y = y) =  \\int_y^y f(y)dy = 0\\). Why?\nBecause of the above property, \\(P(y &lt; Y) = P(y \\le Y)\\). We will typically use the first notation rather than the second, but both are equally valid."
  },
  {
    "objectID": "slides/03_distribution_ch3_o.html#exponential-random-variable",
    "href": "slides/03_distribution_ch3_o.html#exponential-random-variable",
    "title": "Distribution Theory",
    "section": "Exponential Random Variable",
    "text": "Exponential Random Variable\n\nSuppose we have a Poisson process with rate \\(\\lambda\\), and we wish to model the wait time \\(Y\\) until the first event.\nWe could model \\(Y\\) using an exponential distribution, where\n\n. . .\n\\[\\begin{equation}\nf(y) = \\lambda e^{-\\lambda y} \\quad \\textrm{for} \\quad y &gt; 0,\n\\end{equation}\\] - \\(\\operatorname{E}(Y) = 1/\\lambda\\) and \\(\\operatorname{SD}(Y) = 1/\\lambda\\)."
  },
  {
    "objectID": "slides/03_distribution_ch3_o.html#exponential-distribution",
    "href": "slides/03_distribution_ch3_o.html#exponential-distribution",
    "title": "Distribution Theory",
    "section": "Exponential Distribution",
    "text": "Exponential Distribution\nExponential distributions with \\(\\lambda = 0.5, 1,\\) and \\(5\\).\n\nPlotCode\n\n\n\n\n\n\n\n\n\nExponential Distribution\n\n\n\n\n\n\nAs \\(\\lambda\\) increases, \\(\\operatorname{E}(Y)\\) tends towards 0, and distributions “die off” quicker.\n\n\n\n\n\n\n#exponentialPlots\nx=seq(0,4,by=0.01)  # possible values\nprobex1 &lt;- dexp(x,.5)  # P(Y=y)\nprobex2 &lt;- dexp(x,1)\nprobex3 &lt;- dexp(x,5)\nExpdf &lt;- tibble(x,probex1, probex2, probex3) %&gt;%\n  rename(x = x,\n         `0.5` = probex1,\n         `1` = probex2,\n         `5` = probex3) %&gt;%\n  gather(2:4, key = \"Lambda\", value = \"value\") %&gt;%\n  mutate(Lambda = factor(Lambda, levels = c(\"0.5\", \"1\", \"5\")))\nggplot(data = Expdf, aes(x = x, y = value, color = Lambda)) +\n  geom_line(aes(linetype = Lambda)) +\n  xlab(\"values\") + ylab(\"density\") + \n  labs(title = \"Exponential Distributions\") + \n  xlim(0,4) + ylim(0,3)\n\n\n\n\n\n\n\n\n\nTo use R, pexp(y, lambda) outputs the probability \\(P(Y &lt; y)\\) given \\(\\lambda\\)."
  },
  {
    "objectID": "slides/03_distribution_ch3_o.html#gamma-random-variable",
    "href": "slides/03_distribution_ch3_o.html#gamma-random-variable",
    "title": "Distribution Theory",
    "section": "Gamma Random Variable",
    "text": "Gamma Random Variable\n\nOnce again consider a Poisson process.\nWhen discussing exponential random variables, we modeled the wait time before one event occurred.\nIf \\(Y\\) represents the wait time before \\(r\\) events occur in a Poisson process with rate \\(\\lambda\\), \\(Y\\) follows a gamma distribution where\n\n. . .\n\\[\\begin{equation}\nf(y) = \\frac{\\lambda^r}{\\Gamma(r)} y^{r-1} e^{-\\lambda y}\\quad \\textrm{for} \\quad y &gt;0.\n\\end{equation}\\]\n\nIf \\(Y \\sim \\textrm{Gamma}(r, \\lambda)\\) then \\(\\operatorname{E}(Y) = r/\\lambda\\) and \\(\\operatorname{SD}(Y) = \\sqrt{r/\\lambda^2}\\).\nNote: \\(\\Gamma(r)=(r-1)!\\) (There is more to it)"
  },
  {
    "objectID": "slides/03_distribution_ch3_o.html#gamma-distribution",
    "href": "slides/03_distribution_ch3_o.html#gamma-distribution",
    "title": "Distribution Theory",
    "section": "Gamma Distribution",
    "text": "Gamma Distribution\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\nMeans increase as \\(r\\) increases, but decrease as \\(\\lambda\\) increases.\n\n\n\n\nx &lt;- seq(0, 7, by = 0.01)\n`r = 1, lambda = 1` &lt;- dgamma(x, 1, rate = 1)\n`r = 2, lambda = 1` &lt;- dgamma(x, 2, rate = 1) \n`r = 5, lambda = 5` &lt;- dgamma(x, 5, rate = 5)\n`r = 5, lambda = 7` &lt;- dgamma(x, 5, rate = 7)\n\ngammaDf &lt;- tibble(x, `r = 1, lambda = 1`, `r = 2, lambda = 1`, `r = 5, lambda = 5`, `r = 5, lambda = 7`) %&gt;%\n  gather(2:5, key = \"Distribution\", value = \"value\") %&gt;%\n  mutate(Distribution = factor(Distribution, \n                               levels = c(\"r = 2, lambda = 1\", \n                                          \"r = 1, lambda = 1\", \n                                          \"r = 5, lambda = 5\", \n                                          \"r = 5, lambda = 7\")))\n\nggplot(data = gammaDf, aes(x = x, y = value, \n                           color = Distribution)) +\n  geom_line(aes(linetype = Distribution)) +\n  xlab(\"values\") + ylab(\"density\") + \n  labs(title = \"Gamma Distributions\") +\n  theme(legend.title = element_blank())"
  },
  {
    "objectID": "slides/03_distribution_ch3_o.html#gamma-vs-others",
    "href": "slides/03_distribution_ch3_o.html#gamma-vs-others",
    "title": "Distribution Theory",
    "section": "Gamma vs Others",
    "text": "Gamma vs Others\n\nNote that if we let \\(r = 1\\), we have the following pdf,\n\n. . .\n\\[\\begin{align*}\nf(y) &= \\frac{\\lambda}{\\Gamma(1)} y^{1-1} e^{-\\lambda y} \\\\\n      &= \\lambda e^{-\\lambda y} \\quad \\textrm{for} \\quad y &gt; 0,\n\\end{align*}\\] an exponential distribution. - Just as how the geometric distribution was a special case of the negative binomial, exponential distributions are in fact a special case of gamma distributions!\n\nJust like negative binomial, the pdf of a gamma distribution is defined for all real, non-negative \\(r\\).\nIn R, pgamma(y, r, lambda) outputs the probability \\(P(Y &lt; y)\\) given \\(r\\) and \\(\\lambda\\)."
  },
  {
    "objectID": "slides/03_distribution_ch3_o.html#distributions-used-in-testing",
    "href": "slides/03_distribution_ch3_o.html#distributions-used-in-testing",
    "title": "Distribution Theory",
    "section": "Distributions Used in Testing",
    "text": "Distributions Used in Testing\n\nWe have spent most of this chapter discussing probability distributions that may come in handy when modeling.\nThe following distributions, while rarely used in modeling, prove useful in hypothesis testing as certain commonly used test statistics follow these distributions.\n\\(\\chi^2\\) distribution (requires a degree of freedom)\nStudent \\(t\\) distribution\n\\(F\\) distribution (need 2 different degrees of freedom)\nSince we have used these In the past, we will leave their definitions to be referenced if needed"
  },
  {
    "objectID": "slides/03_distribution_ch3_o.html#distribution-table",
    "href": "slides/03_distribution_ch3_o.html#distribution-table",
    "title": "Distribution Theory",
    "section": "Distribution Table!",
    "text": "Distribution Table!\n\nWould not fit on a slide\nClick HERE\nThe “web” of distributions: https://www.acsu.buffalo.edu/~adamcunn/probability/poisson.html"
  },
  {
    "objectID": "slides/03_distribution_ch3_o.html#acknowledgements",
    "href": "slides/03_distribution_ch3_o.html#acknowledgements",
    "title": "Distribution Theory",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThese slides are based on content in BMLR: Chapter 1 - Review of Multiple Linear Regression"
  },
  {
    "objectID": "slides/01-welcome_ch1.html#instructor",
    "href": "slides/01-welcome_ch1.html#instructor",
    "title": "Welcome and Chapter 1",
    "section": "Instructor",
    "text": "Instructor\n\nDr. Tyler George: tgeorge@cornellcollege.edu"
  },
  {
    "objectID": "slides/01-welcome_ch1.html#course-logistics",
    "href": "slides/01-welcome_ch1.html#course-logistics",
    "title": "Welcome and Chapter 1",
    "section": "Course logistics",
    "text": "Course logistics\n\nCourse Dates: August 26th to September 18th\nCourse sessions: By Appointment\nExam Dates: tbd"
  },
  {
    "objectID": "slides/01-welcome_ch1.html#generalized-linear-models",
    "href": "slides/01-welcome_ch1.html#generalized-linear-models",
    "title": "Welcome and Chapter 1",
    "section": "Generalized Linear Models",
    "text": "Generalized Linear Models\nIn statistics, a generalized linear model (GLM) is a flexible generalization of ordinary linear regression. The GLM generalizes linear regression by allowing the linear model to be related to the response variable via a link function and by allowing the magnitude of the variance of each measurement to be a function of its predicted value.\n\n\nLogistic regression\n\\[\\begin{aligned}\\pi = P(y = 1 | x) \\hspace{2mm} &\\Rightarrow \\hspace{2mm} \\text{Link function: } \\log\\big(\\frac{\\pi}{1-\\pi}\\big) \\\\\n&\\Rightarrow \\log\\big(\\frac{\\pi}{1-\\pi}\\big) = \\beta_0 + \\beta_1~x\\end{aligned}\\]\n\n\nWikipedia"
  },
  {
    "objectID": "slides/01-welcome_ch1.html#what-were-covering-this-semester13",
    "href": "slides/01-welcome_ch1.html#what-were-covering-this-semester13",
    "title": "Welcome and Chapter 1",
    "section": "What we’re covering this semester(1/3)",
    "text": "What we’re covering this semester(1/3)\nGeneralized Linear Models (Ch 1 - 6)\n\nIntroduce models for non-normal response variables\nEstimation, interpretation, and inference\nMathematical details showing how GLMs are connected"
  },
  {
    "objectID": "slides/01-welcome_ch1.html#what-were-covering-this-semester23",
    "href": "slides/01-welcome_ch1.html#what-were-covering-this-semester23",
    "title": "Welcome and Chapter 1",
    "section": "What we’re covering this semester(2/3)",
    "text": "What we’re covering this semester(2/3)\nModeling correlated data (Ch 7 - 9)\n\nIntroduce multilevel models for correlated and longitudinal data\nEstimation, interpretation, and inference\nMathematical details, particularly diving into covariance structures"
  },
  {
    "objectID": "slides/01-welcome_ch1.html#what-were-covering-this-semester33",
    "href": "slides/01-welcome_ch1.html#what-were-covering-this-semester33",
    "title": "Welcome and Chapter 1",
    "section": "What we’re covering this semester(3/3)",
    "text": "What we’re covering this semester(3/3)\nMore Regression Models (ITSL Chapter 7) - Polynomial Regression - Regression Splines - Smoothing Splines - Generalized Additive Models (GAMS)"
  },
  {
    "objectID": "slides/01-welcome_ch1.html#meet-your-classmates",
    "href": "slides/01-welcome_ch1.html#meet-your-classmates",
    "title": "Welcome and Chapter 1",
    "section": "Meet your classmates!",
    "text": "Meet your classmates!\n\nCreate larger groups\nQuick introductions - Name, year, and major\nChoose a reporter\n\nNeed help choosing? Person with birthday closest to December 1st.\n\nIdentify 8 things everyone in the group has in common\n\nNot being a Cornell Student\nNot clothes (we’re all wearing socks)\nNot body parts (we all have a nose)\n\n\n\nReporter will share list with the class"
  },
  {
    "objectID": "slides/01-welcome_ch1.html#what-background-is-assumed-for-the-course",
    "href": "slides/01-welcome_ch1.html#what-background-is-assumed-for-the-course",
    "title": "Welcome and Chapter 1",
    "section": "What background is assumed for the course?",
    "text": "What background is assumed for the course?\n\nPre-reqs\n\nSTA 201, 202 and DSC 223\n\n\n\nBackground knowledge\n\n\n\nStatistical content\n\nLinear and logistic regression\nStatistical inference\nBasic understanding of random variables\n\n\n\n\nComputing\n\nUsing R for data analysis\nWriting reports using R Markdown or Quarto"
  },
  {
    "objectID": "slides/01-welcome_ch1.html#course-toolkit-12",
    "href": "slides/01-welcome_ch1.html#course-toolkit-12",
    "title": "Welcome and Chapter 1",
    "section": "Course Toolkit (1/2)",
    "text": "Course Toolkit (1/2)\n\nWebsite\n\nhttps://stats-tgeorge.github.io/STA363_AdvReg/\nCentral hub for the course\nNotes\nLabs\nDatasets"
  },
  {
    "objectID": "slides/01-welcome_ch1.html#course-toolkit-12-1",
    "href": "slides/01-welcome_ch1.html#course-toolkit-12-1",
    "title": "Welcome and Chapter 1",
    "section": "Course Toolkit (1/2)",
    "text": "Course Toolkit (1/2)\n\nMoodle:\n\nhttps://moodle.cornellcollege.edu/course/view.php?id=7908\nSubmissions\nGradebook\nAnnouncements"
  },
  {
    "objectID": "slides/01-welcome_ch1.html#class-meetings",
    "href": "slides/01-welcome_ch1.html#class-meetings",
    "title": "Welcome and Chapter 1",
    "section": "Class Meetings",
    "text": "Class Meetings\nLectures\n\nSome traditional lecture\nIndividual and group labs\nBring fully-charged laptop\nMini-projects\nExams\n\n\nAttendance is expected (if you are healthy!)"
  },
  {
    "objectID": "slides/01-welcome_ch1.html#textbook",
    "href": "slides/01-welcome_ch1.html#textbook",
    "title": "Welcome and Chapter 1",
    "section": "Textbook",
    "text": "Textbook\n\n\n\n\nBeyond Multiple Linear Regression by Paul Roback and Julie Legler\n\nAvailable online\nHard copies available for purchase"
  },
  {
    "objectID": "slides/01-welcome_ch1.html#textbook-2",
    "href": "slides/01-welcome_ch1.html#textbook-2",
    "title": "Welcome and Chapter 1",
    "section": "Textbook 2",
    "text": "Textbook 2\nThe secondary text is: An Introduction to Statistical Learning with Applications in R by Gareth James, Daniela Witten, Trevor Hastie, and Robert Tibshirani – it is freely available online. Chapter 7.\n\nHard copies available for purchase"
  },
  {
    "objectID": "slides/01-welcome_ch1.html#using-r-rstudio",
    "href": "slides/01-welcome_ch1.html#using-r-rstudio",
    "title": "Welcome and Chapter 1",
    "section": "Using R / RStudio",
    "text": "Using R / RStudio\n\nRStudio Server is installed and should be used\nhttp://turing.cornellcollege.edu:8787/"
  },
  {
    "objectID": "slides/01-welcome_ch1.html#activities-assessments",
    "href": "slides/01-welcome_ch1.html#activities-assessments",
    "title": "Welcome and Chapter 1",
    "section": "Activities & Assessments",
    "text": "Activities & Assessments\nReadings\n\nPrimarily from Beyond Multiple Linear Regression\nRecommend reading assigned text before lecture\n\n\nHomework - Primarily from Beyond Multiple Linear Regression - Individual assignments - Work together but must complete your own work. Discuss but don’t copy."
  },
  {
    "objectID": "slides/01-welcome_ch1.html#activities-assessments-1",
    "href": "slides/01-welcome_ch1.html#activities-assessments-1",
    "title": "Welcome and Chapter 1",
    "section": "Activities & Assessments",
    "text": "Activities & Assessments\nMini-projects\nExamples:\n\nMini-project 01: Focused on models for non-normal response variables, such as count data\nMini-project 02: Focused on models for correlated data\n\n\n\nShort write up and short presentation\nTeam-based"
  },
  {
    "objectID": "slides/01-welcome_ch1.html#exams",
    "href": "slides/01-welcome_ch1.html#exams",
    "title": "Welcome and Chapter 1",
    "section": "Exams",
    "text": "Exams\n\nTwo exams this block, tbd.\nEach will have two components\n\nComponent 1 will be on these dates and you will get a choice of oral or written format.\nComponent 2 will be a take-home, open-book, open-note, exam.\nYou will have 12 hours or more to complete this component."
  },
  {
    "objectID": "slides/01-welcome_ch1.html#grading",
    "href": "slides/01-welcome_ch1.html#grading",
    "title": "Welcome and Chapter 1",
    "section": "Grading",
    "text": "Grading\nFinal grades will be calculated as follows\n\n\n\nCategory\nPoints\n\n\n\n\nHomework\n200\n\n\nParticipation\n100\n\n\nLabs and Mini Projects\n300\n\n\nExams\n400\n\n\nTotal\n1000\n\n\n\nSee Syllabus on website for letter grade thresholds."
  },
  {
    "objectID": "slides/01-welcome_ch1.html#resources",
    "href": "slides/01-welcome_ch1.html#resources",
    "title": "Welcome and Chapter 1",
    "section": "Resources",
    "text": "Resources\n\nOffice hours to meet with your instructor in West 311\n\nTypically MWTh 3:05pm-4:05pm and by appt.\nDouble check course calendar\nMake appointments by going to https://calendar.app.google/oHqiU2FZDJLXKq429\n\nEmail Tyler George for private questions regarding personal matters or grades.\n\nPlease put STA 363 in the subject line since I am also teaching capstone this semester\n\nCollege support at https://stats-tgeorge.github.io/personal_website/course-support.html."
  },
  {
    "objectID": "slides/01-welcome_ch1.html#setup---r-packages",
    "href": "slides/01-welcome_ch1.html#setup---r-packages",
    "title": "Welcome and Chapter 1",
    "section": "Setup - R Packages",
    "text": "Setup - R Packages\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(GGally)\nlibrary(knitr)\nlibrary(patchwork)\nlibrary(viridis)\nlibrary(ggfortify)"
  },
  {
    "objectID": "slides/01-welcome_ch1.html#assumptions-for-linear-regression",
    "href": "slides/01-welcome_ch1.html#assumptions-for-linear-regression",
    "title": "Welcome and Chapter 1",
    "section": "Assumptions for linear regression",
    "text": "Assumptions for linear regression\nWhat are the assumptions for linear regression? . . .\nLinearity: Linear relationship between mean response and predictor variable(s)\n\nIndependence: Residuals are independent. There is no connection between how far any two points lie above or below regression line.\n\n\nNormality: Response follows a normal distribution at each level of the predictor (or combination of predictors)\n\n\nEqual variance: Variability (variance or standard deviation) of the response is equal for all levels of the predictor (or combination of predictors)\n\n\nUse residual plots to check that the conditions hold before using the model for statistical inference."
  },
  {
    "objectID": "slides/01-welcome_ch1.html#assumptions-for-linear-regression-1",
    "href": "slides/01-welcome_ch1.html#assumptions-for-linear-regression-1",
    "title": "Welcome and Chapter 1",
    "section": "Assumptions for linear regression",
    "text": "Assumptions for linear regression\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLinearity: Linear relationship between mean of the response \\(Y\\) and the predictor \\(X\\)\nIndependence: No connection between how any two points lie above or below the regression line\nNormality: Response, \\(Y\\), follows a normal distribution at each level of the predictor, \\(X\\) (indicated by red curves)\nEqual variance: Variance (or standard deviation) of the response, \\(Y\\), is equal for all levels of the predictor, \\(X\\)\n\n\n\nModified from Figure 1.1. in BMLR]"
  },
  {
    "objectID": "slides/01-welcome_ch1.html#questions",
    "href": "slides/01-welcome_ch1.html#questions",
    "title": "Welcome and Chapter 1",
    "section": "Questions",
    "text": "Questions\nHow do we assess these conditions?"
  },
  {
    "objectID": "slides/01-welcome_ch1.html#beyond-linear-regression",
    "href": "slides/01-welcome_ch1.html#beyond-linear-regression",
    "title": "Welcome and Chapter 1",
    "section": "Beyond linear regression",
    "text": "Beyond linear regression\n\nWhen we use linear least squares regression to draw conclusions, we do so under the assumption that L.I.N.E. are all met.\nGeneralized linear models require different assumptions and can accommodate violations in L.I.N.E.\n\nRelationship between response and predictor(s) can be nonlinear\nResponse variable can be non-normal\nVariance in response can differ at each level of predictor(s)\n\n\n\nBut the independence assumption must hold!\n\nMultilevel models will be used for data with correlated observations"
  },
  {
    "objectID": "slides/01-welcome_ch1.html#review-of-multiple-linear-regression",
    "href": "slides/01-welcome_ch1.html#review-of-multiple-linear-regression",
    "title": "Welcome and Chapter 1",
    "section": "Review of multiple linear regression",
    "text": "Review of multiple linear regression"
  },
  {
    "objectID": "slides/01-welcome_ch1.html#data-kentucky-derby-winners",
    "href": "slides/01-welcome_ch1.html#data-kentucky-derby-winners",
    "title": "Welcome and Chapter 1",
    "section": "Data: Kentucky Derby Winners",
    "text": "Data: Kentucky Derby Winners\nToday’s data is from the Kentucky Derby, an annual 1.25-mile horse race held at the Churchill Downs race track in Louisville, KY. The data is in the file derbyplus.csv and contains information for races 1896 - 2017.\n\n\n\nResponse variable\n\nspeed: Average speed of the winner in feet per second (ft/s)]\n\nAdditional variable\n\nwinner: Winning horse\n\n\nPredictor variables\n\nyear: Year of the race\ncondition: Condition of the track (good, fast, slow)\nstarters: Number of horses who raced]"
  },
  {
    "objectID": "slides/01-welcome_ch1.html#data",
    "href": "slides/01-welcome_ch1.html#data",
    "title": "Welcome and Chapter 1",
    "section": "Data",
    "text": "Data\n\nderby &lt;- read_csv(\"data/derbyplus.csv\")\n\n\nderby |&gt;\n  head(5) |&gt; kable()\n\n\n\n\nyear\nwinner\ncondition\nspeed\nstarters\n\n\n\n\n1896\nBen Brush\ngood\n51.66\n8\n\n\n1897\nTyphoon II\nslow\n49.81\n6\n\n\n1898\nPlaudit\ngood\n51.16\n4\n\n\n1899\nManuel\nfast\n50.00\n5\n\n\n1900\nLieut. Gibson\nfast\n52.28\n7"
  },
  {
    "objectID": "slides/01-welcome_ch1.html#data-analysis-life-cycle",
    "href": "slides/01-welcome_ch1.html#data-analysis-life-cycle",
    "title": "Welcome and Chapter 1",
    "section": "Data Analysis Life Cycle",
    "text": "Data Analysis Life Cycle"
  },
  {
    "objectID": "slides/01-welcome_ch1.html#exploratory-data-analysis-eda",
    "href": "slides/01-welcome_ch1.html#exploratory-data-analysis-eda",
    "title": "Welcome and Chapter 1",
    "section": "Exploratory data analysis (EDA)",
    "text": "Exploratory data analysis (EDA)\n\nOnce you’re ready for the statistical analysis (explore), the first step should always be exploratory data analysis.\nThe EDA will help you\n\nbegin to understand the variables and observations\nidentify outliers or potential data entry errors\nbegin to see relationships between variables\nidentify the appropriate model and identify a strategy\n\nThe EDA is exploratory; formal modeling and statistical inference should be used to draw conclusions."
  },
  {
    "objectID": "slides/01-welcome_ch1.html#plots-for-univariate-eda",
    "href": "slides/01-welcome_ch1.html#plots-for-univariate-eda",
    "title": "Welcome and Chapter 1",
    "section": "Plots for univariate EDA",
    "text": "Plots for univariate EDA\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\np1 &lt;- ggplot(data = derby, aes(x = speed)) + \n  geom_histogram(fill = \"forestgreen\", color = \"black\") + \n  labs(x = \"Winning speed (ft/s)\", y = \"Count\")\n\np2 &lt;- ggplot(data = derby, aes(x = starters)) + \n  geom_histogram(fill = \"forestgreen\", color = \"black\") + \n  labs(x = \"Starters\", y = \"Count\")\n\np3 &lt;- ggplot(data = derby, aes(x = condition)) +\n   geom_bar(fill = \"forestgreen\", color = \"black\", aes(x = ))\n\np1 + (p2 / p3) + \n  plot_annotation(title = \"Univariate data analysis\")"
  },
  {
    "objectID": "slides/01-welcome_ch1.html#plots-for-bivariate-eda",
    "href": "slides/01-welcome_ch1.html#plots-for-bivariate-eda",
    "title": "Welcome and Chapter 1",
    "section": "Plots for bivariate EDA",
    "text": "Plots for bivariate EDA\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\np4 &lt;- ggplot(data = derby, aes(x = starters, y = speed)) + \n  geom_point() + \n  labs(x = \"Starters\", y = \"Speed (ft / s)\")\n\np5 &lt;- ggplot(data = derby, aes(x = year, y = speed)) + \n  geom_point() + \n  labs(x = \"Year\", y = \"Speed (ft / s)\")\n\np6 &lt;- ggplot(data = derby, aes(x = condition, y = speed)) + \n  geom_boxplot(fill = \"forestgreen\", color = \"black\") + \n  labs(x = \"Conditions\", y = \"Speed (ft / s)\")\n\n(p4 + p5) + p6 +\n  plot_annotation(title = \"Bivariate data analysis\")"
  },
  {
    "objectID": "slides/01-welcome_ch1.html#scatterplot-matrix",
    "href": "slides/01-welcome_ch1.html#scatterplot-matrix",
    "title": "Welcome and Chapter 1",
    "section": "Scatterplot matrix",
    "text": "Scatterplot matrix\nA scatterplot matrix helps quickly visualize relationships between many variable pairs. They are particularly useful to identify potentially correlated predictors.\n\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n#library(GGally)\nggpairs(data = derby, \n        columns = c(\"condition\", \"year\", \"starters\", \"speed\"))"
  },
  {
    "objectID": "slides/01-welcome_ch1.html#plots-for-multivariate-eda",
    "href": "slides/01-welcome_ch1.html#plots-for-multivariate-eda",
    "title": "Welcome and Chapter 1",
    "section": "Plots for multivariate EDA",
    "text": "Plots for multivariate EDA\nPlot the relationship between the response and a predictor based on levels of another predictor to assess potential interactions.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n#library(viridis)\nggplot(data = derby, aes(x = year, y = speed, color = condition, \n                         shape = condition, linetype = condition)) + \n  geom_point() + \n  geom_smooth(method = \"lm\", se = FALSE, aes(linetype = condition)) + \n  labs(x = \"Year\", y = \"Speed (ft/s)\", color = \"Condition\",\n       title = \"Speed vs. year\", \n       subtitle = \"by track condition\") +\n  guides(lty = FALSE, shape = FALSE) +\n  scale_color_viridis_d(end = 0.9)"
  },
  {
    "objectID": "slides/01-welcome_ch1.html#model-1-main-effects-model",
    "href": "slides/01-welcome_ch1.html#model-1-main-effects-model",
    "title": "Welcome and Chapter 1",
    "section": "Model 1: Main effects model",
    "text": "Model 1: Main effects model\n\nOutputCode\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n8.197\n4.508\n1.818\n0.072\n\n\nstarters\n-0.005\n0.017\n-0.299\n0.766\n\n\nyear\n0.023\n0.002\n9.766\n0.000\n\n\nconditiongood\n-0.443\n0.231\n-1.921\n0.057\n\n\nconditionslow\n-1.543\n0.161\n-9.616\n0.000\n\n\n\n\n\n\n\n\n# Fit and display model\nmodel1 &lt;- lm(speed ~ starters + year + condition, data = derby)\ntidy(model1) |&gt; \n  kable(digits = 3)"
  },
  {
    "objectID": "slides/01-welcome_ch1.html#interpretation",
    "href": "slides/01-welcome_ch1.html#interpretation",
    "title": "Welcome and Chapter 1",
    "section": "Interpretation",
    "text": "Interpretation\n\\[\\widehat{speed} = 8.197 - 0.005 ~ starters + 0.023 ~ year - 0.443 ~ good - 1.543 ~ slow\\]\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n8.197\n4.508\n1.818\n0.072\n\n\nstarters\n-0.005\n0.017\n-0.299\n0.766\n\n\nyear\n0.023\n0.002\n9.766\n0.000\n\n\nconditiongood\n-0.443\n0.231\n-1.921\n0.057\n\n\nconditionslow\n-1.543\n0.161\n-9.616\n0.000\n\n\n\n\n\n\n\nWrite out the interpretations for starters and conditiongood.\nDoes the intercept have a meaningful interpretation?"
  },
  {
    "objectID": "slides/01-welcome_ch1.html#centering",
    "href": "slides/01-welcome_ch1.html#centering",
    "title": "Welcome and Chapter 1",
    "section": "Centering",
    "text": "Centering\nCentering: Subtract a constant from each observation of a given variable\n\nDo this to make interpretation of model parameters more meaningful (particularly intercept)\nIn STA 202, we used mean-centering where we subtracted the mean from each observation of given variable\nHow does centering change the model?"
  },
  {
    "objectID": "slides/01-welcome_ch1.html#centering-year",
    "href": "slides/01-welcome_ch1.html#centering-year",
    "title": "Welcome and Chapter 1",
    "section": "Centering year",
    "text": "Centering year\n\nderby &lt;- derby |&gt;\n  mutate(yearnew = year - 1896) #1896 = starting year\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n52.175\n0.194\n269.079\n0.000\n\n\nstarters\n-0.005\n0.017\n-0.299\n0.766\n\n\nyearnew\n0.023\n0.002\n9.766\n0.000\n\n\nconditiongood\n-0.443\n0.231\n-1.921\n0.057\n\n\nconditionslow\n-1.543\n0.161\n-9.616\n0.000\n\n\n\n\n\n\n\n\\[\\widehat{speed} = 52.175 - 0.005 ~ starters + 0.023 ~ yearnew - 0.443 ~ good - 1.543 ~ slow\\]"
  },
  {
    "objectID": "slides/01-welcome_ch1.html#model-1-check-model-assumptions",
    "href": "slides/01-welcome_ch1.html#model-1-check-model-assumptions",
    "title": "Welcome and Chapter 1",
    "section": "Model 1: Check model assumptions",
    "text": "Model 1: Check model assumptions\n\nPlotsQuestions\n\n\n\n#library(ggfortify)\nautoplot(model1Cent)\n\n\n\n\n\n\n\n\n\n\nWhich of the model assumptions (LINE) does this pass and/or fail?"
  },
  {
    "objectID": "slides/01-welcome_ch1.html#model-2-add-quadratic-effect-for-year",
    "href": "slides/01-welcome_ch1.html#model-2-add-quadratic-effect-for-year",
    "title": "Welcome and Chapter 1",
    "section": "Model 2: Add quadratic effect for year?",
    "text": "Model 2: Add quadratic effect for year?\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(data = derby, aes(x = yearnew, y = speed)) + \n  geom_point(alpha = 0.7) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"blue\") + \n  geom_smooth(se = FALSE, color = \"red\", linetype = 2) + \n  labs(x = \"Years since 1896\", y = \"Speed (ft/s)\", \n       title = \"Speed vs. Years since 1896\")"
  },
  {
    "objectID": "slides/01-welcome_ch1.html#model-2-add-yearnew2",
    "href": "slides/01-welcome_ch1.html#model-2-add-yearnew2",
    "title": "Welcome and Chapter 1",
    "section": "Model 2: Add \\(yearnew^2\\)",
    "text": "Model 2: Add \\(yearnew^2\\)\n\nPlotCode\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n51.4130\n0.1826\n281.5645\n0.0000\n\n\nstarters\n-0.0253\n0.0136\n-1.8588\n0.0656\n\n\nyearnew\n0.0700\n0.0061\n11.4239\n0.0000\n\n\nI(yearnew^2)\n-0.0004\n0.0000\n-8.0411\n0.0000\n\n\nconditiongood\n-0.4770\n0.1857\n-2.5689\n0.0115\n\n\nconditionslow\n-1.3927\n0.1305\n-10.6701\n0.0000\n\n\n\n\n\n\n\n\nmodel2 &lt;- lm(speed ~ starters + yearnew + I(yearnew^2) + condition, \n             data = derby)\ntidy(model2) |&gt; kable(digits = 4)"
  },
  {
    "objectID": "slides/01-welcome_ch1.html#interpreting-quadratic-effects",
    "href": "slides/01-welcome_ch1.html#interpreting-quadratic-effects",
    "title": "Welcome and Chapter 1",
    "section": "Interpreting quadratic effects",
    "text": "Interpreting quadratic effects\n\\[\\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 ~ x_1  + \\hat{\\beta}_2 ~ x_2 + \\hat{\\beta}_3 ~ x_2^2\\]\nGeneral interpretation: When \\(x_2\\) increases from a to b, \\(y\\) is expected to change by \\(\\hat{\\beta}_2(b - a) + \\hat{\\beta}_3(b^2 - a^2)\\), holding \\(x_1\\) constant."
  },
  {
    "objectID": "slides/01-welcome_ch1.html#interpreting-quadratic-effects-1",
    "href": "slides/01-welcome_ch1.html#interpreting-quadratic-effects-1",
    "title": "Welcome and Chapter 1",
    "section": "Interpreting quadratic effects",
    "text": "Interpreting quadratic effects\n\\[\\begin{aligned}\\widehat{speed} = &51.413 - 0.025 ~ starters + 0.070 ~ yearnew \\\\\n& - 0.0004 ~ yearnew^2 - 0.477 ~ good - 1.393 ~ slow\\end{aligned}\\]\n\nQuestions:\nInterpret the effect of year for the 5 most recent years (2013 - 2017)."
  },
  {
    "objectID": "slides/01-welcome_ch1.html#model-2-check-model-assumptions",
    "href": "slides/01-welcome_ch1.html#model-2-check-model-assumptions",
    "title": "Welcome and Chapter 1",
    "section": "Model 2: Check model assumptions",
    "text": "Model 2: Check model assumptions"
  },
  {
    "objectID": "slides/01-welcome_ch1.html#model-3-include-interaction-term",
    "href": "slides/01-welcome_ch1.html#model-3-include-interaction-term",
    "title": "Welcome and Chapter 1",
    "section": "Model 3: Include interaction term?",
    "text": "Model 3: Include interaction term?\nRecall from the EDA…\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlibrary(viridis)\nggplot(data = derby, aes(x = year, y = speed, color = condition, \n                         shape = condition, linetype = condition)) + \n  geom_point() + \n  geom_smooth(method = \"lm\", se = FALSE, aes(linetype = condition)) + \n  labs(x = \"Year\", y = \"Speed (ft/s)\", color = \"Condition\",\n       title = \"Speed vs. year\", \n       subtitle = \"by track condition\") +\n  guides(lty = FALSE, shape = FALSE) +\n  scale_color_viridis_d(end = 0.9)"
  },
  {
    "objectID": "slides/01-welcome_ch1.html#model-3-add-interaction-term",
    "href": "slides/01-welcome_ch1.html#model-3-add-interaction-term",
    "title": "Welcome and Chapter 1",
    "section": "Model 3: Add interaction term",
    "text": "Model 3: Add interaction term\n\\[\\begin{aligned}\\widehat{speed} = & 52.387 - 0.003 ~ starters + 0.020 ~ yearnew - 1.070 ~ good - 2.183 ~ slow \\\\ &+0.012 ~ yearnew \\times good + 0.012 ~ yearnew \\times slow \\end{aligned}\\]\n\nOutputCodeAssumptions\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n52.387\n0.200\n262.350\n0.000\n\n\nstarters\n-0.003\n0.016\n-0.189\n0.850\n\n\nyearnew\n0.020\n0.003\n7.576\n0.000\n\n\nconditiongood\n-1.070\n0.423\n-2.527\n0.013\n\n\nconditionslow\n-2.183\n0.270\n-8.097\n0.000\n\n\nyearnew:conditiongood\n0.012\n0.008\n1.598\n0.113\n\n\nyearnew:conditionslow\n0.012\n0.004\n2.866\n0.005\n\n\n\n\n\n\n\n\nmodel3 &lt;- lm(speed ~ starters + yearnew + condition +\n               yearnew * condition, \n             data = derby)\ntidy(model3) |&gt; kable(digits = 4)"
  },
  {
    "objectID": "slides/01-welcome_ch1.html#interpreting-interaction-effects",
    "href": "slides/01-welcome_ch1.html#interpreting-interaction-effects",
    "title": "Welcome and Chapter 1",
    "section": "Interpreting interaction effects",
    "text": "Interpreting interaction effects\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n52.387\n0.200\n262.350\n0.000\n\n\nstarters\n-0.003\n0.016\n-0.189\n0.850\n\n\nyearnew\n0.020\n0.003\n7.576\n0.000\n\n\nconditiongood\n-1.070\n0.423\n-2.527\n0.013\n\n\nconditionslow\n-2.183\n0.270\n-8.097\n0.000\n\n\nyearnew:conditiongood\n0.012\n0.008\n1.598\n0.113\n\n\nyearnew:conditionslow\n0.012\n0.004\n2.866\n0.005\n\n\n\n\n\n\nWrite out the interpretation of…"
  },
  {
    "objectID": "slides/01-welcome_ch1.html#which-model-would-you-choose",
    "href": "slides/01-welcome_ch1.html#which-model-would-you-choose",
    "title": "Welcome and Chapter 1",
    "section": "Which model would you choose?",
    "text": "Which model would you choose?\n\nModel 1: Main effectsModel 2: Main effects + \\(year^2\\)Model 3: Main effects + interactionCode\n\n\n\n\n\n\n\nr.squared\nadj.r.squared\nAIC\nBIC\n\n\n\n\n0.73\n0.721\n259.478\n276.302\n\n\n\n\n\n\n\n\n\n\n\n\nr.squared\nadj.r.squared\nAIC\nBIC\n\n\n\n\n0.827\n0.819\n207.429\n227.057\n\n\n\n\n\n\n\n\n\n\n\n\nr.squared\nadj.r.squared\nAIC\nBIC\n\n\n\n\n0.751\n0.738\n253.584\n276.016\n\n\n\n\n\n\n\n\n# Model 1\nglance(model1Cent) |&gt;\n  select(r.squared, adj.r.squared, AIC, BIC) |&gt;\n  kable(digits = 3)\n\n# Model2\nglance(model2) |&gt;\n  select(r.squared, adj.r.squared, AIC, BIC) |&gt;\n  kable(digits = 3)\n\n# Model 3\nglance(model3) |&gt;\n  select(r.squared, adj.r.squared, AIC, BIC) |&gt;\n  kable(digits = 3)"
  },
  {
    "objectID": "slides/01-welcome_ch1.html#what-are-these-model-quality-metrics",
    "href": "slides/01-welcome_ch1.html#what-are-these-model-quality-metrics",
    "title": "Welcome and Chapter 1",
    "section": "What are these model quality metrics?",
    "text": "What are these model quality metrics?\n\nHow do we define RSquared?\nWhat is adj.r.squared?"
  },
  {
    "objectID": "slides/01-welcome_ch1.html#measures-of-model-performance",
    "href": "slides/01-welcome_ch1.html#measures-of-model-performance",
    "title": "Welcome and Chapter 1",
    "section": "Measures of model performance",
    "text": "Measures of model performance\n\n\\(\\color{#4187aa}{R^2}\\): Proportion of variability in the response explained by the model.\n\nWill always increase as predictors are added, so it shouldn’t be used to compare models\n\n\\(\\color{#4187aa}{Adj. R^2}\\): Similar to \\(R^2\\) with a penalty for extra terms\n\n\n\n\\(\\color{#4187aa}{AIC}\\): Likelihood-based approach balancing model performance and complexity\n\\(\\color{#4187aa}{BIC}\\): Similar to AIC with stronger penalty for extra terms\n\n\n\n\nNested F Test (extra sum of squares F test): Generalization of t-test for individual coefficients to perform significance tests on nested models"
  },
  {
    "objectID": "slides/01-welcome_ch1.html#which-model-would-you-choose-1",
    "href": "slides/01-welcome_ch1.html#which-model-would-you-choose-1",
    "title": "Welcome and Chapter 1",
    "section": "Which model would you choose?",
    "text": "Which model would you choose?\nUse the glance function to get model statistics.\n\nOutputCode\n\n\n\n\n\n\n\nmodel\nr.squared\nadj.r.squared\nAIC\nBIC\n\n\n\n\nModel1\n0.730\n0.721\n259.478\n276.302\n\n\nModel2\n0.827\n0.819\n207.429\n227.057\n\n\nModel3\n0.751\n0.738\n253.584\n276.016\n\n\n\n\n\n\n\n\nmodel1_glance &lt;- glance(model1Cent) |&gt;\n  select(r.squared, adj.r.squared, AIC, BIC)\nmodel2_glance &lt;- glance(model2) |&gt;\n  select(r.squared, adj.r.squared, AIC, BIC)\nmodel3_glance &lt;- glance(model3) |&gt;\n  select(r.squared, adj.r.squared, AIC, BIC)\n\nmodel1_glance |&gt;\n  bind_rows(model2_glance) |&gt;\n  bind_rows(model3_glance) |&gt;\n  bind_cols(model = c(\"Model1\", \"Model2\", \"Model3\")) |&gt;\n  select(model, everything()) |&gt;\nkable(digits = 3)"
  },
  {
    "objectID": "slides/01-welcome_ch1.html#characteristics-of-a-good-final-model",
    "href": "slides/01-welcome_ch1.html#characteristics-of-a-good-final-model",
    "title": "Welcome and Chapter 1",
    "section": "Characteristics of a “good” final model",
    "text": "Characteristics of a “good” final model\n\nModel can be used to answer primary research questions\nPredictor variables control for important covariates\nPotential interactions have been investigated\nVariables are centered, as needed, for more meaningful interpretations\nunnecessary terms are removed\nAssumptions are met and influential points have been addressed\nmodel tells a “persuasive story parsimoniously”\n\n\n\nList from Section 1.6.7 of BMLR"
  },
  {
    "objectID": "slides/01-welcome_ch1.html#inference-for-multiple-linear-regression",
    "href": "slides/01-welcome_ch1.html#inference-for-multiple-linear-regression",
    "title": "Welcome and Chapter 1",
    "section": "Inference for multiple linear regression",
    "text": "Inference for multiple linear regression\nUse statistical inference to\n\nDetermine if predictors are statistically significant (not necessarily practically significant!)\nQuantify uncertainty in coefficient estimates\nQuantify uncertainty in model predictions\n\n\nIf L.I.N.E. assumptions are met, we can conduct inference using the \\(t\\) distribution and estimated standard errors"
  },
  {
    "objectID": "slides/01-welcome_ch1.html#inference-for-regression",
    "href": "slides/01-welcome_ch1.html#inference-for-regression",
    "title": "Welcome and Chapter 1",
    "section": "Inference for regression",
    "text": "Inference for regression\n\nWhen L.I.N.E. conditions are metWe can\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUse least squares regression to get the estimates \\(\\hat{\\beta}_0\\), \\(\\hat{\\beta}_1\\), and \\(\\hat{\\sigma}^2\\)\n\\(\\hat{\\sigma}\\) is the regression standard error\n\n. . .\n\\[\\hat{\\sigma} = \\sqrt{\\frac{\\sum_{i=1}^n(y_i - \\hat{y}_i)^2}{n - p - 1}} = \\sqrt{\\frac{\\sum_{i=1}^n e_i^2}{n-p-1}}\\]"
  },
  {
    "objectID": "slides/01-welcome_ch1.html#acknowledgements",
    "href": "slides/01-welcome_ch1.html#acknowledgements",
    "title": "Welcome and Chapter 1",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThese slides are based on content in BMLR: Chapter 1 - Review of Multiple Linear Regression\nInitial versions of the slides are by Dr. Maria Tackett, Duke University"
  },
  {
    "objectID": "slides/01-welcome_ch1_o.html",
    "href": "slides/01-welcome_ch1_o.html",
    "title": "Welcome and Chapter 1",
    "section": "",
    "text": "Dr. Tyler George: tgeorge@cornellcollege.edu\n\n\n\n\n\nCourse Dates: August 26th to September 18th\nCourse sessions: By Appointment\nExam Dates: tbd\n\n\n\n\nIn statistics, a generalized linear model (GLM) is a flexible generalization of ordinary linear regression. The GLM generalizes linear regression by allowing the linear model to be related to the response variable via a link function and by allowing the magnitude of the variance of each measurement to be a function of its predicted value.\n\n\nWikipedia\n. . .\nLogistic regression\n\\[\\begin{aligned}\\pi = P(y = 1 | x) \\hspace{2mm} &\\Rightarrow \\hspace{2mm} \\text{Link function: } \\log\\big(\\frac{\\pi}{1-\\pi}\\big) \\\\\n&\\Rightarrow \\log\\big(\\frac{\\pi}{1-\\pi}\\big) = \\beta_0 + \\beta_1~x\\end{aligned}\\]\n\n\n\nGeneralized Linear Models (Ch 1 - 6)\n\nIntroduce models for non-normal response variables\nEstimation, interpretation, and inference\nMathematical details showing how GLMs are connected\n\n\n\n\nModeling correlated data (Ch 7 - 9)\n\nIntroduce multilevel models for correlated and longitudinal data\nEstimation, interpretation, and inference\nMathematical details, particularly diving into covariance structures\n\n\n\n\nMore Regression Models (ITSL Chapter 7) - Polynomial Regression - Regression Splines - Smoothing Splines - Generalized Additive Models (GAMS)\n\n\n\n\nCreate larger groups\nQuick introductions - Name, year, and major\nChoose a reporter\n\nNeed help choosing? Person with birthday closest to December 1st.\n\nIdentify 8 things everyone in the group has in common\n\nNot being a Cornell Student\nNot clothes (we’re all wearing socks)\nNot body parts (we all have a nose)\n\n\n. . .\nReporter will share list with the class\n\n\n\n. . .\nPre-reqs\n\nSTA 201, 202 and DSC 223\n\n. . .\nBackground knowledge\n\n\n\nStatistical content\n\nLinear and logistic regression\nStatistical inference\nBasic understanding of random variables\n\n\n\n\nComputing\n\nUsing R for data analysis\nWriting reports using R Markdown or Quarto\n\n\n\n\n\n\n\n\nWebsite\n\nhttps://stats-tgeorge.github.io/STA363_AdvReg/\nCentral hub for the course\nNotes\nLabs\nDatasets\n\n\n\n\n\n\nMoodle:\n\nhttps://moodle.cornellcollege.edu/course/view.php?id=7908\nSubmissions\nGradebook\nAnnouncements\n\n\n\n\n\nLectures\n\nSome traditional lecture\nIndividual and group labs\nBring fully-charged laptop\nMini-projects\nExams\n\n. . .\nAttendance is expected (if you are healthy!)\n\n\n\n\n\n\n\nBeyond Multiple Linear Regression by Paul Roback and Julie Legler\n\nAvailable online\nHard copies available for purchase\n\n\n\n\n\n\nThe secondary text is: An Introduction to Statistical Learning with Applications in R by Gareth James, Daniela Witten, Trevor Hastie, and Robert Tibshirani – it is freely available online. Chapter 7.\n\nHard copies available for purchase\n\n\n\n\n\nRStudio Server is installed and should be used\nhttp://turing.cornellcollege.edu:8787/\n\n\n\n\nReadings\n\nPrimarily from Beyond Multiple Linear Regression\nRecommend reading assigned text before lecture\n\n. . .\nHomework - Primarily from Beyond Multiple Linear Regression - Individual assignments - Work together but must complete your own work. Discuss but don’t copy.\n\n\n\nMini-projects\nExamples:\n\nMini-project 01: Focused on models for non-normal response variables, such as count data\nMini-project 02: Focused on models for correlated data\n\n. . .\n\nShort write up and short presentation\nTeam-based\n\n\n\n\n\nTwo exams this block, tbd.\nEach will have two components\n\nComponent 1 will be on these dates and you will get a choice of oral or written format.\nComponent 2 will be a take-home, open-book, open-note, exam.\nYou will have 12 hours or more to complete this component.\n\n\n\n\n\nFinal grades will be calculated as follows\n\n\n\nCategory\nPoints\n\n\n\n\nHomework\n200\n\n\nParticipation\n100\n\n\nLabs and Mini Projects\n300\n\n\nExams\n400\n\n\nTotal\n1000\n\n\n\nSee Syllabus on website for letter grade thresholds.\n\n\n\n\nOffice hours to meet with your instructor in West 311\n\nTypically MWTh 3:05pm-4:05pm and by appt.\nDouble check course calendar\nMake appointments by going to https://calendar.app.google/oHqiU2FZDJLXKq429\n\nEmail Tyler George for private questions regarding personal matters or grades.\n\nPlease put STA 363 in the subject line since I am also teaching capstone this semester\n\nCollege support at https://stats-tgeorge.github.io/personal_website/course-support.html."
  },
  {
    "objectID": "slides/01-welcome_ch1_o.html#instructor",
    "href": "slides/01-welcome_ch1_o.html#instructor",
    "title": "Welcome and Chapter 1",
    "section": "",
    "text": "Dr. Tyler George: tgeorge@cornellcollege.edu"
  },
  {
    "objectID": "slides/01-welcome_ch1_o.html#course-logistics",
    "href": "slides/01-welcome_ch1_o.html#course-logistics",
    "title": "Welcome and Chapter 1",
    "section": "",
    "text": "Course Dates: August 26th to September 18th\nCourse sessions: By Appointment\nExam Dates: tbd"
  },
  {
    "objectID": "slides/01-welcome_ch1_o.html#generalized-linear-models",
    "href": "slides/01-welcome_ch1_o.html#generalized-linear-models",
    "title": "Welcome and Chapter 1",
    "section": "",
    "text": "In statistics, a generalized linear model (GLM) is a flexible generalization of ordinary linear regression. The GLM generalizes linear regression by allowing the linear model to be related to the response variable via a link function and by allowing the magnitude of the variance of each measurement to be a function of its predicted value.\n\n\nWikipedia\n. . .\nLogistic regression\n\\[\\begin{aligned}\\pi = P(y = 1 | x) \\hspace{2mm} &\\Rightarrow \\hspace{2mm} \\text{Link function: } \\log\\big(\\frac{\\pi}{1-\\pi}\\big) \\\\\n&\\Rightarrow \\log\\big(\\frac{\\pi}{1-\\pi}\\big) = \\beta_0 + \\beta_1~x\\end{aligned}\\]"
  },
  {
    "objectID": "slides/01-welcome_ch1_o.html#what-were-covering-this-semester13",
    "href": "slides/01-welcome_ch1_o.html#what-were-covering-this-semester13",
    "title": "Welcome and Chapter 1",
    "section": "",
    "text": "Generalized Linear Models (Ch 1 - 6)\n\nIntroduce models for non-normal response variables\nEstimation, interpretation, and inference\nMathematical details showing how GLMs are connected"
  },
  {
    "objectID": "slides/01-welcome_ch1_o.html#what-were-covering-this-semester23",
    "href": "slides/01-welcome_ch1_o.html#what-were-covering-this-semester23",
    "title": "Welcome and Chapter 1",
    "section": "",
    "text": "Modeling correlated data (Ch 7 - 9)\n\nIntroduce multilevel models for correlated and longitudinal data\nEstimation, interpretation, and inference\nMathematical details, particularly diving into covariance structures"
  },
  {
    "objectID": "slides/01-welcome_ch1_o.html#what-were-covering-this-semester33",
    "href": "slides/01-welcome_ch1_o.html#what-were-covering-this-semester33",
    "title": "Welcome and Chapter 1",
    "section": "",
    "text": "More Regression Models (ITSL Chapter 7) - Polynomial Regression - Regression Splines - Smoothing Splines - Generalized Additive Models (GAMS)"
  },
  {
    "objectID": "slides/01-welcome_ch1_o.html#meet-your-classmates",
    "href": "slides/01-welcome_ch1_o.html#meet-your-classmates",
    "title": "Welcome and Chapter 1",
    "section": "",
    "text": "Create larger groups\nQuick introductions - Name, year, and major\nChoose a reporter\n\nNeed help choosing? Person with birthday closest to December 1st.\n\nIdentify 8 things everyone in the group has in common\n\nNot being a Cornell Student\nNot clothes (we’re all wearing socks)\nNot body parts (we all have a nose)\n\n\n. . .\nReporter will share list with the class"
  },
  {
    "objectID": "slides/01-welcome_ch1_o.html#what-background-is-assumed-for-the-course",
    "href": "slides/01-welcome_ch1_o.html#what-background-is-assumed-for-the-course",
    "title": "Welcome and Chapter 1",
    "section": "",
    "text": ". . .\nPre-reqs\n\nSTA 201, 202 and DSC 223\n\n. . .\nBackground knowledge\n\n\n\nStatistical content\n\nLinear and logistic regression\nStatistical inference\nBasic understanding of random variables\n\n\n\n\nComputing\n\nUsing R for data analysis\nWriting reports using R Markdown or Quarto"
  },
  {
    "objectID": "slides/01-welcome_ch1_o.html#course-toolkit-12",
    "href": "slides/01-welcome_ch1_o.html#course-toolkit-12",
    "title": "Welcome and Chapter 1",
    "section": "",
    "text": "Website\n\nhttps://stats-tgeorge.github.io/STA363_AdvReg/\nCentral hub for the course\nNotes\nLabs\nDatasets"
  },
  {
    "objectID": "slides/01-welcome_ch1_o.html#course-toolkit-12-1",
    "href": "slides/01-welcome_ch1_o.html#course-toolkit-12-1",
    "title": "Welcome and Chapter 1",
    "section": "",
    "text": "Moodle:\n\nhttps://moodle.cornellcollege.edu/course/view.php?id=7908\nSubmissions\nGradebook\nAnnouncements"
  },
  {
    "objectID": "slides/01-welcome_ch1_o.html#class-meetings",
    "href": "slides/01-welcome_ch1_o.html#class-meetings",
    "title": "Welcome and Chapter 1",
    "section": "",
    "text": "Lectures\n\nSome traditional lecture\nIndividual and group labs\nBring fully-charged laptop\nMini-projects\nExams\n\n. . .\nAttendance is expected (if you are healthy!)"
  },
  {
    "objectID": "slides/01-welcome_ch1_o.html#textbook",
    "href": "slides/01-welcome_ch1_o.html#textbook",
    "title": "Welcome and Chapter 1",
    "section": "",
    "text": "Beyond Multiple Linear Regression by Paul Roback and Julie Legler\n\nAvailable online\nHard copies available for purchase"
  },
  {
    "objectID": "slides/01-welcome_ch1_o.html#textbook-2",
    "href": "slides/01-welcome_ch1_o.html#textbook-2",
    "title": "Welcome and Chapter 1",
    "section": "",
    "text": "The secondary text is: An Introduction to Statistical Learning with Applications in R by Gareth James, Daniela Witten, Trevor Hastie, and Robert Tibshirani – it is freely available online. Chapter 7.\n\nHard copies available for purchase"
  },
  {
    "objectID": "slides/01-welcome_ch1_o.html#using-r-rstudio",
    "href": "slides/01-welcome_ch1_o.html#using-r-rstudio",
    "title": "Welcome and Chapter 1",
    "section": "",
    "text": "RStudio Server is installed and should be used\nhttp://turing.cornellcollege.edu:8787/"
  },
  {
    "objectID": "slides/01-welcome_ch1_o.html#activities-assessments",
    "href": "slides/01-welcome_ch1_o.html#activities-assessments",
    "title": "Welcome and Chapter 1",
    "section": "",
    "text": "Readings\n\nPrimarily from Beyond Multiple Linear Regression\nRecommend reading assigned text before lecture\n\n. . .\nHomework - Primarily from Beyond Multiple Linear Regression - Individual assignments - Work together but must complete your own work. Discuss but don’t copy."
  },
  {
    "objectID": "slides/01-welcome_ch1_o.html#activities-assessments-1",
    "href": "slides/01-welcome_ch1_o.html#activities-assessments-1",
    "title": "Welcome and Chapter 1",
    "section": "",
    "text": "Mini-projects\nExamples:\n\nMini-project 01: Focused on models for non-normal response variables, such as count data\nMini-project 02: Focused on models for correlated data\n\n. . .\n\nShort write up and short presentation\nTeam-based"
  },
  {
    "objectID": "slides/01-welcome_ch1_o.html#exams",
    "href": "slides/01-welcome_ch1_o.html#exams",
    "title": "Welcome and Chapter 1",
    "section": "",
    "text": "Two exams this block, tbd.\nEach will have two components\n\nComponent 1 will be on these dates and you will get a choice of oral or written format.\nComponent 2 will be a take-home, open-book, open-note, exam.\nYou will have 12 hours or more to complete this component."
  },
  {
    "objectID": "slides/01-welcome_ch1_o.html#grading",
    "href": "slides/01-welcome_ch1_o.html#grading",
    "title": "Welcome and Chapter 1",
    "section": "",
    "text": "Final grades will be calculated as follows\n\n\n\nCategory\nPoints\n\n\n\n\nHomework\n200\n\n\nParticipation\n100\n\n\nLabs and Mini Projects\n300\n\n\nExams\n400\n\n\nTotal\n1000\n\n\n\nSee Syllabus on website for letter grade thresholds."
  },
  {
    "objectID": "slides/01-welcome_ch1_o.html#resources",
    "href": "slides/01-welcome_ch1_o.html#resources",
    "title": "Welcome and Chapter 1",
    "section": "",
    "text": "Office hours to meet with your instructor in West 311\n\nTypically MWTh 3:05pm-4:05pm and by appt.\nDouble check course calendar\nMake appointments by going to https://calendar.app.google/oHqiU2FZDJLXKq429\n\nEmail Tyler George for private questions regarding personal matters or grades.\n\nPlease put STA 363 in the subject line since I am also teaching capstone this semester\n\nCollege support at https://stats-tgeorge.github.io/personal_website/course-support.html."
  },
  {
    "objectID": "slides/01-welcome_ch1_o.html#setup---r-packages",
    "href": "slides/01-welcome_ch1_o.html#setup---r-packages",
    "title": "Welcome and Chapter 1",
    "section": "Setup - R Packages",
    "text": "Setup - R Packages\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(GGally)\nlibrary(knitr)\nlibrary(patchwork)\nlibrary(viridis)\nlibrary(ggfortify)"
  },
  {
    "objectID": "slides/01-welcome_ch1_o.html#assumptions-for-linear-regression",
    "href": "slides/01-welcome_ch1_o.html#assumptions-for-linear-regression",
    "title": "Welcome and Chapter 1",
    "section": "Assumptions for linear regression",
    "text": "Assumptions for linear regression\nWhat are the assumptions for linear regression? . . .\nLinearity: Linear relationship between mean response and predictor variable(s)\n. . .\nIndependence: Residuals are independent. There is no connection between how far any two points lie above or below regression line.\n. . .\nNormality: Response follows a normal distribution at each level of the predictor (or combination of predictors)\n. . .\nEqual variance: Variability (variance or standard deviation) of the response is equal for all levels of the predictor (or combination of predictors)\n. . .\nUse residual plots to check that the conditions hold before using the model for statistical inference."
  },
  {
    "objectID": "slides/01-welcome_ch1_o.html#assumptions-for-linear-regression-1",
    "href": "slides/01-welcome_ch1_o.html#assumptions-for-linear-regression-1",
    "title": "Welcome and Chapter 1",
    "section": "Assumptions for linear regression",
    "text": "Assumptions for linear regression\n\n\n\n\n\n\n\n\n\n\n\n\n\nModified from Figure 1.1. in BMLR]\n\n\nLinearity: Linear relationship between mean of the response \\(Y\\) and the predictor \\(X\\)\nIndependence: No connection between how any two points lie above or below the regression line\nNormality: Response, \\(Y\\), follows a normal distribution at each level of the predictor, \\(X\\) (indicated by red curves)\nEqual variance: Variance (or standard deviation) of the response, \\(Y\\), is equal for all levels of the predictor, \\(X\\)"
  },
  {
    "objectID": "slides/01-welcome_ch1_o.html#questions",
    "href": "slides/01-welcome_ch1_o.html#questions",
    "title": "Welcome and Chapter 1",
    "section": "Questions",
    "text": "Questions\nHow do we assess these conditions?"
  },
  {
    "objectID": "slides/01-welcome_ch1_o.html#beyond-linear-regression",
    "href": "slides/01-welcome_ch1_o.html#beyond-linear-regression",
    "title": "Welcome and Chapter 1",
    "section": "Beyond linear regression",
    "text": "Beyond linear regression\n\nWhen we use linear least squares regression to draw conclusions, we do so under the assumption that L.I.N.E. are all met.\nGeneralized linear models require different assumptions and can accommodate violations in L.I.N.E.\n\nRelationship between response and predictor(s) can be nonlinear\nResponse variable can be non-normal\nVariance in response can differ at each level of predictor(s)\n\n\n. . .\nBut the independence assumption must hold!\n\nMultilevel models will be used for data with correlated observations"
  },
  {
    "objectID": "slides/01-welcome_ch1_o.html#review-of-multiple-linear-regression",
    "href": "slides/01-welcome_ch1_o.html#review-of-multiple-linear-regression",
    "title": "Welcome and Chapter 1",
    "section": "Review of multiple linear regression",
    "text": "Review of multiple linear regression"
  },
  {
    "objectID": "slides/01-welcome_ch1_o.html#data-kentucky-derby-winners",
    "href": "slides/01-welcome_ch1_o.html#data-kentucky-derby-winners",
    "title": "Welcome and Chapter 1",
    "section": "Data: Kentucky Derby Winners",
    "text": "Data: Kentucky Derby Winners\nToday’s data is from the Kentucky Derby, an annual 1.25-mile horse race held at the Churchill Downs race track in Louisville, KY. The data is in the file derbyplus.csv and contains information for races 1896 - 2017.\n. . .\n\n\nResponse variable\n\nspeed: Average speed of the winner in feet per second (ft/s)]\n\nAdditional variable\n\nwinner: Winning horse\n\n\nPredictor variables\n\nyear: Year of the race\ncondition: Condition of the track (good, fast, slow)\nstarters: Number of horses who raced]"
  },
  {
    "objectID": "slides/01-welcome_ch1_o.html#data",
    "href": "slides/01-welcome_ch1_o.html#data",
    "title": "Welcome and Chapter 1",
    "section": "Data",
    "text": "Data\n\nderby &lt;- read_csv(\"data/derbyplus.csv\")\n\n\nderby |&gt;\n  head(5) |&gt; kable()\n\n\n\n\nyear\nwinner\ncondition\nspeed\nstarters\n\n\n\n\n1896\nBen Brush\ngood\n51.66\n8\n\n\n1897\nTyphoon II\nslow\n49.81\n6\n\n\n1898\nPlaudit\ngood\n51.16\n4\n\n\n1899\nManuel\nfast\n50.00\n5\n\n\n1900\nLieut. Gibson\nfast\n52.28\n7"
  },
  {
    "objectID": "slides/01-welcome_ch1_o.html#data-analysis-life-cycle",
    "href": "slides/01-welcome_ch1_o.html#data-analysis-life-cycle",
    "title": "Welcome and Chapter 1",
    "section": "Data Analysis Life Cycle",
    "text": "Data Analysis Life Cycle"
  },
  {
    "objectID": "slides/01-welcome_ch1_o.html#exploratory-data-analysis-eda",
    "href": "slides/01-welcome_ch1_o.html#exploratory-data-analysis-eda",
    "title": "Welcome and Chapter 1",
    "section": "Exploratory data analysis (EDA)",
    "text": "Exploratory data analysis (EDA)\n\nOnce you’re ready for the statistical analysis (explore), the first step should always be exploratory data analysis.\nThe EDA will help you\n\nbegin to understand the variables and observations\nidentify outliers or potential data entry errors\nbegin to see relationships between variables\nidentify the appropriate model and identify a strategy\n\nThe EDA is exploratory; formal modeling and statistical inference should be used to draw conclusions."
  },
  {
    "objectID": "slides/01-welcome_ch1_o.html#plots-for-univariate-eda",
    "href": "slides/01-welcome_ch1_o.html#plots-for-univariate-eda",
    "title": "Welcome and Chapter 1",
    "section": "Plots for univariate EDA",
    "text": "Plots for univariate EDA\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\np1 &lt;- ggplot(data = derby, aes(x = speed)) + \n  geom_histogram(fill = \"forestgreen\", color = \"black\") + \n  labs(x = \"Winning speed (ft/s)\", y = \"Count\")\n\np2 &lt;- ggplot(data = derby, aes(x = starters)) + \n  geom_histogram(fill = \"forestgreen\", color = \"black\") + \n  labs(x = \"Starters\", y = \"Count\")\n\np3 &lt;- ggplot(data = derby, aes(x = condition)) +\n   geom_bar(fill = \"forestgreen\", color = \"black\", aes(x = ))\n\np1 + (p2 / p3) + \n  plot_annotation(title = \"Univariate data analysis\")"
  },
  {
    "objectID": "slides/01-welcome_ch1_o.html#plots-for-bivariate-eda",
    "href": "slides/01-welcome_ch1_o.html#plots-for-bivariate-eda",
    "title": "Welcome and Chapter 1",
    "section": "Plots for bivariate EDA",
    "text": "Plots for bivariate EDA\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\np4 &lt;- ggplot(data = derby, aes(x = starters, y = speed)) + \n  geom_point() + \n  labs(x = \"Starters\", y = \"Speed (ft / s)\")\n\np5 &lt;- ggplot(data = derby, aes(x = year, y = speed)) + \n  geom_point() + \n  labs(x = \"Year\", y = \"Speed (ft / s)\")\n\np6 &lt;- ggplot(data = derby, aes(x = condition, y = speed)) + \n  geom_boxplot(fill = \"forestgreen\", color = \"black\") + \n  labs(x = \"Conditions\", y = \"Speed (ft / s)\")\n\n(p4 + p5) + p6 +\n  plot_annotation(title = \"Bivariate data analysis\")"
  },
  {
    "objectID": "slides/01-welcome_ch1_o.html#scatterplot-matrix",
    "href": "slides/01-welcome_ch1_o.html#scatterplot-matrix",
    "title": "Welcome and Chapter 1",
    "section": "Scatterplot matrix",
    "text": "Scatterplot matrix\nA scatterplot matrix helps quickly visualize relationships between many variable pairs. They are particularly useful to identify potentially correlated predictors.\n. . .\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n#library(GGally)\nggpairs(data = derby, \n        columns = c(\"condition\", \"year\", \"starters\", \"speed\"))"
  },
  {
    "objectID": "slides/01-welcome_ch1_o.html#plots-for-multivariate-eda",
    "href": "slides/01-welcome_ch1_o.html#plots-for-multivariate-eda",
    "title": "Welcome and Chapter 1",
    "section": "Plots for multivariate EDA",
    "text": "Plots for multivariate EDA\nPlot the relationship between the response and a predictor based on levels of another predictor to assess potential interactions.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n#library(viridis)\nggplot(data = derby, aes(x = year, y = speed, color = condition, \n                         shape = condition, linetype = condition)) + \n  geom_point() + \n  geom_smooth(method = \"lm\", se = FALSE, aes(linetype = condition)) + \n  labs(x = \"Year\", y = \"Speed (ft/s)\", color = \"Condition\",\n       title = \"Speed vs. year\", \n       subtitle = \"by track condition\") +\n  guides(lty = FALSE, shape = FALSE) +\n  scale_color_viridis_d(end = 0.9)"
  },
  {
    "objectID": "slides/01-welcome_ch1_o.html#model-1-main-effects-model",
    "href": "slides/01-welcome_ch1_o.html#model-1-main-effects-model",
    "title": "Welcome and Chapter 1",
    "section": "Model 1: Main effects model",
    "text": "Model 1: Main effects model\n\nOutputCode\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n8.197\n4.508\n1.818\n0.072\n\n\nstarters\n-0.005\n0.017\n-0.299\n0.766\n\n\nyear\n0.023\n0.002\n9.766\n0.000\n\n\nconditiongood\n-0.443\n0.231\n-1.921\n0.057\n\n\nconditionslow\n-1.543\n0.161\n-9.616\n0.000\n\n\n\n\n\n\n\n\n# Fit and display model\nmodel1 &lt;- lm(speed ~ starters + year + condition, data = derby)\ntidy(model1) |&gt; \n  kable(digits = 3)"
  },
  {
    "objectID": "slides/01-welcome_ch1_o.html#interpretation",
    "href": "slides/01-welcome_ch1_o.html#interpretation",
    "title": "Welcome and Chapter 1",
    "section": "Interpretation",
    "text": "Interpretation\n\\[\\widehat{speed} = 8.197 - 0.005 ~ starters + 0.023 ~ year - 0.443 ~ good - 1.543 ~ slow\\]\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n8.197\n4.508\n1.818\n0.072\n\n\nstarters\n-0.005\n0.017\n-0.299\n0.766\n\n\nyear\n0.023\n0.002\n9.766\n0.000\n\n\nconditiongood\n-0.443\n0.231\n-1.921\n0.057\n\n\nconditionslow\n-1.543\n0.161\n-9.616\n0.000\n\n\n\n\n\n. . .\n\nWrite out the interpretations for starters and conditiongood.\nDoes the intercept have a meaningful interpretation?"
  },
  {
    "objectID": "slides/01-welcome_ch1_o.html#centering",
    "href": "slides/01-welcome_ch1_o.html#centering",
    "title": "Welcome and Chapter 1",
    "section": "Centering",
    "text": "Centering\nCentering: Subtract a constant from each observation of a given variable\n\nDo this to make interpretation of model parameters more meaningful (particularly intercept)\nIn STA 202, we used mean-centering where we subtracted the mean from each observation of given variable\nHow does centering change the model?"
  },
  {
    "objectID": "slides/01-welcome_ch1_o.html#centering-year",
    "href": "slides/01-welcome_ch1_o.html#centering-year",
    "title": "Welcome and Chapter 1",
    "section": "Centering year",
    "text": "Centering year\n\nderby &lt;- derby |&gt;\n  mutate(yearnew = year - 1896) #1896 = starting year\n\n. . .\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n52.175\n0.194\n269.079\n0.000\n\n\nstarters\n-0.005\n0.017\n-0.299\n0.766\n\n\nyearnew\n0.023\n0.002\n9.766\n0.000\n\n\nconditiongood\n-0.443\n0.231\n-1.921\n0.057\n\n\nconditionslow\n-1.543\n0.161\n-9.616\n0.000\n\n\n\n\n\n. . .\n\\[\\widehat{speed} = 52.175 - 0.005 ~ starters + 0.023 ~ yearnew - 0.443 ~ good - 1.543 ~ slow\\]"
  },
  {
    "objectID": "slides/01-welcome_ch1_o.html#model-1-check-model-assumptions",
    "href": "slides/01-welcome_ch1_o.html#model-1-check-model-assumptions",
    "title": "Welcome and Chapter 1",
    "section": "Model 1: Check model assumptions",
    "text": "Model 1: Check model assumptions\n\nPlotsQuestions\n\n\n\n#library(ggfortify)\nautoplot(model1Cent)\n\n\n\n\n\n\n\n\n\n\nWhich of the model assumptions (LINE) does this pass and/or fail?"
  },
  {
    "objectID": "slides/01-welcome_ch1_o.html#model-2-add-quadratic-effect-for-year",
    "href": "slides/01-welcome_ch1_o.html#model-2-add-quadratic-effect-for-year",
    "title": "Welcome and Chapter 1",
    "section": "Model 2: Add quadratic effect for year?",
    "text": "Model 2: Add quadratic effect for year?\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(data = derby, aes(x = yearnew, y = speed)) + \n  geom_point(alpha = 0.7) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"blue\") + \n  geom_smooth(se = FALSE, color = \"red\", linetype = 2) + \n  labs(x = \"Years since 1896\", y = \"Speed (ft/s)\", \n       title = \"Speed vs. Years since 1896\")"
  },
  {
    "objectID": "slides/01-welcome_ch1_o.html#model-2-add-yearnew2",
    "href": "slides/01-welcome_ch1_o.html#model-2-add-yearnew2",
    "title": "Welcome and Chapter 1",
    "section": "Model 2: Add \\(yearnew^2\\)",
    "text": "Model 2: Add \\(yearnew^2\\)\n\nPlotCode\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n51.4130\n0.1826\n281.5645\n0.0000\n\n\nstarters\n-0.0253\n0.0136\n-1.8588\n0.0656\n\n\nyearnew\n0.0700\n0.0061\n11.4239\n0.0000\n\n\nI(yearnew^2)\n-0.0004\n0.0000\n-8.0411\n0.0000\n\n\nconditiongood\n-0.4770\n0.1857\n-2.5689\n0.0115\n\n\nconditionslow\n-1.3927\n0.1305\n-10.6701\n0.0000\n\n\n\n\n\n\n\n\nmodel2 &lt;- lm(speed ~ starters + yearnew + I(yearnew^2) + condition, \n             data = derby)\ntidy(model2) |&gt; kable(digits = 4)"
  },
  {
    "objectID": "slides/01-welcome_ch1_o.html#interpreting-quadratic-effects",
    "href": "slides/01-welcome_ch1_o.html#interpreting-quadratic-effects",
    "title": "Welcome and Chapter 1",
    "section": "Interpreting quadratic effects",
    "text": "Interpreting quadratic effects\n\\[\\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 ~ x_1  + \\hat{\\beta}_2 ~ x_2 + \\hat{\\beta}_3 ~ x_2^2\\]\nGeneral interpretation: When \\(x_2\\) increases from a to b, \\(y\\) is expected to change by \\(\\hat{\\beta}_2(b - a) + \\hat{\\beta}_3(b^2 - a^2)\\), holding \\(x_1\\) constant.\n. . ."
  },
  {
    "objectID": "slides/01-welcome_ch1_o.html#interpreting-quadratic-effects-1",
    "href": "slides/01-welcome_ch1_o.html#interpreting-quadratic-effects-1",
    "title": "Welcome and Chapter 1",
    "section": "Interpreting quadratic effects",
    "text": "Interpreting quadratic effects\n\\[\\begin{aligned}\\widehat{speed} = &51.413 - 0.025 ~ starters + 0.070 ~ yearnew \\\\\n& - 0.0004 ~ yearnew^2 - 0.477 ~ good - 1.393 ~ slow\\end{aligned}\\]\n. . .\nQuestions:\nInterpret the effect of year for the 5 most recent years (2013 - 2017)."
  },
  {
    "objectID": "slides/01-welcome_ch1_o.html#model-2-check-model-assumptions",
    "href": "slides/01-welcome_ch1_o.html#model-2-check-model-assumptions",
    "title": "Welcome and Chapter 1",
    "section": "Model 2: Check model assumptions",
    "text": "Model 2: Check model assumptions"
  },
  {
    "objectID": "slides/01-welcome_ch1_o.html#model-3-include-interaction-term",
    "href": "slides/01-welcome_ch1_o.html#model-3-include-interaction-term",
    "title": "Welcome and Chapter 1",
    "section": "Model 3: Include interaction term?",
    "text": "Model 3: Include interaction term?\nRecall from the EDA…\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlibrary(viridis)\nggplot(data = derby, aes(x = year, y = speed, color = condition, \n                         shape = condition, linetype = condition)) + \n  geom_point() + \n  geom_smooth(method = \"lm\", se = FALSE, aes(linetype = condition)) + \n  labs(x = \"Year\", y = \"Speed (ft/s)\", color = \"Condition\",\n       title = \"Speed vs. year\", \n       subtitle = \"by track condition\") +\n  guides(lty = FALSE, shape = FALSE) +\n  scale_color_viridis_d(end = 0.9)"
  },
  {
    "objectID": "slides/01-welcome_ch1_o.html#model-3-add-interaction-term",
    "href": "slides/01-welcome_ch1_o.html#model-3-add-interaction-term",
    "title": "Welcome and Chapter 1",
    "section": "Model 3: Add interaction term",
    "text": "Model 3: Add interaction term\n\\[\\begin{aligned}\\widehat{speed} = & 52.387 - 0.003 ~ starters + 0.020 ~ yearnew - 1.070 ~ good - 2.183 ~ slow \\\\ &+0.012 ~ yearnew \\times good + 0.012 ~ yearnew \\times slow \\end{aligned}\\]\n\nOutputCodeAssumptions\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n52.387\n0.200\n262.350\n0.000\n\n\nstarters\n-0.003\n0.016\n-0.189\n0.850\n\n\nyearnew\n0.020\n0.003\n7.576\n0.000\n\n\nconditiongood\n-1.070\n0.423\n-2.527\n0.013\n\n\nconditionslow\n-2.183\n0.270\n-8.097\n0.000\n\n\nyearnew:conditiongood\n0.012\n0.008\n1.598\n0.113\n\n\nyearnew:conditionslow\n0.012\n0.004\n2.866\n0.005\n\n\n\n\n\n\n\n\nmodel3 &lt;- lm(speed ~ starters + yearnew + condition +\n               yearnew * condition, \n             data = derby)\ntidy(model3) |&gt; kable(digits = 4)"
  },
  {
    "objectID": "slides/01-welcome_ch1_o.html#interpreting-interaction-effects",
    "href": "slides/01-welcome_ch1_o.html#interpreting-interaction-effects",
    "title": "Welcome and Chapter 1",
    "section": "Interpreting interaction effects",
    "text": "Interpreting interaction effects\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n52.387\n0.200\n262.350\n0.000\n\n\nstarters\n-0.003\n0.016\n-0.189\n0.850\n\n\nyearnew\n0.020\n0.003\n7.576\n0.000\n\n\nconditiongood\n-1.070\n0.423\n-2.527\n0.013\n\n\nconditionslow\n-2.183\n0.270\n-8.097\n0.000\n\n\nyearnew:conditiongood\n0.012\n0.008\n1.598\n0.113\n\n\nyearnew:conditionslow\n0.012\n0.004\n2.866\n0.005\n\n\n\n\n\n\nWrite out the interpretation of…"
  },
  {
    "objectID": "slides/01-welcome_ch1_o.html#which-model-would-you-choose",
    "href": "slides/01-welcome_ch1_o.html#which-model-would-you-choose",
    "title": "Welcome and Chapter 1",
    "section": "Which model would you choose?",
    "text": "Which model would you choose?\n\nModel 1: Main effectsModel 2: Main effects + \\(year^2\\)Model 3: Main effects + interactionCode\n\n\n\n\n\n\n\nr.squared\nadj.r.squared\nAIC\nBIC\n\n\n\n\n0.73\n0.721\n259.478\n276.302\n\n\n\n\n\n\n\n\n\n\n\n\nr.squared\nadj.r.squared\nAIC\nBIC\n\n\n\n\n0.827\n0.819\n207.429\n227.057\n\n\n\n\n\n\n\n\n\n\n\n\nr.squared\nadj.r.squared\nAIC\nBIC\n\n\n\n\n0.751\n0.738\n253.584\n276.016\n\n\n\n\n\n\n\n\n# Model 1\nglance(model1Cent) |&gt;\n  select(r.squared, adj.r.squared, AIC, BIC) |&gt;\n  kable(digits = 3)\n\n# Model2\nglance(model2) |&gt;\n  select(r.squared, adj.r.squared, AIC, BIC) |&gt;\n  kable(digits = 3)\n\n# Model 3\nglance(model3) |&gt;\n  select(r.squared, adj.r.squared, AIC, BIC) |&gt;\n  kable(digits = 3)"
  },
  {
    "objectID": "slides/01-welcome_ch1_o.html#what-are-these-model-quality-metrics",
    "href": "slides/01-welcome_ch1_o.html#what-are-these-model-quality-metrics",
    "title": "Welcome and Chapter 1",
    "section": "What are these model quality metrics?",
    "text": "What are these model quality metrics?\n\nHow do we define RSquared?\nWhat is adj.r.squared?"
  },
  {
    "objectID": "slides/01-welcome_ch1_o.html#measures-of-model-performance",
    "href": "slides/01-welcome_ch1_o.html#measures-of-model-performance",
    "title": "Welcome and Chapter 1",
    "section": "Measures of model performance",
    "text": "Measures of model performance\n\n\\(\\color{#4187aa}{R^2}\\): Proportion of variability in the response explained by the model.\n\nWill always increase as predictors are added, so it shouldn’t be used to compare models\n\n\\(\\color{#4187aa}{Adj. R^2}\\): Similar to \\(R^2\\) with a penalty for extra terms\n\n. . .\n\n\\(\\color{#4187aa}{AIC}\\): Likelihood-based approach balancing model performance and complexity\n\\(\\color{#4187aa}{BIC}\\): Similar to AIC with stronger penalty for extra terms\n\n. . .\n\nNested F Test (extra sum of squares F test): Generalization of t-test for individual coefficients to perform significance tests on nested models"
  },
  {
    "objectID": "slides/01-welcome_ch1_o.html#which-model-would-you-choose-1",
    "href": "slides/01-welcome_ch1_o.html#which-model-would-you-choose-1",
    "title": "Welcome and Chapter 1",
    "section": "Which model would you choose?",
    "text": "Which model would you choose?\nUse the glance function to get model statistics.\n\nOutputCode\n\n\n\n\n\n\n\nmodel\nr.squared\nadj.r.squared\nAIC\nBIC\n\n\n\n\nModel1\n0.730\n0.721\n259.478\n276.302\n\n\nModel2\n0.827\n0.819\n207.429\n227.057\n\n\nModel3\n0.751\n0.738\n253.584\n276.016\n\n\n\n\n\n\n\n\nmodel1_glance &lt;- glance(model1Cent) |&gt;\n  select(r.squared, adj.r.squared, AIC, BIC)\nmodel2_glance &lt;- glance(model2) |&gt;\n  select(r.squared, adj.r.squared, AIC, BIC)\nmodel3_glance &lt;- glance(model3) |&gt;\n  select(r.squared, adj.r.squared, AIC, BIC)\n\nmodel1_glance |&gt;\n  bind_rows(model2_glance) |&gt;\n  bind_rows(model3_glance) |&gt;\n  bind_cols(model = c(\"Model1\", \"Model2\", \"Model3\")) |&gt;\n  select(model, everything()) |&gt;\nkable(digits = 3)"
  },
  {
    "objectID": "slides/01-welcome_ch1_o.html#characteristics-of-a-good-final-model",
    "href": "slides/01-welcome_ch1_o.html#characteristics-of-a-good-final-model",
    "title": "Welcome and Chapter 1",
    "section": "Characteristics of a “good” final model",
    "text": "Characteristics of a “good” final model\n\nModel can be used to answer primary research questions\nPredictor variables control for important covariates\nPotential interactions have been investigated\nVariables are centered, as needed, for more meaningful interpretations\nunnecessary terms are removed\nAssumptions are met and influential points have been addressed\nmodel tells a “persuasive story parsimoniously”\n\n\n\nList from Section 1.6.7 of BMLR"
  },
  {
    "objectID": "slides/01-welcome_ch1_o.html#inference-for-multiple-linear-regression",
    "href": "slides/01-welcome_ch1_o.html#inference-for-multiple-linear-regression",
    "title": "Welcome and Chapter 1",
    "section": "Inference for multiple linear regression",
    "text": "Inference for multiple linear regression\nUse statistical inference to\n\nDetermine if predictors are statistically significant (not necessarily practically significant!)\nQuantify uncertainty in coefficient estimates\nQuantify uncertainty in model predictions\n\n. . .\nIf L.I.N.E. assumptions are met, we can conduct inference using the \\(t\\) distribution and estimated standard errors"
  },
  {
    "objectID": "slides/01-welcome_ch1_o.html#inference-for-regression",
    "href": "slides/01-welcome_ch1_o.html#inference-for-regression",
    "title": "Welcome and Chapter 1",
    "section": "Inference for regression",
    "text": "Inference for regression\n\nWhen L.I.N.E. conditions are metWe can\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUse least squares regression to get the estimates \\(\\hat{\\beta}_0\\), \\(\\hat{\\beta}_1\\), and \\(\\hat{\\sigma}^2\\)\n\\(\\hat{\\sigma}\\) is the regression standard error\n\n. . .\n\\[\\hat{\\sigma} = \\sqrt{\\frac{\\sum_{i=1}^n(y_i - \\hat{y}_i)^2}{n - p - 1}} = \\sqrt{\\frac{\\sum_{i=1}^n e_i^2}{n-p-1}}\\]"
  },
  {
    "objectID": "slides/01-welcome_ch1_o.html#acknowledgements",
    "href": "slides/01-welcome_ch1_o.html#acknowledgements",
    "title": "Welcome and Chapter 1",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThese slides are based on content in BMLR: Chapter 1 - Review of Multiple Linear Regression\nInitial versions of the slides are by Dr. Maria Tackett, Duke University"
  },
  {
    "objectID": "mini_project/03_project_MLM.html",
    "href": "mini_project/03_project_MLM.html",
    "title": "Mini Project 3",
    "section": "",
    "text": "Pick a partner.\nFind an appropriate data set.\n\nMultilevel data may be harder to find.\nThe data you choose should have a few predictors that may be useful when modeling (ideally at least 1 quantitative and 1 categorical).\nIn this project, you can choose the data from the book between Chapter 9 Guided number 2 or any of Chapter 9’s Open Ended exercises. You are not going to answer their questions but use the questions data to answer your own questions.\nI do ask that different groups choose different datasets. Choose a few options.\n\n\nData Options: Check out Data Links on the Useful Links part of the course website. TidyTuesday has quickly accessible data.\n\nWrite out the anticipated cleaning and/or feature engineering steps you will need to take. Some examples:\n\nCreating simplified categorical variables or transforming a continuous variable into categorical.\nAggregating data\nConverting date columns\netc.\n\nSetup your R Project(s) on the server.\nRead your data into R. This likely will require you to download the data to your computer and upload the data the server."
  },
  {
    "objectID": "mini_project/03_project_MLM.html#timing",
    "href": "mini_project/03_project_MLM.html#timing",
    "title": "Mini Project 3",
    "section": "Timing",
    "text": "Timing\nThis project will be in a workshop style. The intention is for you to start and finish by the end of class time. We will follow a timeline:\n\n\n\n\n\n\n\nTask\nTiming\n\n\n\n\nClean Data\n9:00 am - 9:30 am\n\n\nPerform EDA\n9:30 am - 10:30 am\n\n\nFit, Assess, and Compare Regression Models\n10:30 am - 11:00 am, 1:00 pm - 2:00 pm\n\n\nPrepare presentation\n2:00 pm - 2:20 pm\n\n\nPresent your findings\n2:20 pm - 2:40 pm\n\n\nSubmit your Final Report\nSubmit HTML Sunday 9/8 at 11:59 pm\n\n\n\n\nGrading\nEach Mini Project is worth 50 points (Labs are 10 points each).\n\n\n\n\n\n\n\nCategory\nPoints\n\n\n\n\nR project was created the day before, data read into the environment, and cleaning steps were written.\n2\n\n\nThe data chosen is appropriate, and the cleaning steps are correct and explained.\n3\n\n\nEDA is thorough. All graphs and tables included are paired with a discussion. EDA supports the choice of modeling technique.\n15\n\n\nThe model fitting process has a logical flow. Multiple models are considered and compared using statistical tests and various metrics. Any interpreted model has been assessed using residual plots and appropriate statistical tests.\n15\n\n\nThe code follows a sensible order and has been appropriately commented on.\n5\n\n\nThe presentation is concise, describes the data, highlights key parts of the EDA, describes minimally the final model, gives at least 1 interpretation in the context of a coefficient, and discusses limitations and potential future work.\n5\n\n\nThe report is well written, with correct spelling and grammar. The used code is included either inline or in an appendix at the end.\n5\n\n\n\n\n\nSubmission\nAdd format part of your final report document and then re-render:\n\n---\ntitle: \"Document title\"\nauthor: \"my name\"\nformat:\n  html:\n    embed-resources: true\n---\n\nWhen you are finished with your homework, be sure to Render the final document. Once rendered, you can download your file by:\n\nFinding the .html file in your File pane (on the bottom right of the screen)\nClick the check box next to the file\nClick the blue gear above and then click “Export” to download\nSubmit your final html document to the respective assignment on Moodle"
  },
  {
    "objectID": "mini_project/01_project_poisson.html",
    "href": "mini_project/01_project_poisson.html",
    "title": "Mini Project 1",
    "section": "",
    "text": "Pick a partner.\nOne student should create a new R project in the class shared folder, STA363_inst_files-&gt;mini_projects (not your own folder). This allows both students to have access. Put both member’s initials in the project name.\nCreate a new folder inside your project folder called data\nCreate qmd or script files as needed."
  },
  {
    "objectID": "mini_project/01_project_poisson.html#project-setup",
    "href": "mini_project/01_project_poisson.html#project-setup",
    "title": "Mini Project 1",
    "section": "",
    "text": "Pick a partner.\nOne student should create a new R project in the class shared folder, STA363_inst_files-&gt;mini_projects (not your own folder). This allows both students to have access. Put both member’s initials in the project name.\nCreate a new folder inside your project folder called data\nCreate qmd or script files as needed."
  },
  {
    "objectID": "mini_project/01_project_poisson.html#instructions",
    "href": "mini_project/01_project_poisson.html#instructions",
    "title": "Mini Project 1",
    "section": "Instructions",
    "text": "Instructions\nFind data with a reasonable poisson response that you are interested in. Clean that data. Fit and interpret poisson regression models. Summarize your findings.\nData Options. Check out Data Links on the Useful Links part of the course website. TidyTuesday has quickly accessible data.\nThis project will be in a workshop style. The intention is for you to start and finish by the end of class time. We will follow a timeline:\n\n\n\n\n\n\n\nTask\nTiming\n\n\n\n\nFind Appropriate Data\n9:00 am - 9:20 am\n\n\nClean Data\n9:20 am - 9:50 am\n\n\nPerform EDA\n10:00 am - 10:30 am\n\n\nFit, Assess, and Compare Regression Models\n10:30 am - 11:00 am, 1:00 pm - 2:00 pm\n\n\nPrepare presentation\n2:00 pm - 2:20 pm\n\n\nPresent your findings\n2:20 pm - 2:40 pm\n\n\nSubmit your Final Report\nSubmit HTML Sunday 9/1 at 11:59 pm\n\n\n\n\nGrading\nEach Mini Project is worth 50 points (Labs are 10 points each).\n\n\n\n\n\n\n\nCategory\nPoints\n\n\n\n\nThe data chosen is appropriate, and the cleaning steps are correct and explained.\n5\n\n\nEDA is thorough. All included graphs and tables are paired with a discussion. EDA supports the choice of modeling technique.\n15\n\n\nThe model fitting process has a logical flow. Multiple models are considered and compared using statistical tests and multiple metrics. Any model that is interpreted has been assessed using residual plots and appropriate statistical tests.\n15\n\n\nThe code follows a sensible order and has been appropriately commented on.\n5\n\n\nThe presentation is concise, describes the data, highlights key parts of the EDA, describes minimally the final model, gives at least 1 interpretation in the context of a coefficient, and discusses limitations and potential future work.\n5\n\n\nThe report is well written, with correct spelling and grammar. The used code is included either inline or in an appendix at the end.\n5\n\n\n\n\n\nSubmission\nAdd format part of your final report document and then re-render:\n```{r}\n#| label: yaml_example\n#| eval: false\n---\ntitle: \"Document title\"\nauthor: \"my name\"\nformat:\n  html:\n    embed-resources: true\n---\n```\nWhen you are finished with your homework, be sure to Render the final document. Once rendered, you can download your file by:\n\nFinding the .html file in your File pane (on the bottom right of the screen)\nClick the check box next to the file\nClick the blue gear above and then click “Export” to download\nSubmit your final html document to the respective assignment on Moodle"
  },
  {
    "objectID": "labs/05_multilevel_mod/05_happy_fillable.html",
    "href": "labs/05_multilevel_mod/05_happy_fillable.html",
    "title": "Chapter 8 - Multilevel Models",
    "section": "",
    "text": "Questions\n\nPerform an exploratory data analysis by comparing positive affect (happiness) to Level One and Level Two covariates using appropriate graphs. Comment on interesting trends, supporting your comments with appropriate summary statistics.\nReport estimated fixed effects and variance components from Model A, using proper notation from this chapter (no interpretations required). Also report and interpret an intraclass correlation coefficient.\nReport estimated fixed effects and variance components from Model B, using proper notation from this chapter. Interpret your MLE estimates for \\(\\hat{\\alpha}_{0}\\) (the intercept), \\(\\hat{\\beta}_{1}\\) (the instructor indicator), and \\(\\hat{\\sigma}_{u}\\) (the Level Two standard deviation for the intercept). Also report and interpret an appropriate pseudo R-squared value.\nWrite out Model C, using both separate Level One and Level Two models as well as a composite model. Be sure to express distributions for error terms. How many parameters must be estimated in Model C?\nReport and interpret the following parameter estimates from Model C: \\(\\hat{\\alpha}_{0}\\), \\(\\hat{\\alpha}_{1}\\), \\(\\hat{\\gamma}_{0}\\), \\(\\hat{\\beta}_{1}\\), \\(\\hat{\\sigma}_{u}\\), \\(\\hat{\\sigma}_{v}\\), and \\(\\hat{\\rho}_{uv}\\). Interpretations for variance components should be done in terms of standard deviations and correlation coefficients.\nReport and interpret the same parameter estimates listed above from Model D. In each case, the new interpretation should involve a small modification of your interpretation from Model C. Use underlines or highlights to denote the part of the Model D interpretation that differs from the Model C interpretation.\nAlso report and interpret the following parameter estimates from Model D: \\(\\hat{\\alpha}_{2}\\) and \\(\\hat{\\beta}_{2}\\).\nUse a drop in deviance statistic (likelihood ratio test) to compare Model C vs. Model D. Give a test statistic and p-value, then state a conclusion. Also compare Models C and D with appropriate pseudo R-squared value(s) and with AIC and BIC statistics."
  },
  {
    "objectID": "labs/04_logistic/04_moths_fillable.html",
    "href": "labs/04_logistic/04_moths_fillable.html",
    "title": "Chapter 6 - Binomial Regression",
    "section": "",
    "text": "library(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(readr)\nlibrary(ggplot2)\nlibrary(gridExtra) \n\nWarning: package 'gridExtra' was built under R version 4.4.1\n\n\n\nAttaching package: 'gridExtra'\n\n\nThe following object is masked from 'package:dplyr':\n\n    combine\n\nmoth &lt;- read_csv(\"data/moth.csv\")\n\nRows: 14 Columns: 4\n\n\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): MORPH\ndbl (3): DISTANCE, PLACED, REMOVED\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nmoth &lt;- mutate(moth, notremoved = PLACED - REMOVED, \n               logit1 = log(REMOVED / notremoved),\n               prop1 = REMOVED / PLACED, \n               dark = ifelse(MORPH==\"dark\",1,0) )\n\nmoth |&gt; head(5)\n\n# A tibble: 5 × 8\n  MORPH DISTANCE PLACED REMOVED notremoved logit1 prop1  dark\n  &lt;chr&gt;    &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;      &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 light      0       56      17         39 -0.830 0.304     0\n2 dark       0       56      14         42 -1.10  0.25      1\n3 light      7.2     80      28         52 -0.619 0.35      0\n4 dark       7.2     80      20         60 -1.10  0.25      1\n5 light     24.1     52      18         34 -0.636 0.346     0\n\n\n\n\n\n# EDA\nggplot(data = moth, aes(x = DISTANCE, y = logit1)) + \n  geom_point() + geom_smooth(method=\"lm\") \n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nggplot(data = moth, aes(x = MORPH, y = logit1)) + \n  geom_boxplot() \n\n\n\n\n\n\n\n# Plot empirical logits vs. distance separately by morph\nggplot(data = moth, aes(x = DISTANCE, y = logit1, color = MORPH, \n                        shape = MORPH)) + \n  geom_point() + \n  geom_smooth(method=\"lm\", alpha = 1/4) \n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n# Repeat above plot but \"Connect the dots\" within morph\nggplot(data = moth, aes(x = DISTANCE, y = logit1, color = MORPH, \n                        shape = MORPH)) + \n  geom_point() + \n  geom_line() \n\n\n\n\n\n\n\n\n\n\n\n\n# Initial model\nbreg2 &lt;- glm(prop1 ~ DISTANCE + dark, weights = PLACED, family = binomial, \n             data = moth)\nsummary(breg2)\n\n\nCall:\nglm(formula = prop1 ~ DISTANCE + dark, family = binomial, data = moth, \n    weights = PLACED)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -1.136742   0.156758  -7.252 4.12e-13 ***\nDISTANCE     0.005314   0.004002   1.328  0.18422    \ndark         0.404052   0.139377   2.899  0.00374 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 35.385  on 13  degrees of freedom\nResidual deviance: 25.161  on 11  degrees of freedom\nAIC: 93.836\n\nNumber of Fisher Scoring iterations: 4\n\nexp(coef(breg2))\n\n(Intercept)    DISTANCE        dark \n  0.3208626   1.0053278   1.4978822 \n\n# Alternative way to express initial model\nbreg2a &lt;- glm(cbind(REMOVED, notremoved) ~ DISTANCE + dark, \n              family = binomial, data=moth)\nsummary(breg2a)\n\n\nCall:\nglm(formula = cbind(REMOVED, notremoved) ~ DISTANCE + dark, family = binomial, \n    data = moth)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -1.136742   0.156758  -7.252 4.12e-13 ***\nDISTANCE     0.005314   0.004002   1.328  0.18422    \ndark         0.404052   0.139377   2.899  0.00374 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 35.385  on 13  degrees of freedom\nResidual deviance: 25.161  on 11  degrees of freedom\nAIC: 93.836\n\nNumber of Fisher Scoring iterations: 4\n\n\n\n# Yet one more way to express initial model\nbreg2b &lt;- glm(prop1 ~ DISTANCE + MORPH, weights = PLACED, family = binomial, data = moth)\nsummary(breg2b)\n\n\nCall:\nglm(formula = prop1 ~ DISTANCE + MORPH, family = binomial, data = moth, \n    weights = PLACED)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -0.732690   0.151221  -4.845 1.27e-06 ***\nDISTANCE     0.005314   0.004002   1.328  0.18422    \nMORPHlight  -0.404052   0.139377  -2.899  0.00374 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 35.385  on 13  degrees of freedom\nResidual deviance: 25.161  on 11  degrees of freedom\nAIC: 93.836\n\nNumber of Fisher Scoring iterations: 4\n\n\n\n\n\n\n# Plot initial model (no interaction)\nmoth &lt;- mutate(moth, breg2.pred = predict(breg2, type=\"link\") )\nggplot(data = moth, aes(x = DISTANCE, color = MORPH, shape = MORPH)) + \n  geom_point(aes(y = logit1)) + \n  geom_smooth(aes(y = breg2.pred), method=\"lm\") +\n  ggtitle(\"Initial model (no interaction)\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n# Model with interaction (full model)\nbreg3 &lt;- glm(prop1 ~ DISTANCE + dark + DISTANCE:dark, weights = PLACED, \n             family = binomial, data = moth)\nsummary(breg3)\n\n\nCall:\nglm(formula = prop1 ~ DISTANCE + dark + DISTANCE:dark, family = binomial, \n    data = moth, weights = PLACED)\n\nCoefficients:\n               Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)   -0.717729   0.190205  -3.773 0.000161 ***\nDISTANCE      -0.009287   0.005788  -1.604 0.108629    \ndark          -0.411257   0.274490  -1.498 0.134066    \nDISTANCE:dark  0.027789   0.008085   3.437 0.000588 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 35.385  on 13  degrees of freedom\nResidual deviance: 13.230  on 10  degrees of freedom\nAIC: 83.904\n\nNumber of Fisher Scoring iterations: 4\n\n# Interpret exponentiated coefficients and associated profile CIs\nexp(coef(breg3))\n\n  (Intercept)      DISTANCE          dark DISTANCE:dark \n    0.4878587     0.9907562     0.6628165     1.0281788 \n\nexp(confint(breg3))\n\nWaiting for profiling to be done...\n\n\n                  2.5 %    97.5 %\n(Intercept)   0.3335844 0.7042152\nDISTANCE      0.9795673 1.0020840\ndark          0.3860419 1.1340516\nDISTANCE:dark 1.0120693 1.0446879\n\n# Generate CIs - Wald-based method\nbetas &lt;- summary(breg3)$coefficients[,1]\nsebetas &lt;- summary(breg3)$coefficients[,2]\nlb &lt;- betas - qnorm(.975)*sebetas\nub &lt;- betas + qnorm(.975)*sebetas\ncbind(exp(lb),exp(ub))\n\n                   [,1]      [,2]\n(Intercept)   0.3360403 0.7082667\nDISTANCE      0.9795796 1.0020603\ndark          0.3870322 1.1351139\nDISTANCE:dark 1.0120134 1.0446024\n\n# Drop in deviance test\nanova(breg2, breg3, test=\"Chisq\")\n\nAnalysis of Deviance Table\n\nModel 1: prop1 ~ DISTANCE + dark\nModel 2: prop1 ~ DISTANCE + dark + DISTANCE:dark\n  Resid. Df Resid. Dev Df Deviance  Pr(&gt;Chi)    \n1        11     25.161                          \n2        10     13.230  1   11.931 0.0005519 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# Test goodness of fit for full model\ngof &lt;- 1-pchisq(breg3$deviance, breg3$df.residual)\ngof  \n\n[1] 0.2111003\n\n# Plot full model (with interaction)\nmoth &lt;- mutate(moth, breg3.pred = predict(breg3, type=\"link\") )\nggplot(data = moth, aes(x = DISTANCE, color = MORPH, shape = MORPH)) + \n  geom_point(aes(y = logit1)) + \n  geom_smooth(aes(y = breg3.pred), method=\"lm\") +\n  ggtitle(\"Full model (with interaction)\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n# Construct table of predicted values and prediction errors\nbin.prop = moth$prop1\nmodel.est = predict(breg3, type=\"response\")\ndev.resid = residuals(breg3, type=\"deviance\")\npears.resid = residuals(breg3, type=\"pearson\")\ndisplay1 = data.frame(DISTANCE=moth$DISTANCE, MORPH=moth$MORPH, bin.prop, \n  model.est, dev.resid, pears.resid)\ndisplay1\n\n   DISTANCE MORPH  bin.prop model.est   dev.resid pears.resid\n1       0.0 light 0.3035714 0.3278932 -0.39029161 -0.38770684\n2       0.0  dark 0.2500000 0.2443482  0.09817395  0.09842764\n3       7.2 light 0.3500000 0.3133306  0.70012843  0.70708882\n4       7.2  dark 0.2500000 0.2697738 -0.40168046 -0.39848098\n5      24.1 light 0.3461538 0.2805892  1.03033392  1.05231913\n6      24.1  dark 0.4230769 0.3355738  1.31241839  1.33631238\n7      30.2 light 0.1500000 0.2692974 -2.21182753 -2.08314815\n8      30.2  dark 0.2666667 0.3611865 -1.56021941 -1.52421287\n9      36.4 light 0.2666667 0.2581190  0.15076812  0.15130310\n10     36.4  dark 0.3833333 0.3880529 -0.07507580 -0.07501947\n11     41.5 light 0.2380952 0.2491537 -0.23551483 -0.23432889\n12     41.5  dark 0.4761905 0.4106830  1.21248773  1.22040170\n13     51.2 light 0.2608696 0.2326848  0.63131047  0.63978903\n14     51.2  dark 0.4239130 0.4547068 -0.59447466 -0.59316567\n\n\n\n\n\n\n# Adjusting for extra-binomial variation (overdispersion)\nbreg4 &lt;- glm(prop1 ~ DISTANCE + dark + DISTANCE:dark, weights = PLACED,\n             family = quasibinomial, data = moth)\nsummary(breg4)\n\n\nCall:\nglm(formula = prop1 ~ DISTANCE + dark + DISTANCE:dark, family = quasibinomial, \n    data = moth, weights = PLACED)\n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)   -0.717729   0.214423  -3.347   0.0074 **\nDISTANCE      -0.009287   0.006525  -1.423   0.1851   \ndark          -0.411257   0.309439  -1.329   0.2134   \nDISTANCE:dark  0.027789   0.009115   3.049   0.0123 * \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for quasibinomial family taken to be 1.270859)\n\n    Null deviance: 35.385  on 13  degrees of freedom\nResidual deviance: 13.230  on 10  degrees of freedom\nAIC: NA\n\nNumber of Fisher Scoring iterations: 4\n\nexp(coef(breg4))\n\n  (Intercept)      DISTANCE          dark DISTANCE:dark \n    0.4878587     0.9907562     0.6628165     1.0281788 \n\n\n\n# Adjusted drop in deviance test\nbreg4a &lt;- glm(prop1 ~ DISTANCE + dark, weights = PLACED,\n              family = quasibinomial, data = moth)\nsummary(breg4a)\n\n\nCall:\nglm(formula = prop1 ~ DISTANCE + dark, family = quasibinomial, \n    data = moth, weights = PLACED)\n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -1.136742   0.235365  -4.830 0.000528 ***\nDISTANCE     0.005314   0.006008   0.884 0.395404    \ndark         0.404052   0.209269   1.931 0.079680 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for quasibinomial family taken to be 2.25436)\n\n    Null deviance: 35.385  on 13  degrees of freedom\nResidual deviance: 25.161  on 11  degrees of freedom\nAIC: NA\n\nNumber of Fisher Scoring iterations: 4\n\nanova(breg4a, breg4, test = \"F\")\n\nAnalysis of Deviance Table\n\nModel 1: prop1 ~ DISTANCE + dark\nModel 2: prop1 ~ DISTANCE + dark + DISTANCE:dark\n  Resid. Df Resid. Dev Df Deviance      F  Pr(&gt;F)  \n1        11     25.161                             \n2        10     13.230  1   11.931 9.3885 0.01196 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# Find phi = overdispersion parrameter\nphi.num = sum(pears.resid^2)\nphi.denom = breg3$df.residual   # equals 14-4\nphihat = phi.num / phi.denom\nphihat\n\n[1] 1.270859\n\nsqrt(phihat)\n\n[1] 1.127324\n\n# Significance tests for interaction term after adjust for overdispersion\ndropindev = breg2$deviance - breg3$deviance\nd = breg2$df.residual - breg3$df.residual\nFstat = (dropindev/d) / phihat\npval = 1-pf(Fstat, d, breg3$df.residual)\nFstat\n\n[1] 9.388515\n\npval    # p-value from adjusted Drop in Deviance test\n\n[1] 0.01196119\n\nbeta = summary(breg4)$coefficients[4,1]\nsebeta = summary(breg4)$coefficients[4,2]\ntstat = beta / sebeta\npval = 2*(1-pt(tstat, breg4$df.residual))\ntstat\n\n[1] 3.048731\n\npval    # p-value from adjusted Wald test\n\n[1] 0.01227827\n\n# Generate adjusted CIs - profile likelihood method\nexp(confint(breg4))\n\nWaiting for profiling to be done...\n\n\n                  2.5 %    97.5 %\n(Intercept)   0.3174445 0.7373944\nDISTANCE      0.9781472 1.0035420\ndark          0.3601859 1.2143078\nDISTANCE:dark 1.0100422 1.0468234\n\n# Generate adjusted CIs - Wald-based method\nqlcoef = summary(breg4)$coefficients[,1]\nqlse = summary(breg4)$coefficients[,2]\nlb = qlcoef - qt(.975,breg4$df.residual)*qlse\nub = qlcoef + qt(.975,breg4$df.residual)*qlse\ncbind(exp(lb),exp(ub))\n\n                   [,1]      [,2]\n(Intercept)   0.3025549 0.7866545\nDISTANCE      0.9764554 1.0052664\ndark          0.3326282 1.3207711\nDISTANCE:dark 1.0075077 1.0492739\n\n\n\n# Note relationships between betas and SE(betas) under maximum \n# likelihood (sebetas) and quasi likelihood (qlse):\nqlse\n\n  (Intercept)      DISTANCE          dark DISTANCE:dark \n  0.214422569   0.006525361   0.309438917   0.009114954 \n\nsebetas\n\n  (Intercept)      DISTANCE          dark DISTANCE:dark \n  0.190204947   0.005788365   0.274489822   0.008085480 \n\nsqrt(phihat)\n\n[1] 1.127324\n\nsebetas*sqrt(phihat)\n\n  (Intercept)      DISTANCE          dark DISTANCE:dark \n  0.214422569   0.006525361   0.309438916   0.009114954 \n\nqlcoef\n\n  (Intercept)      DISTANCE          dark DISTANCE:dark \n -0.717729416  -0.009286783  -0.411257136   0.027789044 \n\nbetas\n\n  (Intercept)      DISTANCE          dark DISTANCE:dark \n -0.717729416  -0.009286783  -0.411257136   0.027789044 \n\n\n\n\n\n\n# Distance as factor variable\nmoth &lt;- mutate(moth, location = as.factor(DISTANCE) )\nbreg5 = glm(prop1 ~ location + dark, weights = PLACED, \n            family = binomial, data = moth)\nsummary(breg5)\n\n\nCall:\nglm(formula = prop1 ~ location + dark, family = binomial, data = moth, \n    weights = PLACED)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)   -1.1739     0.2254  -5.207 1.92e-07 ***\nlocation7.2    0.1141     0.2739   0.417  0.67686    \nlocation24.1   0.4950     0.2933   1.688  0.09148 .  \nlocation30.2  -0.3774     0.3096  -1.219  0.22287    \nlocation36.4   0.2316     0.2886   0.802  0.42236    \nlocation41.5   0.3760     0.2667   1.410  0.15864    \nlocation51.2   0.3105     0.2633   1.179  0.23829    \ndark           0.4083     0.1401   2.914  0.00357 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 35.385  on 13  degrees of freedom\nResidual deviance: 14.774  on  6  degrees of freedom\nAIC: 93.449\n\nNumber of Fisher Scoring iterations: 4\n\n\n\nbreg5a = glm(prop1 ~ location + dark + location:dark, weights = PLACED,\n            family = binomial, data = moth)\nsummary(breg5a)\n\n\nCall:\nglm(formula = prop1 ~ location + dark + location:dark, family = binomial, \n    data = moth, weights = PLACED)\n\nCoefficients:\n                  Estimate Std. Error z value Pr(&gt;|z|)   \n(Intercept)        -0.8303     0.2906  -2.857  0.00428 **\nlocation7.2         0.2113     0.3734   0.566  0.57143   \nlocation24.1        0.1944     0.4116   0.472  0.63680   \nlocation30.2       -0.9043     0.4639  -1.949  0.05126 . \nlocation36.4       -0.1813     0.4119  -0.440  0.65994   \nlocation41.5       -0.3328     0.3874  -0.859  0.39032   \nlocation51.2       -0.2111     0.3753  -0.563  0.57376   \ndark               -0.2683     0.4239  -0.633  0.52685   \nlocation7.2:dark   -0.2113     0.5489  -0.385  0.70027   \nlocation24.1:dark   0.5941     0.5861   1.014  0.31071   \nlocation30.2:dark   0.9913     0.6290   1.576  0.11504   \nlocation36.4:dark   0.8044     0.5792   1.389  0.16484   \nlocation41.5:dark   1.3361     0.5413   2.468  0.01358 * \nlocation51.2:dark   1.0030     0.5297   1.893  0.05829 . \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 3.5385e+01  on 13  degrees of freedom\nResidual deviance: 8.2157e-15  on  0  degrees of freedom\nAIC: 90.674\n\nNumber of Fisher Scoring iterations: 3\n\ngof = 1-pchisq(breg5a$deviance, breg5a$df.residual)\ngof\n\n[1] 0\n\n\n\n# Try running as logistic regression.  \n#    First must expand data to one row per moth.\nmtemp1 = rep(moth$dark[1],moth$REMOVED[1])\ndtemp1 = rep(moth$DISTANCE[1],moth$REMOVED[1])\nrtemp1 = rep(1,moth$REMOVED[1])\nmtemp1 = c(mtemp1,rep(moth$dark[1],moth$PLACED[1]-moth$REMOVED[1]))\ndtemp1 = c(dtemp1,rep(moth$DISTANCE[1],moth$PLACED[1]-moth$REMOVED[1]))\nrtemp1 = c(rtemp1,rep(0,moth$PLACED[1]-moth$REMOVED[1]))\nfor(i in 2:14)  {\n  mtemp1 = c(mtemp1,rep(moth$dark[i],moth$REMOVED[i]))\n  dtemp1 = c(dtemp1,rep(moth$DISTANCE[i],moth$REMOVED[i]))\n  rtemp1 = c(rtemp1,rep(1,moth$REMOVED[i]))\n  mtemp1 = c(mtemp1,rep(moth$dark[i],moth$PLACED[i]-moth$REMOVED[i]))\n  dtemp1 = c(dtemp1,rep(moth$DISTANCE[i],moth$PLACED[i]-moth$REMOVED[i]))\n  rtemp1 = c(rtemp1,rep(0,moth$PLACED[i]-moth$REMOVED[i]))  }\nnewdata = data.frame(removed=rtemp1,dark=mtemp1,dist=dtemp1)\nnewdata[1:25,]\n\n   removed dark dist\n1        1    0    0\n2        1    0    0\n3        1    0    0\n4        1    0    0\n5        1    0    0\n6        1    0    0\n7        1    0    0\n8        1    0    0\n9        1    0    0\n10       1    0    0\n11       1    0    0\n12       1    0    0\n13       1    0    0\n14       1    0    0\n15       1    0    0\n16       1    0    0\n17       1    0    0\n18       0    0    0\n19       0    0    0\n20       0    0    0\n21       0    0    0\n22       0    0    0\n23       0    0    0\n24       0    0    0\n25       0    0    0\n\ncdplot(as.factor(rtemp1)~dtemp1)\n\n\n\n\n\n\n\nlreg1 = glm(rtemp1 ~ mtemp1 + dtemp1 + mtemp1:dtemp1,\n  family=binomial(link=\"logit\") )\nsummary(lreg1)\n\n\nCall:\nglm(formula = rtemp1 ~ mtemp1 + dtemp1 + mtemp1:dtemp1, family = binomial(link = \"logit\"))\n\nCoefficients:\n               Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)   -0.717729   0.190205  -3.773 0.000161 ***\nmtemp1        -0.411257   0.274490  -1.498 0.134065    \ndtemp1        -0.009287   0.005788  -1.604 0.108628    \nmtemp1:dtemp1  0.027789   0.008085   3.437 0.000588 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1207.9  on 967  degrees of freedom\nResidual deviance: 1185.7  on 964  degrees of freedom\nAIC: 1193.7\n\nNumber of Fisher Scoring iterations: 4\n\n# Try running as a multilevel model\nlibrary(lme4)\n\nWarning: package 'lme4' was built under R version 4.4.1\n\n\nLoading required package: Matrix\n\nid1 = as.factor(dtemp1)\nmodel1 = glmer(rtemp1 ~ mtemp1 + dtemp1 + mtemp1:dtemp1 + (1|id1), family=binomial)\nsummary(model1)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: rtemp1 ~ mtemp1 + dtemp1 + mtemp1:dtemp1 + (1 | id1)\n\n     AIC      BIC   logLik deviance df.resid \n  1195.5   1219.9   -592.8   1185.5      963 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-0.9114 -0.6946 -0.5859  1.1770  1.8210 \n\nRandom effects:\n Groups Name        Variance Std.Dev.\n id1    (Intercept) 0.01148  0.1072  \nNumber of obs: 968, groups:  id1, 7\n\nFixed effects:\n               Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)   -0.719793   0.205495  -3.503 0.000461 ***\nmtemp1        -0.411110   0.274779  -1.496 0.134615    \ndtemp1        -0.009341   0.006270  -1.490 0.136263    \nmtemp1:dtemp1  0.027818   0.008094   3.437 0.000588 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) mtemp1 dtemp1\nmtemp1      -0.641              \ndtemp1      -0.843  0.538       \nmtmp1:dtmp1  0.558 -0.859 -0.660"
  },
  {
    "objectID": "labs/04_logistic/04_moths_fillable.html#setup",
    "href": "labs/04_logistic/04_moths_fillable.html#setup",
    "title": "Chapter 6 - Binomial Regression",
    "section": "",
    "text": "library(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(readr)\nlibrary(ggplot2)\nlibrary(gridExtra) \n\nWarning: package 'gridExtra' was built under R version 4.4.1\n\n\n\nAttaching package: 'gridExtra'\n\n\nThe following object is masked from 'package:dplyr':\n\n    combine\n\nmoth &lt;- read_csv(\"data/moth.csv\")\n\nRows: 14 Columns: 4\n\n\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): MORPH\ndbl (3): DISTANCE, PLACED, REMOVED\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nmoth &lt;- mutate(moth, notremoved = PLACED - REMOVED, \n               logit1 = log(REMOVED / notremoved),\n               prop1 = REMOVED / PLACED, \n               dark = ifelse(MORPH==\"dark\",1,0) )\n\nmoth |&gt; head(5)\n\n# A tibble: 5 × 8\n  MORPH DISTANCE PLACED REMOVED notremoved logit1 prop1  dark\n  &lt;chr&gt;    &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;      &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 light      0       56      17         39 -0.830 0.304     0\n2 dark       0       56      14         42 -1.10  0.25      1\n3 light      7.2     80      28         52 -0.619 0.35      0\n4 dark       7.2     80      20         60 -1.10  0.25      1\n5 light     24.1     52      18         34 -0.636 0.346     0\n\n\n\n\n\n# EDA\nggplot(data = moth, aes(x = DISTANCE, y = logit1)) + \n  geom_point() + geom_smooth(method=\"lm\") \n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nggplot(data = moth, aes(x = MORPH, y = logit1)) + \n  geom_boxplot() \n\n\n\n\n\n\n\n# Plot empirical logits vs. distance separately by morph\nggplot(data = moth, aes(x = DISTANCE, y = logit1, color = MORPH, \n                        shape = MORPH)) + \n  geom_point() + \n  geom_smooth(method=\"lm\", alpha = 1/4) \n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n# Repeat above plot but \"Connect the dots\" within morph\nggplot(data = moth, aes(x = DISTANCE, y = logit1, color = MORPH, \n                        shape = MORPH)) + \n  geom_point() + \n  geom_line() \n\n\n\n\n\n\n\n\n\n\n\n\n# Initial model\nbreg2 &lt;- glm(prop1 ~ DISTANCE + dark, weights = PLACED, family = binomial, \n             data = moth)\nsummary(breg2)\n\n\nCall:\nglm(formula = prop1 ~ DISTANCE + dark, family = binomial, data = moth, \n    weights = PLACED)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -1.136742   0.156758  -7.252 4.12e-13 ***\nDISTANCE     0.005314   0.004002   1.328  0.18422    \ndark         0.404052   0.139377   2.899  0.00374 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 35.385  on 13  degrees of freedom\nResidual deviance: 25.161  on 11  degrees of freedom\nAIC: 93.836\n\nNumber of Fisher Scoring iterations: 4\n\nexp(coef(breg2))\n\n(Intercept)    DISTANCE        dark \n  0.3208626   1.0053278   1.4978822 \n\n# Alternative way to express initial model\nbreg2a &lt;- glm(cbind(REMOVED, notremoved) ~ DISTANCE + dark, \n              family = binomial, data=moth)\nsummary(breg2a)\n\n\nCall:\nglm(formula = cbind(REMOVED, notremoved) ~ DISTANCE + dark, family = binomial, \n    data = moth)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -1.136742   0.156758  -7.252 4.12e-13 ***\nDISTANCE     0.005314   0.004002   1.328  0.18422    \ndark         0.404052   0.139377   2.899  0.00374 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 35.385  on 13  degrees of freedom\nResidual deviance: 25.161  on 11  degrees of freedom\nAIC: 93.836\n\nNumber of Fisher Scoring iterations: 4\n\n\n\n# Yet one more way to express initial model\nbreg2b &lt;- glm(prop1 ~ DISTANCE + MORPH, weights = PLACED, family = binomial, data = moth)\nsummary(breg2b)\n\n\nCall:\nglm(formula = prop1 ~ DISTANCE + MORPH, family = binomial, data = moth, \n    weights = PLACED)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -0.732690   0.151221  -4.845 1.27e-06 ***\nDISTANCE     0.005314   0.004002   1.328  0.18422    \nMORPHlight  -0.404052   0.139377  -2.899  0.00374 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 35.385  on 13  degrees of freedom\nResidual deviance: 25.161  on 11  degrees of freedom\nAIC: 93.836\n\nNumber of Fisher Scoring iterations: 4\n\n\n\n\n\n\n# Plot initial model (no interaction)\nmoth &lt;- mutate(moth, breg2.pred = predict(breg2, type=\"link\") )\nggplot(data = moth, aes(x = DISTANCE, color = MORPH, shape = MORPH)) + \n  geom_point(aes(y = logit1)) + \n  geom_smooth(aes(y = breg2.pred), method=\"lm\") +\n  ggtitle(\"Initial model (no interaction)\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n# Model with interaction (full model)\nbreg3 &lt;- glm(prop1 ~ DISTANCE + dark + DISTANCE:dark, weights = PLACED, \n             family = binomial, data = moth)\nsummary(breg3)\n\n\nCall:\nglm(formula = prop1 ~ DISTANCE + dark + DISTANCE:dark, family = binomial, \n    data = moth, weights = PLACED)\n\nCoefficients:\n               Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)   -0.717729   0.190205  -3.773 0.000161 ***\nDISTANCE      -0.009287   0.005788  -1.604 0.108629    \ndark          -0.411257   0.274490  -1.498 0.134066    \nDISTANCE:dark  0.027789   0.008085   3.437 0.000588 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 35.385  on 13  degrees of freedom\nResidual deviance: 13.230  on 10  degrees of freedom\nAIC: 83.904\n\nNumber of Fisher Scoring iterations: 4\n\n# Interpret exponentiated coefficients and associated profile CIs\nexp(coef(breg3))\n\n  (Intercept)      DISTANCE          dark DISTANCE:dark \n    0.4878587     0.9907562     0.6628165     1.0281788 \n\nexp(confint(breg3))\n\nWaiting for profiling to be done...\n\n\n                  2.5 %    97.5 %\n(Intercept)   0.3335844 0.7042152\nDISTANCE      0.9795673 1.0020840\ndark          0.3860419 1.1340516\nDISTANCE:dark 1.0120693 1.0446879\n\n# Generate CIs - Wald-based method\nbetas &lt;- summary(breg3)$coefficients[,1]\nsebetas &lt;- summary(breg3)$coefficients[,2]\nlb &lt;- betas - qnorm(.975)*sebetas\nub &lt;- betas + qnorm(.975)*sebetas\ncbind(exp(lb),exp(ub))\n\n                   [,1]      [,2]\n(Intercept)   0.3360403 0.7082667\nDISTANCE      0.9795796 1.0020603\ndark          0.3870322 1.1351139\nDISTANCE:dark 1.0120134 1.0446024\n\n# Drop in deviance test\nanova(breg2, breg3, test=\"Chisq\")\n\nAnalysis of Deviance Table\n\nModel 1: prop1 ~ DISTANCE + dark\nModel 2: prop1 ~ DISTANCE + dark + DISTANCE:dark\n  Resid. Df Resid. Dev Df Deviance  Pr(&gt;Chi)    \n1        11     25.161                          \n2        10     13.230  1   11.931 0.0005519 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# Test goodness of fit for full model\ngof &lt;- 1-pchisq(breg3$deviance, breg3$df.residual)\ngof  \n\n[1] 0.2111003\n\n# Plot full model (with interaction)\nmoth &lt;- mutate(moth, breg3.pred = predict(breg3, type=\"link\") )\nggplot(data = moth, aes(x = DISTANCE, color = MORPH, shape = MORPH)) + \n  geom_point(aes(y = logit1)) + \n  geom_smooth(aes(y = breg3.pred), method=\"lm\") +\n  ggtitle(\"Full model (with interaction)\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n# Construct table of predicted values and prediction errors\nbin.prop = moth$prop1\nmodel.est = predict(breg3, type=\"response\")\ndev.resid = residuals(breg3, type=\"deviance\")\npears.resid = residuals(breg3, type=\"pearson\")\ndisplay1 = data.frame(DISTANCE=moth$DISTANCE, MORPH=moth$MORPH, bin.prop, \n  model.est, dev.resid, pears.resid)\ndisplay1\n\n   DISTANCE MORPH  bin.prop model.est   dev.resid pears.resid\n1       0.0 light 0.3035714 0.3278932 -0.39029161 -0.38770684\n2       0.0  dark 0.2500000 0.2443482  0.09817395  0.09842764\n3       7.2 light 0.3500000 0.3133306  0.70012843  0.70708882\n4       7.2  dark 0.2500000 0.2697738 -0.40168046 -0.39848098\n5      24.1 light 0.3461538 0.2805892  1.03033392  1.05231913\n6      24.1  dark 0.4230769 0.3355738  1.31241839  1.33631238\n7      30.2 light 0.1500000 0.2692974 -2.21182753 -2.08314815\n8      30.2  dark 0.2666667 0.3611865 -1.56021941 -1.52421287\n9      36.4 light 0.2666667 0.2581190  0.15076812  0.15130310\n10     36.4  dark 0.3833333 0.3880529 -0.07507580 -0.07501947\n11     41.5 light 0.2380952 0.2491537 -0.23551483 -0.23432889\n12     41.5  dark 0.4761905 0.4106830  1.21248773  1.22040170\n13     51.2 light 0.2608696 0.2326848  0.63131047  0.63978903\n14     51.2  dark 0.4239130 0.4547068 -0.59447466 -0.59316567\n\n\n\n\n\n\n# Adjusting for extra-binomial variation (overdispersion)\nbreg4 &lt;- glm(prop1 ~ DISTANCE + dark + DISTANCE:dark, weights = PLACED,\n             family = quasibinomial, data = moth)\nsummary(breg4)\n\n\nCall:\nglm(formula = prop1 ~ DISTANCE + dark + DISTANCE:dark, family = quasibinomial, \n    data = moth, weights = PLACED)\n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)   -0.717729   0.214423  -3.347   0.0074 **\nDISTANCE      -0.009287   0.006525  -1.423   0.1851   \ndark          -0.411257   0.309439  -1.329   0.2134   \nDISTANCE:dark  0.027789   0.009115   3.049   0.0123 * \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for quasibinomial family taken to be 1.270859)\n\n    Null deviance: 35.385  on 13  degrees of freedom\nResidual deviance: 13.230  on 10  degrees of freedom\nAIC: NA\n\nNumber of Fisher Scoring iterations: 4\n\nexp(coef(breg4))\n\n  (Intercept)      DISTANCE          dark DISTANCE:dark \n    0.4878587     0.9907562     0.6628165     1.0281788 \n\n\n\n# Adjusted drop in deviance test\nbreg4a &lt;- glm(prop1 ~ DISTANCE + dark, weights = PLACED,\n              family = quasibinomial, data = moth)\nsummary(breg4a)\n\n\nCall:\nglm(formula = prop1 ~ DISTANCE + dark, family = quasibinomial, \n    data = moth, weights = PLACED)\n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -1.136742   0.235365  -4.830 0.000528 ***\nDISTANCE     0.005314   0.006008   0.884 0.395404    \ndark         0.404052   0.209269   1.931 0.079680 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for quasibinomial family taken to be 2.25436)\n\n    Null deviance: 35.385  on 13  degrees of freedom\nResidual deviance: 25.161  on 11  degrees of freedom\nAIC: NA\n\nNumber of Fisher Scoring iterations: 4\n\nanova(breg4a, breg4, test = \"F\")\n\nAnalysis of Deviance Table\n\nModel 1: prop1 ~ DISTANCE + dark\nModel 2: prop1 ~ DISTANCE + dark + DISTANCE:dark\n  Resid. Df Resid. Dev Df Deviance      F  Pr(&gt;F)  \n1        11     25.161                             \n2        10     13.230  1   11.931 9.3885 0.01196 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# Find phi = overdispersion parrameter\nphi.num = sum(pears.resid^2)\nphi.denom = breg3$df.residual   # equals 14-4\nphihat = phi.num / phi.denom\nphihat\n\n[1] 1.270859\n\nsqrt(phihat)\n\n[1] 1.127324\n\n# Significance tests for interaction term after adjust for overdispersion\ndropindev = breg2$deviance - breg3$deviance\nd = breg2$df.residual - breg3$df.residual\nFstat = (dropindev/d) / phihat\npval = 1-pf(Fstat, d, breg3$df.residual)\nFstat\n\n[1] 9.388515\n\npval    # p-value from adjusted Drop in Deviance test\n\n[1] 0.01196119\n\nbeta = summary(breg4)$coefficients[4,1]\nsebeta = summary(breg4)$coefficients[4,2]\ntstat = beta / sebeta\npval = 2*(1-pt(tstat, breg4$df.residual))\ntstat\n\n[1] 3.048731\n\npval    # p-value from adjusted Wald test\n\n[1] 0.01227827\n\n# Generate adjusted CIs - profile likelihood method\nexp(confint(breg4))\n\nWaiting for profiling to be done...\n\n\n                  2.5 %    97.5 %\n(Intercept)   0.3174445 0.7373944\nDISTANCE      0.9781472 1.0035420\ndark          0.3601859 1.2143078\nDISTANCE:dark 1.0100422 1.0468234\n\n# Generate adjusted CIs - Wald-based method\nqlcoef = summary(breg4)$coefficients[,1]\nqlse = summary(breg4)$coefficients[,2]\nlb = qlcoef - qt(.975,breg4$df.residual)*qlse\nub = qlcoef + qt(.975,breg4$df.residual)*qlse\ncbind(exp(lb),exp(ub))\n\n                   [,1]      [,2]\n(Intercept)   0.3025549 0.7866545\nDISTANCE      0.9764554 1.0052664\ndark          0.3326282 1.3207711\nDISTANCE:dark 1.0075077 1.0492739\n\n\n\n# Note relationships between betas and SE(betas) under maximum \n# likelihood (sebetas) and quasi likelihood (qlse):\nqlse\n\n  (Intercept)      DISTANCE          dark DISTANCE:dark \n  0.214422569   0.006525361   0.309438917   0.009114954 \n\nsebetas\n\n  (Intercept)      DISTANCE          dark DISTANCE:dark \n  0.190204947   0.005788365   0.274489822   0.008085480 \n\nsqrt(phihat)\n\n[1] 1.127324\n\nsebetas*sqrt(phihat)\n\n  (Intercept)      DISTANCE          dark DISTANCE:dark \n  0.214422569   0.006525361   0.309438916   0.009114954 \n\nqlcoef\n\n  (Intercept)      DISTANCE          dark DISTANCE:dark \n -0.717729416  -0.009286783  -0.411257136   0.027789044 \n\nbetas\n\n  (Intercept)      DISTANCE          dark DISTANCE:dark \n -0.717729416  -0.009286783  -0.411257136   0.027789044 \n\n\n\n\n\n\n# Distance as factor variable\nmoth &lt;- mutate(moth, location = as.factor(DISTANCE) )\nbreg5 = glm(prop1 ~ location + dark, weights = PLACED, \n            family = binomial, data = moth)\nsummary(breg5)\n\n\nCall:\nglm(formula = prop1 ~ location + dark, family = binomial, data = moth, \n    weights = PLACED)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)   -1.1739     0.2254  -5.207 1.92e-07 ***\nlocation7.2    0.1141     0.2739   0.417  0.67686    \nlocation24.1   0.4950     0.2933   1.688  0.09148 .  \nlocation30.2  -0.3774     0.3096  -1.219  0.22287    \nlocation36.4   0.2316     0.2886   0.802  0.42236    \nlocation41.5   0.3760     0.2667   1.410  0.15864    \nlocation51.2   0.3105     0.2633   1.179  0.23829    \ndark           0.4083     0.1401   2.914  0.00357 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 35.385  on 13  degrees of freedom\nResidual deviance: 14.774  on  6  degrees of freedom\nAIC: 93.449\n\nNumber of Fisher Scoring iterations: 4\n\n\n\nbreg5a = glm(prop1 ~ location + dark + location:dark, weights = PLACED,\n            family = binomial, data = moth)\nsummary(breg5a)\n\n\nCall:\nglm(formula = prop1 ~ location + dark + location:dark, family = binomial, \n    data = moth, weights = PLACED)\n\nCoefficients:\n                  Estimate Std. Error z value Pr(&gt;|z|)   \n(Intercept)        -0.8303     0.2906  -2.857  0.00428 **\nlocation7.2         0.2113     0.3734   0.566  0.57143   \nlocation24.1        0.1944     0.4116   0.472  0.63680   \nlocation30.2       -0.9043     0.4639  -1.949  0.05126 . \nlocation36.4       -0.1813     0.4119  -0.440  0.65994   \nlocation41.5       -0.3328     0.3874  -0.859  0.39032   \nlocation51.2       -0.2111     0.3753  -0.563  0.57376   \ndark               -0.2683     0.4239  -0.633  0.52685   \nlocation7.2:dark   -0.2113     0.5489  -0.385  0.70027   \nlocation24.1:dark   0.5941     0.5861   1.014  0.31071   \nlocation30.2:dark   0.9913     0.6290   1.576  0.11504   \nlocation36.4:dark   0.8044     0.5792   1.389  0.16484   \nlocation41.5:dark   1.3361     0.5413   2.468  0.01358 * \nlocation51.2:dark   1.0030     0.5297   1.893  0.05829 . \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 3.5385e+01  on 13  degrees of freedom\nResidual deviance: 8.2157e-15  on  0  degrees of freedom\nAIC: 90.674\n\nNumber of Fisher Scoring iterations: 3\n\ngof = 1-pchisq(breg5a$deviance, breg5a$df.residual)\ngof\n\n[1] 0\n\n\n\n# Try running as logistic regression.  \n#    First must expand data to one row per moth.\nmtemp1 = rep(moth$dark[1],moth$REMOVED[1])\ndtemp1 = rep(moth$DISTANCE[1],moth$REMOVED[1])\nrtemp1 = rep(1,moth$REMOVED[1])\nmtemp1 = c(mtemp1,rep(moth$dark[1],moth$PLACED[1]-moth$REMOVED[1]))\ndtemp1 = c(dtemp1,rep(moth$DISTANCE[1],moth$PLACED[1]-moth$REMOVED[1]))\nrtemp1 = c(rtemp1,rep(0,moth$PLACED[1]-moth$REMOVED[1]))\nfor(i in 2:14)  {\n  mtemp1 = c(mtemp1,rep(moth$dark[i],moth$REMOVED[i]))\n  dtemp1 = c(dtemp1,rep(moth$DISTANCE[i],moth$REMOVED[i]))\n  rtemp1 = c(rtemp1,rep(1,moth$REMOVED[i]))\n  mtemp1 = c(mtemp1,rep(moth$dark[i],moth$PLACED[i]-moth$REMOVED[i]))\n  dtemp1 = c(dtemp1,rep(moth$DISTANCE[i],moth$PLACED[i]-moth$REMOVED[i]))\n  rtemp1 = c(rtemp1,rep(0,moth$PLACED[i]-moth$REMOVED[i]))  }\nnewdata = data.frame(removed=rtemp1,dark=mtemp1,dist=dtemp1)\nnewdata[1:25,]\n\n   removed dark dist\n1        1    0    0\n2        1    0    0\n3        1    0    0\n4        1    0    0\n5        1    0    0\n6        1    0    0\n7        1    0    0\n8        1    0    0\n9        1    0    0\n10       1    0    0\n11       1    0    0\n12       1    0    0\n13       1    0    0\n14       1    0    0\n15       1    0    0\n16       1    0    0\n17       1    0    0\n18       0    0    0\n19       0    0    0\n20       0    0    0\n21       0    0    0\n22       0    0    0\n23       0    0    0\n24       0    0    0\n25       0    0    0\n\ncdplot(as.factor(rtemp1)~dtemp1)\n\n\n\n\n\n\n\nlreg1 = glm(rtemp1 ~ mtemp1 + dtemp1 + mtemp1:dtemp1,\n  family=binomial(link=\"logit\") )\nsummary(lreg1)\n\n\nCall:\nglm(formula = rtemp1 ~ mtemp1 + dtemp1 + mtemp1:dtemp1, family = binomial(link = \"logit\"))\n\nCoefficients:\n               Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)   -0.717729   0.190205  -3.773 0.000161 ***\nmtemp1        -0.411257   0.274490  -1.498 0.134065    \ndtemp1        -0.009287   0.005788  -1.604 0.108628    \nmtemp1:dtemp1  0.027789   0.008085   3.437 0.000588 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1207.9  on 967  degrees of freedom\nResidual deviance: 1185.7  on 964  degrees of freedom\nAIC: 1193.7\n\nNumber of Fisher Scoring iterations: 4\n\n# Try running as a multilevel model\nlibrary(lme4)\n\nWarning: package 'lme4' was built under R version 4.4.1\n\n\nLoading required package: Matrix\n\nid1 = as.factor(dtemp1)\nmodel1 = glmer(rtemp1 ~ mtemp1 + dtemp1 + mtemp1:dtemp1 + (1|id1), family=binomial)\nsummary(model1)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: rtemp1 ~ mtemp1 + dtemp1 + mtemp1:dtemp1 + (1 | id1)\n\n     AIC      BIC   logLik deviance df.resid \n  1195.5   1219.9   -592.8   1185.5      963 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-0.9114 -0.6946 -0.5859  1.1770  1.8210 \n\nRandom effects:\n Groups Name        Variance Std.Dev.\n id1    (Intercept) 0.01148  0.1072  \nNumber of obs: 968, groups:  id1, 7\n\nFixed effects:\n               Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)   -0.719793   0.205495  -3.503 0.000461 ***\nmtemp1        -0.411110   0.274779  -1.496 0.134615    \ndtemp1        -0.009341   0.006270  -1.490 0.136263    \nmtemp1:dtemp1  0.027818   0.008094   3.437 0.000588 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) mtemp1 dtemp1\nmtemp1      -0.641              \ndtemp1      -0.843  0.538       \nmtmp1:dtmp1  0.558 -0.859 -0.660"
  },
  {
    "objectID": "labs/03_poisson/03_FullMoon_fillable.html",
    "href": "labs/03_poisson/03_FullMoon_fillable.html",
    "title": "Chapter 4 - Poisson Regression",
    "section": "",
    "text": "Is there initial evidence of more bites during full moon phases?\nWhy can’t we just perform a t-test comparing full moon periods to non-full moon periods using period-level data (n=10)?\nWhat options can you think of for handling the fact that Period 5 is based on only 2 lunar days? What are the modeling implications of including number of days in a period as an offset term?\nInterpret model parameters from fit1.\nIs there any evidence of lack of fit in fit1? What factors may lead to lack of fit?\nDoes full moon have an effect above and beyond the linear cycle trend? Interpret the coefficient and a 95% confidence interval for the fullmoon term in fit3.\nWhat is fit4 doing? Does it offer an improvement over fit3?\nWhat evidence is there that fit5 (which uses bitesbyday.csv) is the analysis performed by the authors of this paper?\nBased on this analysis, does it make you more wary of animal bites during full moons?\n\n\n\n\nIs there evidence of lack of fit in fit5? Cite evidence both from a goodness of fit test and from comparing means and variances by period.\nIf overdispersion goes uncorrected, what are implications for p-values and CIs for model coefficients?\n\n\n\nOne solution is to simply add a second parameter to inflate variances, so that \\(Var(Y_i)=\\phi\\lambda_i\\). This is called a “quasi-Poisson” or, in general, a “quasi-likelihood” approach, because our data no longer follows a true Poisson distribution.\n\n\n\nAnother solution is to model response using a negative binomial distribution, so that \\(Y\\sim NegBinom(\\theta,p)\\). This distribution comes about if \\(Y|λ\\sim Poisson(λ)\\), but the \\(\\lambda\\)’s themselves are randomly chosen according to a gamma distribution: \\(\\lambda \\sim gamma(θ,\\frac{(1-p)}{p})\\). In this case, $E(Y)==$ and \\(Var(Y)=\\theta \\frac{p}{(1-p)^2} =\\mu+\\frac{\\mu^2}{\\theta}\\), so that \\(\\frac{\\mu^2}{\\theta}\\) is the amount of overdispersion.\n\nCompare the following estimates, tests, and intervals under usual Poisson regression, quasi-Poisson regression, and negative binomial regression:\n\n\n\n\n\n\n\n\n\n\n\nPoisson\nQuasi-poisson\nNegative Binomial\n\n\n\n\n\\(\\phi\\)\n\n\n\n\n\n\\(SE(\\hat{\\beta}_4\\))\n\n\n\n\n\nWald-type test stat\n\n\n\n\n\nWald-type p-value\n\n\n\n\n\nLRT-type test stat\n\n\n\n\n\nLRT-type test stat\n\n\n\n\n\nLRT-type p-value\n\n\n\n\n\nCI - profile for \\(e^{\\beta_4}\\)\n\n\n\n\n\nCI-Wald-type for \\(e^{\\beta_4}\\)"
  },
  {
    "objectID": "labs/03_poisson/03_FullMoon_fillable.html#questions",
    "href": "labs/03_poisson/03_FullMoon_fillable.html#questions",
    "title": "Chapter 4 - Poisson Regression",
    "section": "",
    "text": "Is there initial evidence of more bites during full moon phases?\nWhy can’t we just perform a t-test comparing full moon periods to non-full moon periods using period-level data (n=10)?\nWhat options can you think of for handling the fact that Period 5 is based on only 2 lunar days? What are the modeling implications of including number of days in a period as an offset term?\nInterpret model parameters from fit1.\nIs there any evidence of lack of fit in fit1? What factors may lead to lack of fit?\nDoes full moon have an effect above and beyond the linear cycle trend? Interpret the coefficient and a 95% confidence interval for the fullmoon term in fit3.\nWhat is fit4 doing? Does it offer an improvement over fit3?\nWhat evidence is there that fit5 (which uses bitesbyday.csv) is the analysis performed by the authors of this paper?\nBased on this analysis, does it make you more wary of animal bites during full moons?\n\n\n\n\nIs there evidence of lack of fit in fit5? Cite evidence both from a goodness of fit test and from comparing means and variances by period.\nIf overdispersion goes uncorrected, what are implications for p-values and CIs for model coefficients?\n\n\n\nOne solution is to simply add a second parameter to inflate variances, so that \\(Var(Y_i)=\\phi\\lambda_i\\). This is called a “quasi-Poisson” or, in general, a “quasi-likelihood” approach, because our data no longer follows a true Poisson distribution.\n\n\n\nAnother solution is to model response using a negative binomial distribution, so that \\(Y\\sim NegBinom(\\theta,p)\\). This distribution comes about if \\(Y|λ\\sim Poisson(λ)\\), but the \\(\\lambda\\)’s themselves are randomly chosen according to a gamma distribution: \\(\\lambda \\sim gamma(θ,\\frac{(1-p)}{p})\\). In this case, $E(Y)==$ and \\(Var(Y)=\\theta \\frac{p}{(1-p)^2} =\\mu+\\frac{\\mu^2}{\\theta}\\), so that \\(\\frac{\\mu^2}{\\theta}\\) is the amount of overdispersion.\n\nCompare the following estimates, tests, and intervals under usual Poisson regression, quasi-Poisson regression, and negative binomial regression:\n\n\n\n\n\n\n\n\n\n\n\nPoisson\nQuasi-poisson\nNegative Binomial\n\n\n\n\n\\(\\phi\\)\n\n\n\n\n\n\\(SE(\\hat{\\beta}_4\\))\n\n\n\n\n\nWald-type test stat\n\n\n\n\n\nWald-type p-value\n\n\n\n\n\nLRT-type test stat\n\n\n\n\n\nLRT-type test stat\n\n\n\n\n\nLRT-type p-value\n\n\n\n\n\nCI - profile for \\(e^{\\beta_4}\\)\n\n\n\n\n\nCI-Wald-type for \\(e^{\\beta_4}\\)"
  },
  {
    "objectID": "labs/02_likelihood/02_likelihood_fillable.html",
    "href": "labs/02_likelihood/02_likelihood_fillable.html",
    "title": "Chapter 2 - Likelihoods",
    "section": "",
    "text": "Consider a small example with 3 families with compositions of children given by BBG, GBG, and GG.\nFind the maximum likelihood estimator (MLE) for by:\n\n\nConducting a numerical search in R for the largest likelihood over a fine grid of values 0-1.\nConducting a numerical search in R for the largest log-likelihood between 0 and 1. Illustrate the process graphically, and report the maximum value of the likelihood and log-likelihood functions. Does it make sense that both methods would agree (and agree with the mathematical approach)?\n\n\nApply Model 1 to the NLSY data (families in Table 2 with 3 or fewer children). Find the MLE for by adapting the R code for (1).\n\n\n\n\n= probability of a boy when previously have had an equal number of boys and girls (neutral)\n= probability of a boy when previously have had more boys than girls (boy bias)\n= probability of a boy when previously have had more girls than boys (girl bias)\n\nWrite out the likelihood function given Model 2 for the small set of data in (1). [You could also try BGG, GGB, BBB just for fun.]"
  },
  {
    "objectID": "labs/02_likelihood/02_likelihood_fillable.html#questions",
    "href": "labs/02_likelihood/02_likelihood_fillable.html#questions",
    "title": "Chapter 2 - Likelihoods",
    "section": "",
    "text": "Consider a small example with 3 families with compositions of children given by BBG, GBG, and GG.\nFind the maximum likelihood estimator (MLE) for by:\n\n\nConducting a numerical search in R for the largest likelihood over a fine grid of values 0-1.\nConducting a numerical search in R for the largest log-likelihood between 0 and 1. Illustrate the process graphically, and report the maximum value of the likelihood and log-likelihood functions. Does it make sense that both methods would agree (and agree with the mathematical approach)?\n\n\nApply Model 1 to the NLSY data (families in Table 2 with 3 or fewer children). Find the MLE for by adapting the R code for (1).\n\n\n\n\n= probability of a boy when previously have had an equal number of boys and girls (neutral)\n= probability of a boy when previously have had more boys than girls (boy bias)\n= probability of a boy when previously have had more girls than boys (girl bias)\n\nWrite out the likelihood function given Model 2 for the small set of data in (1). [You could also try BGG, GGB, BBB just for fun.]"
  },
  {
    "objectID": "labs/01_linear_regression/01_banksalary_fillable.html",
    "href": "labs/01_linear_regression/01_banksalary_fillable.html",
    "title": "Linear Regression Review Lab",
    "section": "",
    "text": "Example: Sex discrimination in bank salaries. In the 1970’s, Harris Trust was sued for sex discrimination in the salaries it paid its employees. One approach to addressing this issue was to examine the starting salaries of all skilled, entry-level clerical workers between 1965 and 1975. The data is saved under banksalary.csv, and relevant R code can be found in the STA363_inst_files folder on the home directory of the RStudio Server. banksalary.qmd.\nFirst, we will speculate on what we expect to find and then we will perform an analysis using the data.\nRead the data in and use aview() to see the data, but do not do anything else with the data on the computer until question 6.\n\nIdentify the observational units, the response variable, and explanatory variables.\nGiven the mean starting salary of male workers ($5957) was 16% higher than the mean starting salary of female workers ($5139): Is this enough evidence to conclude sex discrimination exists? If not, what further evidence would you need?\nHow would you expect age, experience, and education to each be related to starting salary?\nWhy might it be important to control for seniority (number of years with the bank) if we are only concerned with the salary when the worker started?\nDo you expect any explanatory variables (including sex) to be closely related to each other? What implications would this have for modeling?\n\nUsing the data…\nOne approach is to construct a good model for beginning salaries while requiring sex as a predictor, to determine the significance of sex after controlling for the other covariates. Then we can explore interactions with sex to see if its effect is consistent across levels of other predictors.\n\nUse the data to address the primary question of interest here using only the beginning salary and sex variables. Be sure to discuss plots and summary statistics first, and then look at test(s) of significance.\nConstruct plots to investigate how each of the potential confounders (age, experience, education) is related to beginning salaries. Describe your findings.\nDoes seniority play a role in the variation of starting salaries? In what way?\nExamine how the explanatory variables (including sex) are related to each other, if at all. What implications would this have for modeling?\nFit a simple linear regression model with starting salary as the response and education as the sole explanatory variable. Interpret the intercept and slope of this model; also interpret the R-squared value. Is there a significant relationship between education and starting salary?\n\nIntercept:\nSlope:\nR2\nSignificance:\n\nDoes model1 from question 10 meet all linear regression assumptions? List each assumption and how you decided if it was met or not.\nIs a model with all 4 confounding variables better than a model with just education? Justify with an appropriate significance test in addition to summary statistics of model performance.\nYou should have noticed that the term for age was not significant in the model3. What does this imply about age and about future modeling steps?\nThe relationship between experience and beginning salary exhibits some curvature. How might it be interpreted in this context? Determine whether a quadratic term in experience improves a model without curvature.\nBased on model6, what conclusions can be drawn about sex discrimination at Harris Trust? Do these conclusions have to be qualified at all, or are they pretty clear cut? Interpret a 95% confidence interval for the male indicator variable in context to help with your response.\nDo any explanatory variables exhibit an interaction with sex. If so, what are the implications for your answer in (15)?\nOften salary data is logged before analysis. Would you recommend logging starting salary in this study? Support your decision analytically.\nRegardless of your answer to (17), provide an interpretation for the coefficient for the male coefficient in model6a after logging starting salary."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Advanced Regression Schedule",
    "section": "",
    "text": "Note: The timeline of topics and assignments might be updated throughout the semester.\n\n\n\n\n\n\n\n\nDay\nDate\nTopic\nSlides\nOutline\nProject\nLab\nHomework\nExam\n\n\n\n\n1\n26 Aug\nSyllabus, Chapter 1 - Review MLR\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2\n27 Aug\nChapter 2 - Likelihoods\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3\n28 Aug\nChapter 3 - Distributions\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4\n29 Aug\nChapter 4 - Poisson Regression\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n5\n30 Aug\nPoisson Regression Mini Project\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n6\n2 Sep\nCh 4 - Poisson Reg. & Ch 5 - GLMs\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n7\n3 Sep\nCh 5 - GLMs & Ch 6 - Logistics Reg.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n8\n4 Sep\nCh 6 - Logistics Regression (no afternoon class)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n9\n5 Sep\nGLM Mini Project\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n10\n6 Sep\nExam\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n11\n9 Sep\nCh 7 - Correlated Data & Ch 8 - Multilevel Models\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n12\n10 Sep\nChapter 8 - Multilevel Models\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n13\n11 Sep\nChapter 8 - Multilevel Models\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n14\n12 Sep\nChapter 9 - Two-Level Longitudinal Data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n15\n13 Sep\nMultilevel Model Practice\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n16\n16 Sep\nPolynomial Regression, Splines, and GAMs\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n17\n17 Sep\nPolynomial Regression, Splines, and GAMs\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n18\n18 Sep\nExam",
    "crumbs": [
      "Course Contents",
      "Schedule & Assignments"
    ]
  },
  {
    "objectID": "hw/08_hw_ch9.html",
    "href": "hw/08_hw_ch9.html",
    "title": "Homework 8",
    "section": "",
    "text": "Each of your assignments will begin with the following steps.\n\nGoing to our RStudio Server at http://turing.cornellcollege.edu:8787/\nCreating a new R project, inside your homework folder on the server, and giving it a sensible name such as homework_8 and having that project in the course folder you created.\nCreate a new quarto document and give it a sensible name such as hw8.\nIn the YAML add the following (add what you don’t have). The embed-resources component will make your final rendered html self-contained.\n\n\n---\ntitle: \"Document title\"\nauthor: \"my name\"\nformat:\n  html:\n    embed-resources: true\n---"
  },
  {
    "objectID": "hw/08_hw_ch9.html#setup",
    "href": "hw/08_hw_ch9.html#setup",
    "title": "Homework 8",
    "section": "",
    "text": "Each of your assignments will begin with the following steps.\n\nGoing to our RStudio Server at http://turing.cornellcollege.edu:8787/\nCreating a new R project, inside your homework folder on the server, and giving it a sensible name such as homework_8 and having that project in the course folder you created.\nCreate a new quarto document and give it a sensible name such as hw8.\nIn the YAML add the following (add what you don’t have). The embed-resources component will make your final rendered html self-contained.\n\n\n---\ntitle: \"Document title\"\nauthor: \"my name\"\nformat:\n  html:\n    embed-resources: true\n---"
  },
  {
    "objectID": "hw/08_hw_ch9.html#instructions",
    "href": "hw/08_hw_ch9.html#instructions",
    "title": "Homework 8",
    "section": "Instructions",
    "text": "Instructions\nBe sure to include the relevant R code as well as full sentences answering each of the questions (i.e. if I ask for the average, you can output the answer in R but also write a full sentence with the answer). Be sure to frequently save your files!\nData for the homework will be in the STA363_inst_files -&gt; data folder."
  },
  {
    "objectID": "hw/08_hw_ch9.html#exercises",
    "href": "hw/08_hw_ch9.html#exercises",
    "title": "Homework 8",
    "section": "Exercises",
    "text": "Exercises\nAll problems are from The main textbook is: Beyond Multiple Linear Regression by Paul Roback and Julie Legler – it is freely available online. Chapters 1-9. Abbreviated BMLR.\nUse the numbering on the left. The codes are for instructor use (Ex: C1).\n\n(C1-5) Walker and Barnes (2001) describe, “Ethnic differences in the effect of parenting on gang involvement and gang delinquency: a longitudinal, hierarchical linear modeling perspective”. In this study, 300 ninth graders from one high school in an urban southeastern city were assessed at the beginning of the school year about their gang activity, the gang activity of their peers, behavior of their parents, and their ethnic and cultural heritage. Then, information about their gang activity was collected at 7 additional occasions during the school year.\n\n\nFor this study:\n\n\nGive the observational units at Level One and Level Two\nList potential explanatory variables at both Level One and Level Two.\n\n\nDescribe the difference between the wide and long formats for longitudinal data in this study.\nDescribe scenarios or research questions in which a lattice plot would be more informative than a spaghetti plot, and other scenarios or research questions in which a spaghetti plot would be preferable to a lattice plot.\nWalker-Barnes and Mason summarize their analytic approach in the following way, where HLM = hierarchical linear models, a synonym for multilevel models:\nThe first series [of analyses] tested whether there was overall change and/or significant individual variability in gang [activity] over time, regardless of parenting behavior, peer behavior, or ethnic and cultural heritage. Second, given the well documented relation between peer and adolescent behavior . . . HLM analyses were conducted examining the effect of peer gang [activity] on [initial gang activity and] changes in gang [activity] over time. Finally, four pairs of analyses were conducted examining the role of each of the four parenting variables on [initial gang activity and] changes in gang [activity].\nThe last series of analyses controlled for peer gang activity and ethnic and cultural heritage, in addition to examining interactions between parenting and ethnic and cultural heritage.\nAlthough the authors examined four parenting behaviors—behavioral control, lax control, psychological control, and parental warmth—they did so one at a time, using four separate multilevel models. Based on their description, write out a sample model from each of the three steps in the series. For each model, (a) write out the two-level model for predicting gang activity, (b) write out the corresponding composite model, and (c) determine how many model parameters (fixed effects and variance components) must be estimated.\nTable 1 shows a portion of Table 2: Results of Hierarchical Linear Modeling Analyses Modeling Gang Involvement from Walker and Barnes (2001). Provide interpretations of significant coefficients in context.\n\n\n\n\nA portion of Table 2: Results of Hierarchical Linear Modeling Analyses Modeling Gang Involvement from Walker-Barnes and Mason (2001).\n\n\nPredictor\nCoefficient\nSE\n\n\n\n\nIntercept (initial status)\n\n\n\n\nBase (intercept for predicting int term)\n-.219\n.160\n\n\nPeer behavior\n.252**\n.026\n\n\nBlack ethnicity\n.671*\n.289\n\n\nWhite/Other ethnicity\n.149\n.252\n\n\nParenting\n.076\n.050\n\n\nBlack ethnicity X parenting\n-.161+\n.088\n\n\nWhite/Other ethnicity X parenting\n-.026\n.082\n\n\nSlope (change)\n\n\n\n\nBase (intercept for predicting slope term)\n.028\n.030\n\n\nPeer behavior\n-.011*\n.005\n\n\nBlack ethnicity\n-.132*\n.054\n\n\nWhite/Other ethnicity\n-.059\n.046\n\n\nParenting\n-.015+\n.009\n\n\nBlack ethnicity X parenting\n.048**\n.017\n\n\nWhite/Other ethnicity X parenting\n.016\n.015\n\n\n\n Table 1: These columns focus on the parenting behavior of psychological control.\n\n\n\n\n Table reports values for coefficients in the final model with all\n\n\n\n\n variables entered. * p&lt;.05; ** p&lt;.01; + p&lt;.10\n\n\n\n\n\n\n\n\n\n\n\n(C6) Differences exist in both sets of boxplots in Figure 9.12. What do these differences imply for multilevel modeling?\n(C7) What implications do the scatterplots in Figures 9.14 (b) and (c) have for multilevel modeling? What implications does the boxplot in Figure 9.14 (a) have?\n(C8) What are the implications of Figure 9.15 for multilevel modeling?\n(C11) In Chapter 8 Model B is called the “random slopes and intercepts model”, while in this chapter Model B is called the “unconditional growth model”. Are these models essentially the same or systematically different? Explain.\n(C12) In Section 9.5.2, why don’t we examine the pseudo R-squared value for Level Two?\n(G1) Curran (1997) collected data on 82 adolescents at three time points starting at age 14 to assess factors that affect teen drinking behavior. Key variables in the data set alcohol.csv (accessed via (Singer, 2003) are as follows:\n\nid = numerical identifier for subject\nage = 14, 15, or 16\ncoa = 1 if the teen is a child of an alcoholic parent; 0 otherwise\nmale = 1 if male; 0 if female\npeer = a measure of peer alcohol use, taken when each subject was 14. This is the square root of the sum of two 6-point items about the proportion of friends who drink occasionally or regularly.\nalcuse = the primary response. Four items—(a) drank beer or wine, (b) drank hard liquor, (c) 5 or more drinks in a row, and (d) got drunk—were each scored on an 8-point scale, from 0=“not at all” to 7=“every day”. Then alcuse is the square root of the sum of these four items.\n\n\nPrimary research questions included: Do trajectories of alcohol use differ by parental alcoholism? Do trajectories of alcohol use differ by peer alcohol use?\n\n\nIdentify Level One and Level Two predictors.\nPerform a quick EDA. What can you say about the shape of alcuse, and the relationship between alcuse and coa, male, and peer? Appeal to plots and summary statistics in making your statements.\nGenerate a plot as in Figure 9.4 with alcohol use over time for all 82 subjects. Comment.\nGenerate three spaghetti plots with loess fits similar to Figure 9.7 (one for coa, one for male, and one after creating a binary variable from peer). Comment on what you can conclude from each plot.\nFit a linear trend to the data from each of the 82 subjects using age as the time variable. Generate histograms as in Figure 9.10 showing the results of these 82 linear regression lines, and generate pairs of boxplots as in Figure 9.12 for coa and male. No commentary necessary. [Hint: to produce Figure Figure 9.12, you will need a data frame with one observation per subject.]\nRepeat (e) using centered age (age14 = age - 14) as the time variable. Also generate a pair of scatterplots as in Figure 9.14 for peer alcohol use. Comment on trends you observe in these plots. [Hint: after forming age14, append it to your current data frame.]\nDiscuss similarities and differences between (e) and (f). Why does using age14 as the time variable make more sense in this example?\n(Model A) Run an unconditional means model. Report and interpret the intraclass correlation coefficient.\n(Model B) Run an unconditional growth model with age14 as the time variable at Level One. Report and interpret estimated fixed effects, using proper notation. Also report and interpret a pseudo R-squared value.\n(Model C) Build upon the unconditional growth model by adding the effects of having an alcoholic parent and peer alcohol use in both Level Two equations. Report and interpret all estimated fixed effects, using proper notation.\n(Model D) Remove the child of an alcoholic indicator variable as a predictor of slope in Model C (it will still be a predictor of intercept). Write out Model D as both a two-level and a composite model using proper notation (including error distributions); how many parameters (fixed effects and variance components) must be estimated? Compare Model D to Model C using an appropriate method and state a conclusion."
  },
  {
    "objectID": "hw/06_hw_ch5_6.html",
    "href": "hw/06_hw_ch5_6.html",
    "title": "Homework 6",
    "section": "",
    "text": "Each of your assignments will begin with the following steps.\n\nGoing to our RStudio Server at http://turing.cornellcollege.edu:8787/\nCreating a new R project, inside your homework folder on the server, and giving it a sensible name such as homework_6 and having that project in the course folder you created.\nCreate a new quarto document and give it a sensible name such as hw6.\nIn the YAML add the following (add what you don’t have). The embed-resources component will make your final rendered html self-contained.\n\n---\ntitle: \"Document title\"\nauthor: \"my name\"\nformat:\n  html:\nembed-resources: true\n---"
  },
  {
    "objectID": "hw/06_hw_ch5_6.html#setup",
    "href": "hw/06_hw_ch5_6.html#setup",
    "title": "Homework 6",
    "section": "",
    "text": "Each of your assignments will begin with the following steps.\n\nGoing to our RStudio Server at http://turing.cornellcollege.edu:8787/\nCreating a new R project, inside your homework folder on the server, and giving it a sensible name such as homework_6 and having that project in the course folder you created.\nCreate a new quarto document and give it a sensible name such as hw6.\nIn the YAML add the following (add what you don’t have). The embed-resources component will make your final rendered html self-contained.\n\n---\ntitle: \"Document title\"\nauthor: \"my name\"\nformat:\n  html:\nembed-resources: true\n---"
  },
  {
    "objectID": "hw/06_hw_ch5_6.html#instructions",
    "href": "hw/06_hw_ch5_6.html#instructions",
    "title": "Homework 6",
    "section": "Instructions",
    "text": "Instructions\nBe sure to include the relevant R code as well as full sentences answering each of the questions (i.e. if I ask for the average, you can output the answer in R but also write a full sentence with the answer). Be sure to frequently save your files!\nData for the homework will be in the STA363_inst_files -&gt; data folder."
  },
  {
    "objectID": "hw/06_hw_ch5_6.html#exercises",
    "href": "hw/06_hw_ch5_6.html#exercises",
    "title": "Homework 6",
    "section": "Exercises",
    "text": "Exercises\nAll problems are from The main textbook is: Beyond Multiple Linear Regression by Paul Roback and Julie Legler – it is freely available online. Chapters 1-9. Abbreviated BMLR.\nUse the numbering on the left. The codes are for instructor use (Ex: C1)."
  },
  {
    "objectID": "hw/06_hw_ch5_6.html#part-1-glms",
    "href": "hw/06_hw_ch5_6.html#part-1-glms",
    "title": "Homework 6",
    "section": "Part 1: GLMs",
    "text": "Part 1: GLMs"
  },
  {
    "objectID": "hw/06_hw_ch5_6.html#exercise-4",
    "href": "hw/06_hw_ch5_6.html#exercise-4",
    "title": "Homework 6",
    "section": "Exercise 4",
    "text": "Exercise 4\n\n(C1) For each distribution below,\n\n\nWrite the pmf or pdf in one-parameter exponential form, if possible.\nDescribe an example of a setting where this random variable might be used.\nIdentify the canonical link function, and\n(For Fun) Compute \\(\\mu = -\\frac{c'(\\theta)}{b'(\\theta)}\\) and \\(\\sigma^2 = \\frac{b''(\\theta)c'(\\theta)-c''(\\theta)b'(\\theta)}{[b'(\\theta)]^3}\\) and compare with known \\(\\operatorname{E}(Y)\\) and \\(\\operatorname{Var}(Y)\\).\n\n\nBinomial (for fixed \\(n\\)): Y = number of successes in \\(n\\) independent, identical trials\n\n\\[P(Y=y;p)=\\left(\\begin{array} {c}  n\\\\y  \\end{array}\\right) p^y(1-p)^{(n-y)}\\]\n\nGamma (for fixed \\(r\\)): Y = time spent waiting for the \\(r^{th}\\) event in a Poisson process with an average rate of \\(\\lambda\\) events per unit of time\n\n\\[f(y; \\lambda) = \\frac{\\lambda^r}{\\Gamma(r)} y^{r-1} e^{-\\lambda y}\\]"
  },
  {
    "objectID": "hw/06_hw_ch5_6.html#part-2-logistic-regression",
    "href": "hw/06_hw_ch5_6.html#part-2-logistic-regression",
    "title": "Homework 6",
    "section": "Part 2: Logistic Regression",
    "text": "Part 2: Logistic Regression\n\nExercise 1\n\n(C2) Interpret the odds ratios in the following abstract.\n\nDaycare Centers and Respiratory Health [@Nafstad1999]\n\nObjective. To estimate the effects of the type of daycare on respiratory health in preschool children.\nMethods. A population-based, cross-sectional study of Oslo children born in 1992 was conducted at the end of 1996. A self-administered questionnaire inquired about daycare arrangements, environmental conditions, and family characteristics (n = 3853; response rate, 79%).\nResults. In a logistic regression controlling for confounding, children in daycare centers had more often nightly cough (adjusted odds ratio, 1.89; 95% confidence interval 1.34-2.67), and blocked or runny nose without common cold (1.55; 1.07-1.61) during the past 12 months compared with children in home care.\n\n\n\nExercise 2\n\n(C3) Construct a table and calculate the corresponding odds and odds ratios. Comment on the reported and calculated results in this New York Times article from @Kolata2009.\n\n\nIn November, the Centers for Disease Control and Prevention published a paper reporting that babies conceived with IVF, or with a technique in which sperm are injected directly into eggs, have a slightly increased risk of several birth defects, including a hole between the two chambers of the heart, a cleft lip or palate, an improperly developed esophagus and a malformed rectum. The study involved 9,584 babies with birth defects and 4,792 babies without. Among the mothers of babies without birth defects, 1.1% had used IVF or related methods, compared with 2.4% of mothers of babies with birth defects.\nThe findings are considered preliminary, and researchers say they believe IVF does not carry excessive risks. There is a 3% chance that any given baby will have a birth defect.\n\n\n\nExercise 3\n\n(G4) Birdkeeping and lung cancer: a retrospective observational study. A 1972-1981 health survey in The Hague, Netherlands, discovered an association between keeping pet birds and increased risk of lung cancer. To investigate birdkeeping as a risk factor, researchers conducted a case-control study of patients in 1985 at four hospitals in The Hague. They identified 49 cases of lung cancer among patients who were registered with a general practice, who were age 65 or younger, and who had resided in the city since 1965. Each patient (case) with cancer was matched with two control subjects (without cancer) by age and sex. Further details can be found in @Holst1988.\n\nAge, sex, and smoking history are all known to be associated with lung cancer incidence. Thus, researchers wished to determine after age, sex, socioeconomic status, and smoking have been controlled for, is an additional risk associated with birdkeeping? The data [@Ramsey2002] is found in birdkeeping.csv, and the variables are listed below. In addition, R code at the end of the problem can be used to input the data and create additional useful variables.\n\nfemale = sex (1 = Female, 0 = Male)\nage = age, in years\nhighstatus = socioeconomic status (1 = High, 0 = Low), determined by the occupation of the household’s primary wage earner\nyrsmoke = years of smoking prior to diagnosis or examination\ncigsday = average rate of smoking, in cigarettes per day\nbird = indicator of birdkeeping (1 = Yes, 0 = No), determined by whether or not there were caged birds in the home for more than 6 consecutive months from 5 to 14 years before diagnosis (cases) or examination (controls)\ncancer = indicator of lung cancer diagnosis (1 = Cancer, 0 = No Cancer)\n\n\nPerform an exploratory data analysis to see how each explanatory variable is related to the response (cancer). Summarize each relationship in one sentence.\nFor quantitative explanatory variables (age, yrsmoke, cigsday), produce a cdplot, a boxplot, and summary statistics by cancer diagnosis.\n\n\nFor categorical explanatory variables (female or sex, highstatus or socioecon_status, bird or keep_bird), produce a segmented bar chart and an appropriate table of proportions showing the relationship with cancer diagnosis.\n\n\nIn (a), you should have found no relationship between whether or not a patient develops lung cancer and either their age or sex. Why might this be? What implications will this have on your modeling?\nBased on a two-way table with keeping birds and developing lung cancer from (a), find an unadjusted odds ratio comparing birdkeepers to non-birdkeepers and interpret this odds ratio in context. (Note: an unadjusted odds ratio is found by not controlling for any other variables.) Also, find an analogous relative risk and interpret it in context as well.\nAre the elogits reasonably linear relating number of years smoked to the estimated log odds of developing lung cancer? Demonstrate with an appropriate plot.\nDoes there appear to be an interaction between number of years smoked and whether the subject keeps a bird? Demonstrate with an interaction plot and a coded scatterplot with empirical logits on the y-axis.\n\n\nBefore answering the next questions, fit logistic regression models in R with cancer as the response and the following sets of explanatory variables:\n\nmodel1 = age, yrsmoke, cigsday, female, highstatus, bird\nmodel2 = yrsmoke, cigsday, highstatus, bird\nmodel4 = yrsmoke, bird\nmodel5 = the complete second order version of model4 (add squared terms and an interaction)\nmodel6 = yrsmoke, bird, yrsmoke:bird\n\n\nIs there evidence that we can remove age and female from our model? Perform an appropriate test comparing model1 to model2; give a test statistic and p-value, and state a conclusion in context.\nIs there evidence that the complete second order version of model4 improves its performance? Perform an appropriate test comparing model4 to model5; give a test statistic and p-value, and state a conclusion in context.\nCarefully interpret each of the four model coefficients in model6 in context.\nIf you replaced yrsmoke everywhere it appears in model6 with a mean-centered version of yrsmoke, tell what would change among these elements: the 4 coefficients, the 4 p-values for coefficients, and the residual deviance.\nmodel4 is a potential final model based on this set of explanatory variables. Find and carefully interpret 95% confidence intervals based on profile likelihoods for the coefficients of yrsmoke and bird.\nHow does the adjusted odds ratio for birdkeeping from model4 compare with the unadjusted odds ratio you found in (c)? Is birdkeeping associated with a significant increase in the odds of developing lung cancer, even after adjusting for other factors?\nUse the categorical variable years_factor based on yrsmoke and replace yrsmoke in model4 with your new variable to create model4a. First, interpret the coefficient for years_factorOver 25 years in context. Then tell if you prefer model4 with years smoked as a numeric predictor or model4a with years smoked as a categorical predictor, and explain your reasoning.\nDiscuss the scope of inference in this study. Can we generalize our findings beyond the subjects in this study? Can we conclude that birdkeeping causes increased odds of developing lung cancer? Do you have other concerns with this study design or the analysis you carried out?\nRead the article that appeared in the British Medical Journal. What similarities and differences do you see between their analyses and yours? What are a couple of things you learned from the article that weren’t apparent in the short summary at the beginning of the assignment.\n\n\nbirds &lt;- read_csv(\"data/birdkeeping.csv\") %&gt;%\n  mutate(sex = ifelse(female == 1, \"Female\", \"Male\"),\n         socioecon_status = ifelse(highstatus == 1, \n                                   \"High\", \"Low\"),\n         keep_bird = ifelse(bird == 1, \"Keep Bird\", \"No Bird\"),\n         lung_cancer = ifelse(cancer == 1, \"Cancer\", \n                              \"No Cancer\")) %&gt;%\n  mutate(years_factor = cut(yrsmoke, \n                            breaks = c(-Inf, 0, 25, Inf),\n            labels = c(\"No smoking\", \"1-25 years\", \n                       \"Over 25 years\")))"
  },
  {
    "objectID": "hw/04_hw_ch4.html",
    "href": "hw/04_hw_ch4.html",
    "title": "Homework 4",
    "section": "",
    "text": "Each of your assignments will begin with the following steps.\n\nGoing to our RStudio Server at http://turing.cornellcollege.edu:8787/\nCreating a new R project, inside your homework folder on the server, and giving it a sensible name such as homework_4 and having that project in the course folder you created.\nCreate a new quarto document and give it a sensible name such as hw4.\nIn the YAML add the following (add what you don’t have). The embed-resources component will make your final rendered html self-contained.\n\n---\ntitle: \"Document title\"\nauthor: \"my name\"\nformat:\n  html:\nembed-resources: true\n---"
  },
  {
    "objectID": "hw/04_hw_ch4.html#setup",
    "href": "hw/04_hw_ch4.html#setup",
    "title": "Homework 4",
    "section": "",
    "text": "Each of your assignments will begin with the following steps.\n\nGoing to our RStudio Server at http://turing.cornellcollege.edu:8787/\nCreating a new R project, inside your homework folder on the server, and giving it a sensible name such as homework_4 and having that project in the course folder you created.\nCreate a new quarto document and give it a sensible name such as hw4.\nIn the YAML add the following (add what you don’t have). The embed-resources component will make your final rendered html self-contained.\n\n---\ntitle: \"Document title\"\nauthor: \"my name\"\nformat:\n  html:\nembed-resources: true\n---"
  },
  {
    "objectID": "hw/04_hw_ch4.html#instructions",
    "href": "hw/04_hw_ch4.html#instructions",
    "title": "Homework 4",
    "section": "Instructions",
    "text": "Instructions\nBe sure to include the relevant R code as well as full sentences answering each of the questions (i.e. if I ask for the average, you can output the answer in R but also write a full sentence with the answer). Be sure to frequently save your files!\nData for the homework will be in the STA363_inst_files -&gt; data folder."
  },
  {
    "objectID": "hw/04_hw_ch4.html#exercises",
    "href": "hw/04_hw_ch4.html#exercises",
    "title": "Homework 4",
    "section": "Exercises",
    "text": "Exercises\nAll problems are from The main textbook is: Beyond Multiple Linear Regression by Paul Roback and Julie Legler – it is freely available online. Chapters 1-9. Abbreviated BMLR.\nUse the numbering on the left. The codes are for instructor use (Ex: C1).\n\nExercise 1 & 2\nExercises 1 & 2 involve predicting a response using one or more explanatory variables, where these examples have response variables that are counts per some unit of time or space. List the response (both what is being counted and over what unit of time or space) and relevant explanatory variables.\n\n\nExercise 1\n\n(C1) Are the number of motorcycle deaths in a given year related to a state’s helmet laws?\n\n\n\nExercise 2\n\n(C2) Does the number of employers conducting on-campus interviews during a year differ for public and private colleges?\n\n\n\nExercise 3\n\n(C5) Models of the form \\(Y_i=\\beta_0+\\beta_1X_i+\\epsilon_i, \\epsilon_i \\sim iidN(0,\\sigma)\\) are fit using the method of least squares. What method is used to fit Poisson regression models?\n\n\n\nExercise 4\n\n(C6) What should be done before adjusting for overdispersion?\n\n\n\nExercise 5\n\n(C7) Why are quasi-Poisson models used, and how do the results typically compare for corresponding models using regular Poisson regression?\n\n\n\nExercise 6\n\n(C8) Why is the log of mean counts, log(\\(\\bar{Y}\\)), not \\(\\bar{Y}\\), plotted against X when assessing the assumptions for Poisson regression?\n\n\n\nExercise 7\n\n(C9) How can the assumption of mean=variance be checked for Poisson regression? What if there are not many repeated observations at each level of X?\n\n\n\nExercise 8\n\n(C10) Is it possible that a predictor is significant for a model fit using Poisson regression, but not for a model for the same data fit using quasi-Poisson regression? Explain.\n\n\n\nExercise 9\n\n(C11) Fish (or, as they say in French, poisson). A state wildlife biologist collected data from 250 park visitors as they left at the end of their stay. Each was asked to report the number of fish they caught during their one-week stay. On average, visitors caught 21.5 fish per week.\n\n\nDefine the response.\nWhat are the possible values for the response?\nWhat does \\(\\lambda\\) represent?\n\n\n\nExercise 10\n\n(G2) Elephant mating. How does age affect male elephant mating patterns? An article by @Poole1989 investigated whether mating success in male elephants increases with age and whether there is a peak age for mating success. To address this question, the research team followed 41 elephants for one year and recorded both their ages and their number of matings. The data [@Ramsey2002] is found in elephant.csv, and the variables are:\n\nMATINGS = the number of matings in a given year\nAGE = the age of the elephant in years.\n\n\nCreate a histogram of MATINGS. Is there preliminary evidence that number of matings could be modeled as a Poisson response? Explain.\nPlot MATINGS by AGE. Add a least squares line. Is there evidence that modeling matings using a linear regression with age might not be appropriate? Explain. (Hints: fit a smoother; check residual plots).\nFor each age, calculate the mean number of matings. Take the log of each mean and plot it by AGE.\n\nWhat assumption can be assessed with this plot?\nIs there evidence of a quadratic trend on this plot?\n\nFit a Poisson regression model with a linear term for AGE. Exponentiate and then interpret the coefficient for AGE.\nConstruct a 95% confidence interval for the slope and interpret in context (you may want to exponentiate endpoints).\nAre the number of matings significantly related to age? Test with\n\na Wald test and\na drop in deviance test.\n\nAdd a quadratic term in AGE to determine whether there is a maximum age for the number of matings for elephants. Is a quadratic model preferred to a linear model? To investigate this question, use\n\na Wald test and\na drop in deviance test.\n\nWhat can we say about the goodness-of-fit of the model with age as the sole predictor? Compare the residual deviance for the linear model to a \\(\\chi^2\\) distribution with the residual model degrees of freedom.\nFit the linear model using quasi-Poisson regression. (Why?)\n\nHow do the estimated coefficients change?\nHow do the standard errors change?\nWhat is the estimated dispersion parameter?\nAn estimated dispersion parameter greater than 1 suggests overdispersion. When adjusting for overdispersion, are you more or less likely to obtain a significant result when testing coefficients? Why?"
  },
  {
    "objectID": "hw/02_hw_ch2.html",
    "href": "hw/02_hw_ch2.html",
    "title": "Homework 2",
    "section": "",
    "text": "Each of your assignments will begin with the following steps.\n\nGoing to our RStudio Server at http://turing.cornellcollege.edu:8787/\nCreating a new R project, inside your homework folder on the server, and giving it a sensible name such as homework_2 and having that project in the course folder you created.\nCreate a new quarto document and give it a sensible name such as hw2.\nIn the YAML add the following (add what you don’t have). The embed-resources component will make your final rendered html self-contained.\n\n---\ntitle: \"Document title\"\nauthor: \"my name\"\nformat:\n  html:\nembed-resources: true\n---"
  },
  {
    "objectID": "hw/02_hw_ch2.html#setup",
    "href": "hw/02_hw_ch2.html#setup",
    "title": "Homework 2",
    "section": "",
    "text": "Each of your assignments will begin with the following steps.\n\nGoing to our RStudio Server at http://turing.cornellcollege.edu:8787/\nCreating a new R project, inside your homework folder on the server, and giving it a sensible name such as homework_2 and having that project in the course folder you created.\nCreate a new quarto document and give it a sensible name such as hw2.\nIn the YAML add the following (add what you don’t have). The embed-resources component will make your final rendered html self-contained.\n\n---\ntitle: \"Document title\"\nauthor: \"my name\"\nformat:\n  html:\nembed-resources: true\n---"
  },
  {
    "objectID": "hw/02_hw_ch2.html#instructions",
    "href": "hw/02_hw_ch2.html#instructions",
    "title": "Homework 2",
    "section": "Instructions",
    "text": "Instructions\nBe sure to include the relevant R code as well as full sentences answering each of the questions (i.e. if I ask for the average, you can output the answer in R but also write a full sentence with the answer). Be sure to frequently save your files!\nData for the homework will be in the STA363_inst_files -&gt; data folder."
  },
  {
    "objectID": "hw/02_hw_ch2.html#exercises",
    "href": "hw/02_hw_ch2.html#exercises",
    "title": "Homework 2",
    "section": "Exercises",
    "text": "Exercises\nAll problems are from The main textbook is: Beyond Multiple Linear Regression by Paul Roback and Julie Legler – it is freely available online. Chapters 1-9.. Abbreviated BMLR.\nUse the numbering on the left. The codes are for instructor use (Ex: C1).\n\nExercise 1\n\n(C1) Suppose we plan to use data to estimate one parameter, \\(p_B\\).\n\nWhen using a likelihood to obtain an estimate for the parameter, which is preferred: a large or a small likelihood value? Why?\nThe height of a likelihood curve is the probability of the data for the given parameter. The horizontal axis represents different possible parameter values. Does the area under the likelihood curve for an interval from .25 to .75 equal the probability that the true probability of a boy is between 0.25 and 0.75?\n\n\n\n\nExercise 2\n\n(C2) Suppose the families with an “only child” were excluded for the Sex Conditional Model. How might the estimates for the three parameters be affected? Would it still be possible to perform a Likelihood Ratio Test to compare the Sex Unconditional and Sex Conditional Models? Why or why not?\n\n\n\nExercise 3\n\n(G2) Case 3 In Case 1 we used hypothetical data with 30 boys and 20 girls. Case 2 was a much larger study with 600 boys and 400 girls. Consider Case 3, a hypothetical data set with 6000 boys and 4000 girls.\n\nUse the methods for Case 1 and Case 2 and determine the MLE for \\(p_B\\) for the case 3 independence model. Compare your result to the MLEs for Cases 1 and 2.\nDescribe how the graph of the log-likelihood for Case 3 would compare to the log-likelihood graphs for Cases 1 and 2.\nCompute the log-likelihood for Case 3. Why is it incorrect to perform an LRT comparing Cases 1, 2, and 3?\n\n\n\n\nExercise 4\n\n(G3) Write out an expression for the likelihood of seeing our NLSY data (5,416 boys and 5,256 girls) if the true probability of a boy is:\n\n\\(p_B=0.5\\)\n\n\\(p_B=0.45\\)\n\n\\(p_B= 0.55\\)\n\n\\(p_B= 0.5075\\)\n\n\n\nCompute the value of the log-likelihood for each of the values of \\(p_B\\) above.\nWhich of these four possibilities, \\(p_B=0.45, p_B=0.5,  p_B=0.55,\\) or \\(p_B=0.5075\\) would be the best estimate of \\(p_B\\) given what we observed (our data)?\n\n\n\n\nExercise 5\n\n(O2) The hot hand in basketball. @Gilovich1985 wrote a controversial but compelling article claiming that there is no such thing as “the hot hand” in basketball. That is, there is no empirical evidence that shooters have stretches where they are more likely to make consecutive shots, and basketball shots are essentially independent events. One of the many ways they tested for evidence of a “hot hand” was to record sequences of shots for players under game conditions and determine if players are more likely to make shots after made baskets than after misses. For instance, assume we recorded data from one player’s first 5 three-point attempts over a 5-game period. We can assume games are independent, but we’ll consider two models for shots within a game:\n\nNo Hot Hand (1 parameter): \\(p_B\\) = probability of making a basket (thus \\(1-p_B\\) = probability of not making a basket).\nHot Hand (2 parameters): \\(p_B\\) = probability of making a basket after a miss (or the first shot of a game); \\(p_{B|B}\\) = probability of making a basket after making the previous shot.\n\n\nFill out Table @ref(tab:hothandchp2)—write out the contribution of each game to the likelihood for both models along with the total likelihood for each model.\nGiven that, for the No Hot Hand model, \\(\\textrm{Lik}(p_B)=p_B^{10}(1-p_B)^{15}\\) for the 5 games where we collected data, how do we know that 0.40 (the maximum likelihood estimator (MLE) of \\(p_B\\)) is a better estimate than, say, 0.30?\nFind the MLEs for the parameters in each model, and then use those MLEs to determine if there’s significant evidence that the hot hand exists.\n\n\n\n\n\nData for Open-ended Exercise 2. (B = made basket. M = missed basket.)\n\n\nGame\nFirst 5 shots\nLikelihood (No Hot Hand)\nLikelihood (Hot Hand)\n\n\n\n\n1\nBMMBB\n\n\n\n\n2\nMBMBM\n\n\n\n\n3\nMMBBB\n\n\n\n\n4\nBMMMB\n\n\n\n\n5\nMMMMM\n\n\n\n\nTotal"
  },
  {
    "objectID": "course-support.html",
    "href": "course-support.html",
    "title": "Course support",
    "section": "",
    "text": "Most of you will need help at some point and we want to make sure you can identify when that is without getting too frustrated and feel comfortable seeking help.",
    "crumbs": [
      "Course information",
      "Support"
    ]
  },
  {
    "objectID": "course-support.html#lectures-and-labs",
    "href": "course-support.html#lectures-and-labs",
    "title": "Course support",
    "section": "Lectures and labs",
    "text": "Lectures and labs\nIf you have a question during lecture or lab, feel free to ask it! There are likely other students with the same question, so by asking you will create a learning opportunity for everyone.",
    "crumbs": [
      "Course information",
      "Support"
    ]
  },
  {
    "objectID": "course-support.html#office-hours",
    "href": "course-support.html#office-hours",
    "title": "Course support",
    "section": "Office hours",
    "text": "Office hours\nYou are encouraged to attend office hours during the times posted on the home page to ask questions about the course content and assignments. A lot of questions are most effectively answered in person, so office hours are a valuable resource. I encourage every one of you to take advantage of this resource! Pledge to stop by during office hours at least once during the first few days of class. If you truly have no questions to ask, just stop by and say hi and introduce yourself. You can find a list of your professor’s office hours here.\n\nProfessor Email\nIf you are not available during office hours times or have a questions later in the evening or other times outside of class, email your professor at tgeorge@cornellcollege.edu. If your question involves code - it is very likely you will need to meet with him to get help. Please reach out with any concerns you have during the course!",
    "crumbs": [
      "Course information",
      "Support"
    ]
  },
  {
    "objectID": "course-support.html#academic-support",
    "href": "course-support.html#academic-support",
    "title": "Course support",
    "section": "Academic support",
    "text": "Academic support\n\nQuantitative Reasoning Studio (QRS)\nThere are times you may need help outside of class or office hours. Or, maybe you need something explained differently. In those instances, I encourage you to visit the Quantitative Reasoning Studio in Cole Library room 322. The Quantitative Reasoning Studio (QRS) offers free tutoring to all students at Cornell College. There will be at least 1 peer tutor who has taken this course and will be able to help you if you arrive at a time they are working. Feel free to email Jessica Johanningmeier at QRS@cornellcollege.edu to ask when the tutor for this class will be available. They often will have a schedule posted on the wall in the studio.\n\n\nQRS Hours\n\n\n\nDay(s)\nTimes\n\n\n\n\nMonday-Thursday\n8 a.m. - 5 p.m. and 7 p.m. - 10 p.m.\n\n\nFriday\n8 a.m. - 5 p.m.\n\n\nSunday\n3 p.m. - 5 p.m. and 7 p.m. - 10 p.m.\n\n\n\n\n\nDungy Writing Studio\nFor help with your writing, visit the Dungy Writing Studio. You can make online appointments individually or in groups to get help with items such as your group project. If you have any questions about the studio, email Dungy Writing Studio Director and Director of Fellowships and Scholarships, Laura Farmer, at lfarmer@cornellcollege.edu.\n\n\nWriting Studio Hours\n\n\n\nDay(s)\nTimes\n\n\n\n\nMonday-Thursday\n8 a.m. - 5 p.m. and 7 p.m. - 10 p.m.\n\n\nFriday\n8 a.m. - 5 p.m.\n\n\nSunday\n1 p.m. - 5 p.m.\n\n\n\n\n\nAcademic Support Services\nThere are a variety of support services offered to all students. See more information at: https://www.cornellcollege.edu/academics/support-services/index.shtml&gt;",
    "crumbs": [
      "Course information",
      "Support"
    ]
  },
  {
    "objectID": "course-support.html#ebersole-health-and-wellbeing-center",
    "href": "course-support.html#ebersole-health-and-wellbeing-center",
    "title": "Course support",
    "section": "Ebersole Health and Wellbeing Center",
    "text": "Ebersole Health and Wellbeing Center\nThe mission of Cornell College Student Health Services complements the mission of the college by promoting the optimal well-being of students. We do this by:\n\nproviding and coordinating quality health care services\nadvocating for students in their pursuit of health and wellness\npreparing students to be their own health advocates and informed consumers of appropriate health care services\nproviding health education to promote the development of healthy lifestyles\n\nThe Student Health Center is located in the Ebersole Building, directly south of the Thomas Commons. Appointments are preferred. You can schedule an appointment online or by phone at 319-895-4292. Walk-ins will be accommodated as time permits. Appointments with the nurse are free.",
    "crumbs": [
      "Course information",
      "Support"
    ]
  },
  {
    "objectID": "course-support.html#technology-support",
    "href": "course-support.html#technology-support",
    "title": "Course support",
    "section": "Technology Support",
    "text": "Technology Support\nIf you have issues with your computer during the block, IT may be able to help. Please submit a ticket.",
    "crumbs": [
      "Course information",
      "Support"
    ]
  },
  {
    "objectID": "course-support.html#course-materials-costs",
    "href": "course-support.html#course-materials-costs",
    "title": "Course support",
    "section": "Course materials costs",
    "text": "Course materials costs\nThere are no costs associated with this course. All readings will come from freely available, open resources (open-source textbooks, journal articles, etc.).",
    "crumbs": [
      "Course information",
      "Support"
    ]
  },
  {
    "objectID": "course-links.html",
    "href": "course-links.html",
    "title": "Useful links",
    "section": "",
    "text": "RStudio Server\n🔗 on Cornell College Cluster\n\n\nCourse GitHub organization\n🔗 on GitHub\n\n\nGradebook\n🔗 on Moodle",
    "crumbs": [
      "Course Contents",
      "Useful links"
    ]
  },
  {
    "objectID": "course-links.html#course-links",
    "href": "course-links.html#course-links",
    "title": "Useful links",
    "section": "",
    "text": "RStudio Server\n🔗 on Cornell College Cluster\n\n\nCourse GitHub organization\n🔗 on GitHub\n\n\nGradebook\n🔗 on Moodle",
    "crumbs": [
      "Course Contents",
      "Useful links"
    ]
  },
  {
    "objectID": "course-links.html#other-useful-links",
    "href": "course-links.html#other-useful-links",
    "title": "Useful links",
    "section": "Other Useful Links",
    "text": "Other Useful Links\n\nData Wrangling and Viz Interactive Tutorials\nRStudio Cheatsheets\nIntroduction to dplyr\nR Date Examples\nNY Times Cornell College Sign-up\nR for Data Science 2nd Ed\nQuarto Documentation\nData visualization\n\nggplot2 Reference\nggplot2: Elegant Graphics for Data Analysis\nData Visualization: A Practice Introduction\nPatchwork R Package",
    "crumbs": [
      "Course Contents",
      "Useful links"
    ]
  },
  {
    "objectID": "course-links.html#data-links",
    "href": "course-links.html#data-links",
    "title": "Useful links",
    "section": "Data Links",
    "text": "Data Links\n\nTidyTuesday\nR Data Sources for Regression Analysis\nFiveThirtyEight data\nAmazon Registry of Open Data\nOpen data StackExchange\nData.gov\nUS Census\nNew York City data\nGeorge Mason University Data Link List\nToward Data Science list of Data Sources\nNHS Scotland Open Data\nEdinburgh Open Data\nOpen access to Scotland’s official statistics\nBikeshare data portal\nUK Gov Data\nKaggle datasets\nOpenIntro datasets\nAwesome public datasets\nYouth Risk Behavior Surveillance System (YRBSS)\nPRISM Data Archive Project\nHarvard Dataverse\nAndrew G. Reiter Poly Scie Datasets\nEuropean Statistics\nStatistics Canada\nPew Research\nUNICEF\nCDC\nWorld Bank\nElection Studies\nU.S. Data\nWorld Health Organization\nThe National Bureau of Economic Research\nInternational Monetary Fund\nGeneral Social Survey\nUnited Nations Data\nUnited Nations Statistics Division\nState of Iowa Open Geospatial Data\nIf you know of others, let me know!",
    "crumbs": [
      "Course Contents",
      "Useful links"
    ]
  },
  {
    "objectID": "computing-access.html",
    "href": "computing-access.html",
    "title": "Computing access",
    "section": "",
    "text": "To access computing resources the course, Introduction to Data Science, offered by the Cornell College Department of Mathematics and Statistics, go to the RStudio Server while on campus and connected to campus internet.\nYour account will be pre-created before the class begins and will use your Cornell College username. The default password will be shared in class and you will need to change it.",
    "crumbs": [
      "Course information",
      "R/RStudio Access"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "course-instructor.html",
    "href": "course-instructor.html",
    "title": "Instructor",
    "section": "",
    "text": "Dr. Tyler George (he/him) is a Assistant Professor of Statistics at Cornell College. He received his PhD in Statistics and Analytics from Central Michigan University. During his PhD he also studied mathematics and statistics education. His dissertation work involved creating a new lack of fit test for linear regression models. His interests are broadly in statistics, data science and best pedagogy to teach them.\n\n\n\nOffice hours\nLocation\n\n\n\n\nMonday - Thursday 3:05pm - 4:05pm\nWest 311\n\n\nOther Times by Appointment\nWest 311\n\n\n\nOffice hours are for STUDENTS. Please take advantage of them to get help with class, advising, and/or getting to know your professor! If you miss class, check out course calendar to verify there have been no changes.",
    "crumbs": [
      "Course information",
      "Instructor"
    ]
  },
  {
    "objectID": "course-instructor.html#instructor",
    "href": "course-instructor.html#instructor",
    "title": "Instructor",
    "section": "",
    "text": "Dr. Tyler George (he/him) is a Assistant Professor of Statistics at Cornell College. He received his PhD in Statistics and Analytics from Central Michigan University. During his PhD he also studied mathematics and statistics education. His dissertation work involved creating a new lack of fit test for linear regression models. His interests are broadly in statistics, data science and best pedagogy to teach them.\n\n\n\nOffice hours\nLocation\n\n\n\n\nMonday - Thursday 3:05pm - 4:05pm\nWest 311\n\n\nOther Times by Appointment\nWest 311\n\n\n\nOffice hours are for STUDENTS. Please take advantage of them to get help with class, advising, and/or getting to know your professor! If you miss class, check out course calendar to verify there have been no changes.",
    "crumbs": [
      "Course information",
      "Instructor"
    ]
  },
  {
    "objectID": "course-overview.html",
    "href": "course-overview.html",
    "title": "Advanced Regression",
    "section": "",
    "text": "Course Description\nFollowing a second regression course, this class will begin with a review of multiple linear regression, but now using R. New topics will include probability distributions, likelihoods, differentiating binary vs binomial logistic regression, and poisson regression including its variants. The class of generalized linear models will then be presented which unifies all past modeling approaches. All methods are presented using realistic case studies. Conducting and communicating the modeling process including exploratory data analysis, model exploration and selection, and inferences are all emphasized.",
    "crumbs": [
      "Course information",
      "Overview"
    ]
  },
  {
    "objectID": "hw/01_hw_ch1.html",
    "href": "hw/01_hw_ch1.html",
    "title": "Homework 1",
    "section": "",
    "text": "Go to our RStudio Server at http://turing.cornellcollege.edu:8787/\nGo to your class folder you made named “STA363”\nCreate a new folder called FirstInitial_Last_Initial_HW with your initials.\n\n\n\n\nEach of your assignments will begin with the following steps.\n\nFinding the instructions on our website: https://stats-tgeorge.github.io/STA363_AdvReg/\nGoing to our RStudio Server at http://turing.cornellcollege.edu:8787/\nCreating a new project. and giving it a sensible name such as homework_1 and having that project in the course folder you created.\nCreate a new quarto document and give it a sensible name such as hw1.\nIn the YAML add the following (add what you don’t have). The embed-resources component will make your final rendered html self-contained.\n\n---\ntitle: \"Document title\"\nauthor: \"my name\"\nformat:\n  html:\nembed-resources: true\n---"
  },
  {
    "objectID": "hw/01_hw_ch1.html#this-one-time",
    "href": "hw/01_hw_ch1.html#this-one-time",
    "title": "Homework 1",
    "section": "",
    "text": "Go to our RStudio Server at http://turing.cornellcollege.edu:8787/\nGo to your class folder you made named “STA363”\nCreate a new folder called FirstInitial_Last_Initial_HW with your initials."
  },
  {
    "objectID": "hw/01_hw_ch1.html#every-homework",
    "href": "hw/01_hw_ch1.html#every-homework",
    "title": "Homework 1",
    "section": "",
    "text": "Each of your assignments will begin with the following steps.\n\nFinding the instructions on our website: https://stats-tgeorge.github.io/STA363_AdvReg/\nGoing to our RStudio Server at http://turing.cornellcollege.edu:8787/\nCreating a new project. and giving it a sensible name such as homework_1 and having that project in the course folder you created.\nCreate a new quarto document and give it a sensible name such as hw1.\nIn the YAML add the following (add what you don’t have). The embed-resources component will make your final rendered html self-contained.\n\n---\ntitle: \"Document title\"\nauthor: \"my name\"\nformat:\n  html:\nembed-resources: true\n---"
  },
  {
    "objectID": "hw/03_hw_ch3.html",
    "href": "hw/03_hw_ch3.html",
    "title": "Homework 3",
    "section": "",
    "text": "Each of your assignments will begin with the following steps.\n\nGoing to our RStudio Server at http://turing.cornellcollege.edu:8787/\nCreating a new R project, inside your homework folder on the server, and giving it a sensible name such as homework_3 and having that project in the course folder you created.\nCreate a new quarto document and give it a sensible name such as hw3.\nIn the YAML add the following (add what you don’t have). The embed-resources component will make your final rendered html self-contained.\n\n---\ntitle: \"Document title\"\nauthor: \"my name\"\nformat:\n  html:\nembed-resources: true\n---"
  },
  {
    "objectID": "hw/03_hw_ch3.html#setup",
    "href": "hw/03_hw_ch3.html#setup",
    "title": "Homework 3",
    "section": "",
    "text": "Each of your assignments will begin with the following steps.\n\nGoing to our RStudio Server at http://turing.cornellcollege.edu:8787/\nCreating a new R project, inside your homework folder on the server, and giving it a sensible name such as homework_3 and having that project in the course folder you created.\nCreate a new quarto document and give it a sensible name such as hw3.\nIn the YAML add the following (add what you don’t have). The embed-resources component will make your final rendered html self-contained.\n\n---\ntitle: \"Document title\"\nauthor: \"my name\"\nformat:\n  html:\nembed-resources: true\n---"
  },
  {
    "objectID": "hw/03_hw_ch3.html#instructions",
    "href": "hw/03_hw_ch3.html#instructions",
    "title": "Homework 3",
    "section": "Instructions",
    "text": "Instructions\nBe sure to include the relevant R code as well as full sentences answering each of the questions (i.e. if I ask for the average, you can output the answer in R but also write a full sentence with the answer). Be sure to frequently save your files!\nData for the homework will be in the STA363_inst_files -&gt; data folder."
  },
  {
    "objectID": "hw/03_hw_ch3.html#exercises",
    "href": "hw/03_hw_ch3.html#exercises",
    "title": "Homework 3",
    "section": "Exercises",
    "text": "Exercises\nAll problems are from The main textbook is: Beyond Multiple Linear Regression by Paul Roback and Julie Legler – it is freely available online. Chapters 1-9.. Abbreviated BMLR.\nUse the numbering on the left. The codes are for instructor use (Ex: C1).\n\nExercise 1\n\n(C1) At what value of \\(p\\) is the standard deviation of a binary random variable smallest? When is standard deviation largest?\n\n\n\nExercise 2\n\n(C3) How are exponential and Poisson random variables related?\n\n\n\nExercise 3\n\n(C7) Chapter 1 also asked you to consider a scenario where “Researchers are attempting to see if socioeconomic status and parental stability are predictive of low birthweight. They classify a low birthweight as below 2500 g, hence our response is binary: 1 for low birthweight, and 0 when the birthweight is not low.” What distribution might be useful to model if a newborn has low birthweight?\n\n\n\nExercise 4\n\n(C9) Describe a scenario which could be modeled using a gamma distribution.\n\n\n\nExercise 5\n\n(G2) Gamma-Poisson mixture I. Use the R function rpois() to generate 10,000 \\(x_i\\) from a plain old vanilla Poisson random variable, \\(X \\sim \\textrm{Poisson}(\\lambda=1.5)\\). Plot a histogram of this distribution and note its mean and standard deviation. Next, let \\(Y \\sim \\textrm{Gamma}(r = 3, \\lambda = 2)\\) and use rgamma() to generate 10,000 random \\(y_i\\) from this distribution. Now, consider 10,000 different Poisson distributions where \\(\\lambda_i = y_i\\). Randomly generate one \\(z_i\\) from each Poisson distribution. Plot a histogram of these \\(z_i\\) and compare it to your original histogram of \\(X\\) (where \\(X \\sim \\textrm{Poisson}(1.5)\\)). How do the means and standard deviations compare?"
  },
  {
    "objectID": "hw/05_hw_ch4_2.html",
    "href": "hw/05_hw_ch4_2.html",
    "title": "Homework 5",
    "section": "",
    "text": "Each of your assignments will begin with the following steps.\n\nGoing to our RStudio Server at http://turing.cornellcollege.edu:8787/\nCreating a new R project, inside your homework folder on the server, and giving it a sensible name such as homework_4 and having that project in the course folder you created.\nCreate a new quarto document and give it a sensible name such as hw4.\nIn the YAML add the following (add what you don’t have). The embed-resources component will make your final rendered html self-contained.\n\n---\ntitle: \"Document title\"\nauthor: \"my name\"\nformat:\n  html:\nembed-resources: true\n---"
  },
  {
    "objectID": "hw/05_hw_ch4_2.html#setup",
    "href": "hw/05_hw_ch4_2.html#setup",
    "title": "Homework 5",
    "section": "",
    "text": "Each of your assignments will begin with the following steps.\n\nGoing to our RStudio Server at http://turing.cornellcollege.edu:8787/\nCreating a new R project, inside your homework folder on the server, and giving it a sensible name such as homework_4 and having that project in the course folder you created.\nCreate a new quarto document and give it a sensible name such as hw4.\nIn the YAML add the following (add what you don’t have). The embed-resources component will make your final rendered html self-contained.\n\n---\ntitle: \"Document title\"\nauthor: \"my name\"\nformat:\n  html:\nembed-resources: true\n---"
  },
  {
    "objectID": "hw/05_hw_ch4_2.html#instructions",
    "href": "hw/05_hw_ch4_2.html#instructions",
    "title": "Homework 5",
    "section": "Instructions",
    "text": "Instructions\nBe sure to include the relevant R code as well as full sentences answering each of the questions (i.e. if I ask for the average, you can output the answer in R but also write a full sentence with the answer). Be sure to frequently save your files!\nData for the homework will be in the STA363_inst_files -&gt; data folder."
  },
  {
    "objectID": "hw/05_hw_ch4_2.html#exercises",
    "href": "hw/05_hw_ch4_2.html#exercises",
    "title": "Homework 5",
    "section": "Exercises",
    "text": "Exercises\nAll problems are from The main textbook is: Beyond Multiple Linear Regression by Paul Roback and Julie Legler – it is freely available online. Chapters 1-9. Abbreviated BMLR.\nUse the numbering on the left. The codes are for instructor use (Ex: C1)."
  },
  {
    "objectID": "hw/05_hw_ch4_2.html#exercise-1",
    "href": "hw/05_hw_ch4_2.html#exercise-1",
    "title": "Homework 5",
    "section": "Exercise 1",
    "text": "Exercise 1\n\n(C15) Dating online. Researchers are interested in the number of dates respondents arranged online and whether the rates differ by age group. Questions which elicit responses similar to this can be found in the Pew Survey concerning dating online and relationships [@Duggan2013]. Each survey respondent was asked how many dates they have arranged online in the past 3 months as well as the typical amount of time, \\(t\\), in hours, they spend online weekly. Some rows of data appear in Table 1.\n\nIdentify the response, predictor, and offset in this context. Does using an offset make sense?\nWrite out a model for this data. As part of your model description, define the parameter, \\(\\lambda\\).\nConsider a zero-inflated Poisson model for this data. Describe what the `true zeros’ would be in this setting.\n\n\n\n\n\nTable 1: Sample data for Exercise 1.\n\n\nAge\nTime Online\nNumber of Dates Arranged Online\n\n\n\n\n19\n35\n3\n\n\n29\n20\n5\n\n\n38\n15\n0\n\n\n55\n10\n0"
  },
  {
    "objectID": "hw/05_hw_ch4_2.html#exercise-2",
    "href": "hw/05_hw_ch4_2.html#exercise-2",
    "title": "Homework 5",
    "section": "Exercise 2",
    "text": "Exercise 2\n\n(C16) Poisson approximation: rare events. For rare diseases, the probability of a case occurring, \\(p\\), in a very large population, \\(n\\), is small. With a small \\(p\\) and large \\(n\\), the random variable \\(Y\\)= the number of cases out of \\(n\\) people can be approximated using a Poisson random variable with \\(\\lambda = np\\). If the count of those with the disease is observed in several different populations independently of one another, the \\(Y_i\\) represents the number of cases in the \\(i^{th}\\) population and can be approximated using a Poisson random variable with \\(\\lambda_i=n_ip_i\\) where \\(p_i\\) is the probability of a case for the \\(i^{th}\\) population. Poisson regression can take into account the differences in the population sizes, \\(n_i\\), using as an offset log(\\(n_i\\)) as well as differences in a population characteristic like \\(x_i\\). The coefficient of the offset is set at one; it is not estimated like the other coefficients. Thus the model statement has the form: \\(log(\\lambda_i) = \\beta_0+\\beta_1x_i + log(n_i)\\), where \\(Y_i  \\sim\\) Poisson(\\(\\lambda_i = n_i p_i\\)). Note that \\(\\lambda_i\\) depends on \\(x_i\\) which may differ for the different populations.\n@Scotto1974 wondered if skin cancer rates by age group differ by city. Based on their data in Table 2, identify and describe the following quantities which appear in the description of the Poisson approximation for rare events:\n\nA case,\nThe population size, \\(n_i\\),\nProbability, \\(p_i\\),\nPoisson parameter, \\(\\lambda_i\\),\nPoisson random variables, \\(Y_i\\), and\nThe predictors, \\(X_i\\).\n\n\n\n\n\nTable 2: Data from Scotto et al. (1974) on the number of cases of non-melanoma skin cancer for women by age group in two metropolitan areas (Minneapolis-St. Paul and Dallas-Ft. Worth); the year is unknown.\n\n\nNumber of Cases\nPopulation\nAge Group\nCity\n\n\n\n\n1\n172675\n15-24\n1\n\n\n16\n123065\n25-34\n1\n\n\n...\n...\n...\n...\n\n\n226\n29007\n75-84\n2\n\n\n65\n7538\n85+\n2\n\n\n\n The columns contain: number of cases, population size, age group, and city (1=Minneapolis-St. Paul, 2=Dallas-Ft. Worth)."
  },
  {
    "objectID": "hw/05_hw_ch4_2.html#exercise-3",
    "href": "hw/05_hw_ch4_2.html#exercise-3",
    "title": "Homework 5",
    "section": "Exercise 3",
    "text": "Exercise 3\n\n(G6) U.S. National Medical Expenditure Survey. The data set NMES1988 in the AER package contains a sample of individuals over 65 who are covered by Medicare in order to assess the demand for health care through physician office visits, outpatient visits, ER visits, hospital stays, etc. The data can be accessed by installing and loading the AER package and then running data(NMES1988). More background information and references about the NMES1988 data can be found in help pages for the AER package.\n\n\nShow through graphical means that there are more respondents with 0 visits than might be expected under a Poisson model.\nFit a ZIP model for the number of physician office visits using chronic, health, and insurance as predictors for the Poisson count, and chronic and insurance as the predictors for the binary part of the model. Then, provide interpretations in context for the following model parameters:\n\n\nchronic in the Poisson part of the model\npoor health in the Poisson part of the model\nthe Intercept in the logistic part of the model\ninsurance in the logistic part of the model\n\n\nIs there significant evidence that the ZIP model is an improvement over a simple Poisson regression model?"
  },
  {
    "objectID": "hw/07_hw_ch7_ch8.html",
    "href": "hw/07_hw_ch7_ch8.html",
    "title": "Homework 7",
    "section": "",
    "text": "Each of your assignments will begin with the following steps.\n\nGoing to our RStudio Server at http://turing.cornellcollege.edu:8787/\nCreating a new R project, inside your homework folder on the server, and giving it a sensible name such as homework_6 and having that project in the course folder you created.\nCreate a new quarto document and give it a sensible name such as hw6.\nIn the YAML add the following (add what you don’t have). The embed-resources component will make your final rendered html self-contained.\n\n---\ntitle: \"Document title\"\nauthor: \"my name\"\nformat:\n  html:\nembed-resources: true\n---"
  },
  {
    "objectID": "hw/07_hw_ch7_ch8.html#setup",
    "href": "hw/07_hw_ch7_ch8.html#setup",
    "title": "Homework 7",
    "section": "",
    "text": "Each of your assignments will begin with the following steps.\n\nGoing to our RStudio Server at http://turing.cornellcollege.edu:8787/\nCreating a new R project, inside your homework folder on the server, and giving it a sensible name such as homework_6 and having that project in the course folder you created.\nCreate a new quarto document and give it a sensible name such as hw6.\nIn the YAML add the following (add what you don’t have). The embed-resources component will make your final rendered html self-contained.\n\n---\ntitle: \"Document title\"\nauthor: \"my name\"\nformat:\n  html:\nembed-resources: true\n---"
  },
  {
    "objectID": "hw/07_hw_ch7_ch8.html#instructions",
    "href": "hw/07_hw_ch7_ch8.html#instructions",
    "title": "Homework 7",
    "section": "Instructions",
    "text": "Instructions\nBe sure to include the relevant R code as well as full sentences answering each of the questions (i.e. if I ask for the average, you can output the answer in R but also write a full sentence with the answer). Be sure to frequently save your files!\nData for the homework will be in the STA363_inst_files -&gt; data folder."
  },
  {
    "objectID": "hw/07_hw_ch7_ch8.html#exercises",
    "href": "hw/07_hw_ch7_ch8.html#exercises",
    "title": "Homework 7",
    "section": "Exercises",
    "text": "Exercises\nAll problems are from The main textbook is: Beyond Multiple Linear Regression by Paul Roback and Julie Legler – it is freely available online. Chapters 1-9. Abbreviated BMLR.\nUse the numbering on the left. The codes are for instructor use (Ex: C1)."
  },
  {
    "objectID": "hw/07_hw_ch7_ch8.html#part-1-chapter-7---correlated-data",
    "href": "hw/07_hw_ch7_ch8.html#part-1-chapter-7---correlated-data",
    "title": "Homework 7",
    "section": "Part 1: Chapter 7 - Correlated Data",
    "text": "Part 1: Chapter 7 - Correlated Data\n\nExercise 1\n\n(C1) Examples with correlated data. For each of the following studies:\n\nIdentify the most basic observational units\nIdentify the grouping units (could be multiple levels of grouping)\nState the response(s) measured and variable type (normal, binary, Poisson, etc.)\nWrite a sentence describing the within-group correlation.\nIdentify fixed and random effects\n\n\n\nEpilepsy study. Researchers conducted a randomized controlled study where patients were randomly assigned to either an anti-epileptic drug or a placebo. For each patient, the number of seizures at baseline was measured over a 2-week period. For four consecutive visits the number of seizures were determined over the past 2-week period. Patient age and sex along with visit number were recorded.\nPrairie restoration. Researchers at a small Midwestern college decided to experimentally explore the underlying causes of variation in soil reconstruction projects in order to make future projects more effective. Introductory ecology classes were organized to collect weekly data on plants in pots containing soil samples. Data will be examined to compare:\n\ngermination and growth of two species of prairie plants—–leadplants (Amorpha canescens) and coneflowers (Ratibida pinnata).\nsoil from a cultivated (agricultural) field, a natural prairie, and a restored (reconstructed) prairie.\nthe effect of sterilization, since half of the sampled soil was sterilized to determine if rhizosphere differences were responsible for the observed variation.\n\nRadon in Minnesota. Radon is a carcinogen – a naturally occurring radioactive gas whose decay products are also radioactive – known to cause lung cancer in high concentrations. The EPA sampled more than 80,000 homes across the U.S. Each house came from a randomly selected county and measurements were made on each level of each home. Uranium measurements at the county level were included to improve the radon estimates."
  },
  {
    "objectID": "hw/07_hw_ch7_ch8.html#part-2-chapter-8---multilevel-models",
    "href": "hw/07_hw_ch7_ch8.html#part-2-chapter-8---multilevel-models",
    "title": "Homework 7",
    "section": "Part 2: Chapter 8 - Multilevel Models",
    "text": "Part 2: Chapter 8 - Multilevel Models\n\nExercise 2\n\n(C1,2,3)@Brown2004 describe “A Hierarchical Linear Model Approach for Assessing the Effects of House and Neighborhood Characteristics on Housing Prices”.\n\n\nBased on the title of their paper: (a) give the observational units at Level One and Level Two, and (b) list potential explanatory variables at both Level One and Level Two.\nIn the preceding problem, why can’t we assume all houses in the data set are independent? What would be the potential implications to our analysis of assuming independence among houses?\nIn the preceding problem, for each of the following sets of predictors:\n\n\nwrite out the two-level model for predicting housing prices,\nwrite out the corresponding composite model, and\ndetermine how many model parameters (fixed effects and variance components) must be estimated.\n\n\nPredictor set 1: Square footage, number of bedrooms\nPredictor set 2: Median neighborhood income, rating of neighborhood schools\nPredictor set 3: Square footage, number of bedrooms, age of house, median neighborhood housing price\nPredictor set 4: Square footage, median neighborhood income, rating of neighborhood schools, median neighborhood housing price\n\n\n\nExercise 3\n\n(C6) Why is the contour plot for multivariate normal density in Figure @ref(fig:contour-boundary)(b) tilted from southwest to northeast, but the contour plot in Figure @ref(fig:contour-boundary)(a) is not tilted?\n\n\n\nExercise 4\n\n(C8) Why is Model A (Section @ref(modela8) in 8.6.2) sometimes called the “unconditional means model”? Why is it also sometimes called the “random intercepts model”? Are these two labels consistent with each other?\n\n\n\nExercise 5\n\n(C9) Consider adding an indicator variable in Model B (Section @ref(randomslopeandint)) for Small Ensemble performances.\n\nWrite out the two-level model for performance anxiety,\nWrite out the corresponding composite model,\nDetermine how many model parameters (fixed effects and variance components) must be estimated, and\nExplain how the interpretation for the coefficient in front of Large Ensembles would change.\n\n\n\n\nExercise 6\n\n(C10) Give a short rule in your own words describing when an interpretation of an estimated coefficient should “hold constant” another covariate or “set to 0” that covariate (see Section @ref(interp:modeld)).\n\n\n\nExercise 7\n\n(C14) Interpret other estimated parameters from Model F beyond those interpreted in Section @ref(modelf): \\(\\hat{\\alpha}_{0}\\), \\(\\hat{\\alpha}_{2}\\), \\(\\hat{\\alpha}_{3}\\), \\(\\hat{\\beta}_{0}\\), \\(\\hat{\\gamma}_{0}\\), \\(\\hat{\\zeta}_{0}\\), \\(\\hat{\\rho}_{wx}\\), \\(\\hat{\\sigma}^{2}\\), \\(\\hat{\\sigma}_{u}^{2}\\), and \\(\\hat{\\sigma}_{z}^{2}\\).\n\n\n\nExercise 8\n\n(O1) @Chapp2018 explored 2014 congressional candidates’ ambiguity on political issues in their paper, Going Vague: Ambiguity and Avoidance in Online Political Messaging. They hand-coded a random sample of 2012 congressional candidates’ websites, assigning an ambiguity score. A total of 870 websites from 2014 were then automatically scored using Wordscores, a program designed for political textual analysis. In their paper, they fit a multilevel model for candidates’ ambiguities with predictors at both the candidate and district levels. Some of their hypotheses include that:\n\n“when incumbents do hazard issue statements, these statements will be marked by a higher degree of clarity.” (Hypothesis 1b)\n“ideological distance [from district residents] will be associated with greater ambiguity.” (Hypothesis 2a)\n“controlling for ideological distance, ideological extremity [of the candidate] should correspond to less ambiguity.” (Hypothesis 2b)\n“more variance in attitudes [among district residents] will correspond to a higher degree of ambiguity in rhetoric” (Hypothesis 3a)\n“a more heterogeneous mix of subgroups [among district residents] will also correspond to a higher degree of ambiguity in rhetoric” (Hypothesis 3b)\n\nTheir data can be found in ambiguity.csv. Variables of interest include:\n\nambiguity = assigned ambiguity score. Higher scores indicate greater clarity (less ambiguity)\ndemocrat = 1 if a Democrat, 0 otherwise (Republican)\nincumbent = 1 if an incumbent, 0 otherwise\nideology = a measure of the candidate’s left-right orientation. Higher (positive) scores indicate more conservative candidates and lower (negative) scores indicate more liberal candidates.\nmismatch = the distance between the candidate’s ideology and the district’s ideology (candidate ideology scores were regressed against district ideology scores; mismatch values represent the absolute value of the residual associated with each candidate)\ndistID = the congressional district’s unique ID\ndistLean = the district’s political leaning. Higher scores imply more conservative districts.\nattHeterogeneity = a measure of the variability of ideologies within the district. Higher scores imply more attitudinal heterogeneity among voters.\ndemHeterogeneity = a measure of the demographic variability within the district. Higher scores imply more demographic heterogeneity among voters.\n\n\nWith this in mind, fit your own models to address these hypotheses from @Chapp2018. Be sure to use a two-level structure to account for variables at both the candidate and district levels.\nHints: (1) Make sure to conduct an EDA. (2) Build appropriate model(s) that allow you to test these hypothesis using the significance of the predictors."
  },
  {
    "objectID": "hw/09_hw_ch7ISLR.html",
    "href": "hw/09_hw_ch7ISLR.html",
    "title": "Homework 9",
    "section": "",
    "text": "Each of your assignments will begin with the following steps.\n\nGoing to our RStudio Server at http://turing.cornellcollege.edu:8787/\nCreating a new R project, inside your homework folder on the server, and giving it a sensible name such as homework_9 and having that project in the course folder you created.\nCreate a new quarto document and give it a sensible name such as hw9.\nIn the YAML add the following (add what you don’t have). The embed-resources component will make your final rendered html self-contained.\n\n\n---\ntitle: \"Document title\"\nauthor: \"my name\"\nformat:\n  html:\n    embed-resources: true\n---"
  },
  {
    "objectID": "hw/09_hw_ch7ISLR.html#setup",
    "href": "hw/09_hw_ch7ISLR.html#setup",
    "title": "Homework 9",
    "section": "",
    "text": "Each of your assignments will begin with the following steps.\n\nGoing to our RStudio Server at http://turing.cornellcollege.edu:8787/\nCreating a new R project, inside your homework folder on the server, and giving it a sensible name such as homework_9 and having that project in the course folder you created.\nCreate a new quarto document and give it a sensible name such as hw9.\nIn the YAML add the following (add what you don’t have). The embed-resources component will make your final rendered html self-contained.\n\n\n---\ntitle: \"Document title\"\nauthor: \"my name\"\nformat:\n  html:\n    embed-resources: true\n---"
  },
  {
    "objectID": "hw/09_hw_ch7ISLR.html#instructions",
    "href": "hw/09_hw_ch7ISLR.html#instructions",
    "title": "Homework 9",
    "section": "Instructions",
    "text": "Instructions\nBe sure to include the relevant R code as well as full sentences answering each of the questions (i.e. if I ask for the average, you can output the answer in R but also write a full sentence with the answer). Be sure to frequently save your files!\nIn this homework we will work with four packages: tidyverse which is a collection of packages for doing data analysis in a “tidy” way, tidymodels for statistical model coefficients, splines for our splines models and ISLR2 for the Boston data."
  },
  {
    "objectID": "hw/09_hw_ch7ISLR.html#code",
    "href": "hw/09_hw_ch7ISLR.html#code",
    "title": "Homework 9",
    "section": "Code",
    "text": "Code\n\nPolynomial Regression\nTo fit a degree 4 polynomial regression model you use:\n\nmodel1 &lt;- lm(wage ~ poly(age , 4, raw = TRUE), data = wage)\n\n\n\nCubic Spline\nTo fit a cubic spline (degree 3) polynomial regression model you use:\n\nmodel2&lt;- lm(wage ~ bs(age , knots = c(22, 33, 50), degree = 3), data = Wage)\n\nUse quantiles of the predictor to choose your knots. The choice should be motivated by some EDA.\n\n\nNatual Cubic Spline\nTo fit a degree natural cubic spline.\n\nmodel3 &lt;- lm(wage ~ ns(age,df = 4), data = Wage)\n\nInstead of providing the degrees of freedom (df), you can provide knots."
  },
  {
    "objectID": "hw/09_hw_ch7ISLR.html#exercises",
    "href": "hw/09_hw_ch7ISLR.html#exercises",
    "title": "Homework 9",
    "section": "Exercises",
    "text": "Exercises\nAll problems are from The secondary text is: An Introduction to Statistical Learning with Applications in R by Gareth James, Daniela Witten, Trevor Hastie, and Robert Tibshirani – it is freely available online. Chapter 7. Abbreviated ISLR.\n\nlibrary(ISLR2)\n\nWarning: package 'ISLR2' was built under R version 4.4.1\n\n\n\nSuppose we fit a curve with basis functions \\(b_1(X) = X,\\space b_2(X) = (X − 1)^2I(X \\geq 1)\\). (Note that \\(I(X \\geq 1)\\) equals 1 for \\(X \\geq 1\\) and 0 otherwise.) We fit the linear regression model\n\n\\[Y = \\beta_0 + \\beta_1b_1(X) + \\beta_2b_2(X) + \\epsilon,\\] \nand obtain coefficient estimates \\(\\hat{\\beta}_0 = 1,\\space \\hat{\\beta}_1 = 1,\\space \\hat{\\beta}_2 = −2\\). Sketch the estimated curve between \\(X = −2\\) and \\(X = 2\\). Note the intercepts, slopes, and other relevant information.\n\n(ISLR,7,9) This question uses the variables dis (the weighted mean of distances to five Boston employment centers) and nox (nitrogen oxides concentration in parts per 10 million) from the Boston data (in package ISLR2). We will treat dis as the predictor and nox as the response.\n\n\nUse the poly() function to fit a cubic polynomial regression to predict nox using dis. Report the regression output, and plot the resulting data and polynomial fits.\nPlot the polynomial fits for a range of different polynomial degrees (say, from 1 to 5), and report the associated residual sum of squares.\nSelect the optimal degree for the polynomial, and explain your results.\nUse the bs() function to fit a regression spline to predict nox using dis. Report the output for the fit using four degrees of freedom. How did you choose the knots? Plot the resulting fit.\nNow fit a regression spline for a range of degrees of freedom, and plot the resulting fits and report the resulting RSS. Describe the results obtained.\nNow fit a natural regression spline for a range of degrees of freedom, and plot the resulting fits and report the resulting RSS. Describe the results obtained.\nPick an overall best model using various metrics, complexity, and regression assumptions to make your choice. They are the same metrics used in multiple linear regression."
  },
  {
    "objectID": "labs/01_linear_regression/01_banksalary.html",
    "href": "labs/01_linear_regression/01_banksalary.html",
    "title": "Stat 363: Multiple Linear Regression Review",
    "section": "",
    "text": "Since this our first lab, I suggest you get setup for files in the following way:\n\nLog into the course RStudio Server. http://turing.cornellcollege.edu:8787/ Make sure to close out of any projects on the top right corner.\nClick the “File” tab in the bottom right corner. Then click “Home” across the top of the panel.\nIf ones does not exists already, use the “+📂” button to create a new folder and call it “STA363”\nCopy project folder called “01_linear_regression” located at Home -&gt; STA363_inst_files-&gt; labs. If you check the box next to the folder name, then click the small gear icon you can “copy to” and put a copy of the folder in your newly created folder.\nNow, click File-&gt; Open Project and navigate to the project file in the folder you just copied.\nYou can place your responses in the file 01_banksalary_fillable.qmd."
  },
  {
    "objectID": "labs/01_linear_regression/01_banksalary.html#first-lab-setup",
    "href": "labs/01_linear_regression/01_banksalary.html#first-lab-setup",
    "title": "Stat 363: Multiple Linear Regression Review",
    "section": "",
    "text": "Since this our first lab, I suggest you get setup for files in the following way:\n\nLog into the course RStudio Server. http://turing.cornellcollege.edu:8787/ Make sure to close out of any projects on the top right corner.\nClick the “File” tab in the bottom right corner. Then click “Home” across the top of the panel.\nIf ones does not exists already, use the “+📂” button to create a new folder and call it “STA363”\nCopy project folder called “01_linear_regression” located at Home -&gt; STA363_inst_files-&gt; labs. If you check the box next to the folder name, then click the small gear icon you can “copy to” and put a copy of the folder in your newly created folder.\nNow, click File-&gt; Open Project and navigate to the project file in the folder you just copied.\nYou can place your responses in the file 01_banksalary_fillable.qmd."
  },
  {
    "objectID": "labs/01_linear_regression/01_banksalary.html#introduction",
    "href": "labs/01_linear_regression/01_banksalary.html#introduction",
    "title": "Stat 363: Multiple Linear Regression Review",
    "section": "Introduction",
    "text": "Introduction\nExample: Sex discrimination in bank salaries. In the 1970’s, Harris Trust was sued for sex discrimination in the salaries it paid its employees. One approach to addressing this issue was to examine the starting salaries of all skilled, entry-level clerical workers between 1965 and 1975. The data is saved under banksalary.csv, and relevant R code can be found in the STA363_inst_files folder on the home directory of the RStudio Server. banksalary.qmd.\nData from skilled entry-level clerical workers at bank being sued for sex discrimination in 1970s (from Statistical Sleuth, Ch 12):\n\nbsal = beginning salary (annual salary at time of hire)\nsal77 = annual salary in 1977\nsex = MALE or FEMALE\nsenior = months since hired\nage = age in months\neduc = years of education\nexper = months of prior work experience\n\nFirst, we will speculate on what we expect to find and then we will perform an analysis using the data.\n\nRead the data in and use aview() to see the data, but do not do anything else with the data on the computer until question 6."
  },
  {
    "objectID": "labs/01_linear_regression/01_banksalary.html#questions",
    "href": "labs/01_linear_regression/01_banksalary.html#questions",
    "title": "Stat 363: Multiple Linear Regression Review",
    "section": "Questions",
    "text": "Questions\n\nIdentify the observational units, the response variable, and explanatory variables.\nGiven the mean starting salary of male workers ($5957) was 16% higher than the mean starting salary of female workers ($5139): Is this enough evidence to conclude sex discrimination exists? If not, what further evidence would you need?\nHow would you expect age, experience, and education to each be related to starting salary?\nWhy might it be important to control for seniority (number of years with the bank) if we are only concerned with the salary when the worker started?\nDo you expect any explanatory variables (including sex) to be closely related to each other? What implications would this have for modeling?\n\nUsing the data…\nOne approach is to construct a good model for beginning salaries while requiring sex as a predictor, to determine the significance of sex after controlling for the other covariates. Then we can explore interactions with sex to see if its effect is consistent across levels of other predictors.\n\nUse the data to address the primary question of interest here using only the beginning salary and sex variables. Be sure to discuss plots and summary statistics first, and then look at test(s) of significance.\nConstruct plots to investigate how each of the potential confounders (age, experience, education) is related to beginning salaries. Describe your findings.\nDoes seniority play a role in the variation of starting salaries? In what way?\nExamine how the explanatory variables (including sex) are related to each other, if at all. What implications would this have for modeling?\nFit a simple linear regression model with starting salary as the response and education as the sole explanatory variable. Interpret the intercept and slope of this model; also interpret the R-squared value. Is there a significant relationship between education and starting salary?\n\nIntercept:\nSlope:\nR2\nSignificance:\n\nDoes model1 from question 10 meet all linear regression assumptions? List each assumption and how you decided if it was met or not.\nIs a model with all 4 confounding variables better than a model with just education? Justify with an appropriate significance test in addition to summary statistics of model performance.\nYou should have noticed that the term for age was not significant in the model3. What does this imply about age and about future modeling steps?\nThe relationship between experience and beginning salary exhibits some curvature. How might it be interpreted in this context? Determine whether a quadratic term in experience improves a model without curvature.\nBased on model6, what conclusions can be drawn about sex discrimination at Harris Trust? Do these conclusions have to be qualified at all, or are they pretty clear cut? Interpret a 95% confidence interval for the male indicator variable in context to help with your response.\nDo any explanatory variables exhibit an interaction with sex. If so, what are the implications for your answer in (15)?\nOften salary data is logged before analysis. Would you recommend logging starting salary in this study? Support your decision analytically.\nRegardless of your answer to (17), provide an interpretation for the coefficient for the male coefficient in model6a after logging starting salary."
  },
  {
    "objectID": "labs/01_linear_regression/01_banksalary.html#code",
    "href": "labs/01_linear_regression/01_banksalary.html#code",
    "title": "Stat 363: Multiple Linear Regression Review",
    "section": "Code",
    "text": "Code\n\nlibrary(mosaic)\nlibrary(skimr)   # may have to install\nlibrary(dplyr)  \nlibrary(ggplot2)\nlibrary(gridExtra)\nbank &lt;- read.csv(\"data/banksalary.csv\")\n\nData from skilled entry-level clerical workers at bank being sued for sex discrimination in 1970s (from Statistical Sleuth, Ch 12):\n\nbsal = beginning salary (annual salary at time of hire)\nsal77 = annual salary in 1977\nsex = MALE or FEMALE\nsenior = months since hired\nage = age in months\neduc = years of education\nexper = months of prior work experience\n\n\n# Examine data frame\nhead(bank)       # print first 6 rows\n\n# Generate relevant summary statistics for response variable\nfavstats(~ bsal, data = bank)\n\n# Look at marginal relationship between sex and beginning salary\n\n# Three options for getting summary statistics\nfavstats(~ bsal | sex, data = bank)\n\nbank |&gt;\n  group_by(sex) |&gt;\n  summarize(mean = mean(bsal),\n            median = median(bsal),\n            sd = sd(bsal),\n            iqr = IQR(bsal),\n            n = n())\n\n# Doesn't seem to render as a pdf\n#bank |&gt;\n#  group_by(sex) |&gt;\n#  skim()\n\n# Several options for plotting 1 categorical and 1 numeric variable\nboxplot(bsal ~ sex, ylab=\"Beginning salary\", data = bank)\nbwplot(sex ~ bsal, data = bank)\nggplot(bank, aes(x = sex, y = bsal)) +\n  geom_boxplot() + coord_flip()\nggplot(data = bank, mapping = aes(x = bsal, y = ..density..)) + \n  geom_freqpoly(mapping = aes(colour = sex), binwidth = 250)\nggplot(data = bank, mapping = aes(x = bsal, y = ..density..)) + \n  geom_density(mapping = aes(colour = sex))\nggplot(bank, aes(x=bsal, fill=sex)) +\n  geom_density(alpha=0.4)\nggplot(bank, aes(x = sex, y = bsal)) +\n  geom_violin() \nggplot(data = bank) + \n  geom_histogram(mapping = aes(x = bsal)) + \n  facet_wrap(~ sex, nrow = 2)\nggplot(data = bank) + \n  geom_histogram(mapping = aes(x = bsal, y = ..density..)) + \n  facet_wrap(~ sex, nrow = 2)\n\n# Initial analysis of sex vs salary, ignoring other covariates\nt.test(bsal ~ sex, data = bank)\n\nmodel1 = lm(bsal ~ sex, data = bank)\nsummary(model1)\n\nbank &lt;- bank |&gt;\n  mutate(male = ifelse(sex==\"MALE\",1,0))   # create our own indicator for sex\nt.test(bsal ~ male, var.equal = TRUE, data = bank)\n\n# How are covariates related to response?\nggplot(bank, aes(y = bsal, age)) + \n  geom_point() + \n  geom_smooth(method = lm)\nggplot(bank, aes(y = bsal, exper)) + \n  geom_point() + \n  geom_smooth(method = lm)\nggplot(bank, aes(y = bsal, educ)) + \n  geom_point() + \n  geom_smooth(method = lm)\nggplot(bank, aes(y = bsal, senior)) + \n  geom_point() + \n  geom_smooth(method = lm)\n\n# How are explanatory variables related to each other?\nbank0 &lt;- bank |&gt; select(bsal, age, exper, educ, senior, male)\npairs(bank0)     # matrix of scatterplots \ncor(bank0)       # matrix of correlations\n\n# How are explanatory variables related to sex?\nfavstats(~ age | sex, data = bank)\nggplot(bank, aes(x = sex, y = age)) +\n  geom_boxplot() + coord_flip()\n\nfavstats(~ exper | sex, data = bank)\nggplot(bank, aes(x = sex, y = exper)) +\n  geom_boxplot() + coord_flip()\n\nfavstats(~ educ | sex, data = bank)\nggplot(bank, aes(x = sex, y = educ)) +\n  geom_boxplot() + coord_flip()\n\nfavstats(~ senior | sex, data = bank)\nggplot(bank, aes(x = sex, y = senior)) +\n  geom_boxplot() + coord_flip()\n\n# Fit linear regression with single predictor (education)\nmodel2 = lm(bsal ~ educ, data = bank)\nsummary(model2)\n\n\n# Residual plots - run off the page unless redo margins\npar(mfrow=c(2,2), mar=c(4,4,4,1), oma=c(0.5,0.5,0.5,0))\nplot(model2)   # Generate residual diagnostic plots\npar(mfrow=c(1,1))\n\n\n# Fit multiple regression model with four predictors\nmodel3 = lm(bsal ~ senior + age + educ + exper, data = bank)\nsummary(model3)\nanova(model2, model3, test=\"F\")\n\n# Examine AIC and BIC scores (lower is better)\nAIC(model2)                     # AIC-model2\nAIC(model3)                     # AIC-model3\nAIC(model2, k=log(nrow(bank)))  # BIC-model2\nAIC(model3, k=log(nrow(bank)))  # BIC-model3\n\n# Fit linear regression with experience\nmodel4 = lm(bsal ~ exper, data = bank)\nsummary(model4)\n\n\n# Residual plots - run off the page unless redo margins\npar(mfrow=c(2,2), mar=c(4,4,4,1), oma=c(0.5,0.5,0.5,0))\nplot(model4)   # Generate residual diagnostic plots\npar(mfrow=c(1,1))\n\n\nbank &lt;- bank |&gt;\n  mutate(exper2 = exper^2)\nmodel5 = lm(bsal ~ exper + exper2, data = bank)\nsummary(model5)\n\n\n# Residual plots - run off the page unless redo margins\npar(mfrow=c(2,2), mar=c(4,4,4,1), oma=c(0.5,0.5,0.5,0))\nplot(model5)   # Generate residual diagnostic plots\npar(mfrow=c(1,1))\n\n\n# One potential final model\nmodel6 = lm(bsal ~ senior + educ + exper + male, data = bank)\nsummary(model6)\n\n# Construct 95% CIs for model coefficients the hard way...\nbetas = summary(model6)$coef[,1]   # store model betas\nSEs = summary(model6)$coef[,2]    # store SEs of betas\ntstar = qt(.975,model6$df)\nlb = betas - tstar*SEs\nub = betas + tstar*SEs\ncbind(lb,ub)\n\n# ... or more simply:\nconfint(model6)\n\n# Investigate interactions with sex\nggplot(bank, aes(y = bsal, x = age, color = sex)) + \n  geom_point() + \n  geom_smooth(method = lm)\nggplot(bank, aes(y = bsal, x = exper, color = sex)) + \n  geom_point() + \n  geom_smooth(method = lm)\nggplot(bank, aes(y = bsal, x = educ, color = sex)) + \n  geom_point() + \n  geom_smooth(method = lm)\nggplot(bank, aes(y = bsal, x = senior, color = sex)) + \n  geom_point() + \n  geom_smooth(method = lm)\n\n# Examine interaction between sex and education\nmodel7 = lm(bsal ~ educ + male + educ:male, data = bank)\nsummary(model7)\n\n\n# Should beginning salary be log transformed?\nbank &lt;- bank |&gt; \n  mutate(logbsal = log(bsal))\nhist1 &lt;- ggplot(bank, aes(bsal)) + geom_histogram(bins = 10)\nhist2 &lt;- ggplot(bank, aes(logbsal)) + geom_histogram(bins = 10)\ngrid.arrange(hist1, hist2, ncol=2)\n\n# Look at marginal relationship between sex and beginning salary\nmodel6a = lm(logbsal ~ senior + educ + exper + male, data = bank)\nsummary(model6a)\n\n\n# Residual plots - run off the page unless redo margins\npar(mfrow=c(2,2), mar=c(4,4,4,1), oma=c(0.5,0.5,0.5,0))\nplot(model6a)   # Generate residual diagnostic plots\npar(mfrow=c(1,1))\n\n\n# Bonus code: to help with HW1 when looking at 2 categorical variables\nytable &lt;- tally(~ sex + educ, data = bank)\nytable\nmosaicplot(ytable, color = c(\"blue\", \"light blue\"))\nprop.table(ytable, 1)\n\n# Other ways to visualize two categorical variables\nbank &lt;- bank |&gt;\n  mutate(education = ifelse(educ &lt; 12, \"No high school\", \"High school\"),\n         education = ifelse(educ &gt; 12, \"Some college\", education))\nggplot(data = bank) + \n  geom_bar(mapping = aes(x = sex, fill = education)) \nggplot(data = bank) + \n  geom_bar(mapping = aes(x = sex, fill = education), position = \"fill\") \nlibrary(ggmosaic)  # need to have ggmosaic package installed\nggplot(data = bank) +\n   geom_mosaic(aes(x = product(education, sex), fill=education), na.rm = TRUE)\n\n# Other ways to summarize relationship between two categorical variables\nbank |&gt;\n  group_by(sex, education) |&gt;\n  summarise (n = n()) |&gt;\n  mutate(freq = n / sum(n))"
  },
  {
    "objectID": "labs/02_likelihood/02_likelihood.html",
    "href": "labs/02_likelihood/02_likelihood.html",
    "title": "Chapter 2 - Likelihoods",
    "section": "",
    "text": "Copy the project lab folder located at Home -&gt; STA363_inst_files-&gt; labs. If you check the box next to the folder name, then click the small gear icon you can “copy to” and put a copy of the folder in your newly created folder.\nNow, click File-&gt; Open Project and navigate to the project file in the folder you just copied.\nYou can place your responses in the file qmd file included."
  },
  {
    "objectID": "labs/02_likelihood/02_likelihood.html#lab-setup",
    "href": "labs/02_likelihood/02_likelihood.html#lab-setup",
    "title": "Chapter 2 - Likelihoods",
    "section": "",
    "text": "Copy the project lab folder located at Home -&gt; STA363_inst_files-&gt; labs. If you check the box next to the folder name, then click the small gear icon you can “copy to” and put a copy of the folder in your newly created folder.\nNow, click File-&gt; Open Project and navigate to the project file in the folder you just copied.\nYou can place your responses in the file qmd file included."
  },
  {
    "objectID": "labs/02_likelihood/02_likelihood.html#introduction",
    "href": "labs/02_likelihood/02_likelihood.html#introduction",
    "title": "Chapter 2 - Likelihoods",
    "section": "Introduction",
    "text": "Introduction\nDoes having boys or girls run in the family? Using demographic data from the National Longitudinal Survey of Youth, can we identify biases in sex composition patterns of children? The data is found in Table 2 in the Rodgers and Doughty (2001) article, and relevant R code can be found under below."
  },
  {
    "objectID": "labs/02_likelihood/02_likelihood.html#questions",
    "href": "labs/02_likelihood/02_likelihood.html#questions",
    "title": "Chapter 2 - Likelihoods",
    "section": "Questions",
    "text": "Questions\n\nModel 1 – Sex Unconditional Model. Each child is independent of the others, and there is a constant probability ( ) that a child is a boy.\n\nConsider a small example with 3 families with compositions of children given by BBG, GBG, and GG.\nFind the maximum likelihood estimator (MLE) for by:\n\n\nConducting a numerical search in R for the largest likelihood over a fine grid of values 0-1.\nConducting a numerical search in R for the largest log-likelihood between 0 and 1. Illustrate the process graphically, and report the maximum value of the likelihood and log-likelihood functions. Does it make sense that both methods would agree (and agree with the mathematical approach)?\n\n\nApply Model 1 to the NLSY data (families in Table 2.4 below who have 3 or fewer children). Find the MLE for by adapting the R code for (1). There are 5,626 families in the table.\n\n\n#Family comp data table\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(kableExtra)\n\n\nAttaching package: 'kableExtra'\n\nThe following object is masked from 'package:dplyr':\n\n    group_rows\n\ntable5chp2 &lt;- data.frame(Famcomp, Numfams, Numchild)\n\ntable5chp2 |&gt; kbl(col.names = c(\"Family Composition\",\n    \"Number of families\", \"Number of children\"),\n      caption = \"Table 2.4 Number of families and children in families with given composition in NLSY data.\") \n\n\nTable 2.4 Number of families and children in families with given composition in NLSY data.\n\n\nFamily Composition\nNumber of families\nNumber of children\n\n\n\n\nB\n930\n930\n\n\nG\n951\n951\n\n\nBB\n582\n1164\n\n\nBG\n666\n1332\n\n\nGB\n666\n1332\n\n\nGG\n530\n1060\n\n\nBBB\n186\n558\n\n\nBBG\n177\n531\n\n\nBGG\n173\n519\n\n\nBGB\n148\n444\n\n\nGBB\n151\n453\n\n\nGGB\n125\n375\n\n\nGBG\n182\n546\n\n\nGGG\n159\n477\n\n\n\n\n\n\n\n\n\nModel 2 – Sex Conditional Model. The probability of having a boy depends on whether you’ve had boys previously, so Model 2 will have three parameters:\n= probability of a boy when previously have had an equal number of boys and girls (neutral)\n= probability of a boy when previously have had more boys than girls (boy bias)\n= probability of a boy when previously have had more girls than boys (girl bias)\n\nWrite out the likelihood function given Model 2 for the small set of data in (1). [You could also try BGG, GGB, BBB just for fun.]"
  },
  {
    "objectID": "labs/02_likelihood/02_likelihood.html#code",
    "href": "labs/02_likelihood/02_likelihood.html#code",
    "title": "Chapter 2 - Likelihoods",
    "section": "Code",
    "text": "Code\n\nModel 1: Sex Unconditional Model\nAssumes the sex for each birth is independent of other births\n\n# Evaluate small set of possible values of pb\npb &lt;- c(.3, .35, .375, .4, .45)  # possible values for prob a boy is born\nlik &lt;- pb^3 * (1 - pb)^5         # likelihood of getting observed data\ncbind(pb, lik)                   # print table of results\nplot(pb, lik, xlab = \"possible values of pb\", ylab = \"Likelihood\")\nmax(lik)                         # maximum likelihood over 5 values of pb\npb[lik == max(lik)]              # value of pb where likelihood maximized\n\n# Evaluate finer grid of possible values of pb\npb &lt;- seq(0, 1, length = 1001)   # possible values for prob a boy is born\nlik &lt;- pb^3 * (1 - pb)^5         # likelihood of getting observed data\nplot(pb, lik, xlab = \"possible values of pb\", ylab = \"Likelihood\", type = \"l\")\nmax(lik)                         # maximum likelihood over 1001 values of pb\npb[lik == max(lik)]              # value of pb where likelihood maximized\n\nloglik &lt;- 3 * log(pb) + 5 * log(1 - pb)      # find log likelihoods\nplot(pb, loglik, xlab = \"possible values of pb\", ylab = \"Loglikelihood\", \n     type = \"l\")\nmax(loglik)                    # maximum loglikelihood over 1001 values of pb\npb[loglik == max(loglik)]      # likelihood and loglikelihood max at same spot\nmle_pb_m1_small &lt;- pb[loglik == max(loglik)]\nmax_logL_m1_small &lt;- max(loglik)\n\n# Create ggplot of likelihood and log-likelihood functions\nmodel1grid &lt;- data.frame(pb = pb, lik1 = lik, loglik1 = loglik)\nggplot(data = model1grid, aes(x = pb, y = lik1)) +\n  geom_line() +\n  labs(x = \"possible values of pb\", y = \"Likelihood\")\nggplot(data = model1grid, aes(x = pb, y = loglik1)) +\n  geom_line() +\n  labs(x = \"possible values of pb\", y = \"log Likelihood\")\n\n\n# Apply Model 1 to NLSY data (for families with 3 or fewer children)\npb &lt;- seq(0, 1, length = 10001)   # possible values for prob a boy is born\nlik &lt;- pb^5416 * (1 - pb)^5256    # likelihood (too small)\nmax(lik)\nsummary(lik)  \n\n# loglikelihood of getting observed data\nloglik &lt;- 5416 * log(pb) + 5256 * log(1 - pb)   \nplot(pb, loglik, xlab = \"possible values of pb\", ylab = \"Loglikelihood\", \n     type = \"l\")\nplot(pb[loglik &gt; (-7500)], loglik[loglik &gt; (-7500)],\n  xlab = \"possible values of pb\", \n  ylab = \"Loglikelihood\", type = \"l\")  # zoom plot\nmax(loglik)                 # maximum loglikelihood over all values of pb\npb[loglik == max(loglik)]   # MLE of pb\nmle_pb_m1_nlsy &lt;- pb[loglik == max(loglik)]\nmax_logL_m1_nlsy &lt;- max(loglik)\n\n# Create ggplot of likelihood and log-likelihood functions\nmodel1grid &lt;- data.frame(pb = pb, lik1 = lik, loglik1 = loglik)\nggplot(data = model1grid, aes(x = pb, y = lik1)) +\n  geom_line() +\n  labs(x = \"possible values of pb\", y = \"Likelihood\")\nggplot(data = model1grid, aes(x = pb, y = loglik1)) +\n  geom_line() +\n  labs(x = \"possible values of pb\", y = \"log Likelihood\")\nmodel1grid |&gt;\n  filter(loglik1 &gt; (-7500)) |&gt;\n  ggplot(aes(x = pb, y = loglik1)) +\n    geom_line() +\n    labs(x = \"possible values of pb with loglik above -7500\", \n         y = \"log Likelihood\")\n\n\n\nModel 2: Sex Conditional Model (small 3 famly data set)\nAssumes probability of having a boy depends on whether you’ve had boys previously\n\n# Find MLEs with 3-dimensional grid search\npbb &lt;- seq(0, 1, length = 101)\npbg &lt;- seq(0, 1, length = 101)\npbn &lt;- seq(0, 1, length = 101)\nmodel2_grid &lt;- expand.grid(pbb = pbb, pbg = pbg, pbn = pbn)\n\nlik &lt;- model2_grid$pbn^1 * (1 - model2_grid$pbn)^3 * model2_grid$pbb^1 * \n  (1 - model2_grid$pbb)^1 * model2_grid$pbg^1 * (1 - model2_grid$pbg)^1\nloglik &lt;- 1 * log(model2_grid$pbn) + 3 * log(1 - model2_grid$pbn) + \n  1 * log(model2_grid$pbb) + 1 * log(1 - model2_grid$pbb) + \n  1 * log(model2_grid$pbg) + 1 * log(1 - model2_grid$pbg)\n\n# Print results\nmax(lik)        # maximum likelihood over all combos of pbb, pbg, pbn\nmax(loglik)     # maximum loglikelihood over all combos of pbb, pbg, pbn\n\nmodel2_grid$pbb[loglik == max(loglik)]   # MLE of pbb\nmodel2_grid$pbg[loglik == max(loglik)]   # MLE of pbg\nmodel2_grid$pbn[loglik == max(loglik)]   # MLE of pbn\n\n# Store results\nmle_pbb_m2_small &lt;- model2_grid$pbb[loglik == max(loglik)]   # MLE of pbb\nmle_pbg_m2_small &lt;- model2_grid$pbg[loglik == max(loglik)]   # MLE of pbg\nmle_pbn_m2_small &lt;- model2_grid$pbn[loglik == max(loglik)]   # MLE of pbn\n\nmax_L_m2_small &lt;- max(lik)\nmax_logL_m2_small &lt;- max(loglik)\n\n\n\nModel comparisons - Model 1 vs. Model 2\n\nlrt &lt;- 2 * (max_logL_m2_small - max_logL_m1_small)   \nlrt                       # likelihood ratio test statistic\n1 - pchisq(lrt, df = 2)   # p-value for testing Ho: no diff between Models 1&2\n\n# AIC and BIC values for Models 1 and 2\naic1 &lt;- -2 * max_logL_m1_small + 2 * 1\naic1\nbic1 &lt;- -2 * max_logL_m1_small + log(8) * 1\nbic1\naic2 &lt;- -2 * max_logL_m2_small + 2 * 3\naic2\nbic2 &lt;- -2 * max_logL_m2_small + log(8) * 3\nbic2"
  },
  {
    "objectID": "labs/03_poisson/03_FullMoon.html",
    "href": "labs/03_poisson/03_FullMoon.html",
    "title": "Chapter 4 - Poisson Regression",
    "section": "",
    "text": "Copy the project lab folder located at Home -&gt; STA363_inst_files-&gt; labs. If you check the box next to the folder name, then click the small gear icon you can “copy to” and put a copy of the folder in your newly created folder.\nNow, click File-&gt; Open Project and navigate to the project file in the folder you just copied.\nYou can place your responses in the file qmd file included."
  },
  {
    "objectID": "labs/03_poisson/03_FullMoon.html#lab-setup",
    "href": "labs/03_poisson/03_FullMoon.html#lab-setup",
    "title": "Chapter 4 - Poisson Regression",
    "section": "",
    "text": "Copy the project lab folder located at Home -&gt; STA363_inst_files-&gt; labs. If you check the box next to the folder name, then click the small gear icon you can “copy to” and put a copy of the folder in your newly created folder.\nNow, click File-&gt; Open Project and navigate to the project file in the folder you just copied.\nYou can place your responses in the file qmd file included."
  },
  {
    "objectID": "labs/03_poisson/03_FullMoon.html#introduction",
    "href": "labs/03_poisson/03_FullMoon.html#introduction",
    "title": "Chapter 4 - Poisson Regression",
    "section": "Introduction",
    "text": "Introduction\nAn article in the British Medical Journal by Bhattacharjee et al. (2000) investigated whether animals bite more often during full moons. To address this question, the researchers conducted a retrospective observational analysis of 1621 consecutive patients who presented at an English hospital ER between 1997 and 1999 with an animal bite. The data is found in bites.csv, and relevant R code can be found below.\nVariables include:\n\nlunar.cycle = period of lunar cycle (1-10), where 10 = full moon\nn.days = number of days in that period\nn.bites = number of patients presenting with an animal bite during that period\n\nLink to the article HERE"
  },
  {
    "objectID": "labs/03_poisson/03_FullMoon.html#questions",
    "href": "labs/03_poisson/03_FullMoon.html#questions",
    "title": "Chapter 4 - Poisson Regression",
    "section": "Questions",
    "text": "Questions\n\nIs there initial evidence of more bites during full moon phases?\nWhy can’t we just perform a t-test comparing full moon periods to non-full moon periods using period-level data (n=10)?\nWhat options can you think of for handling the fact that Period 5 is based on only 2 lunar days? What are the modeling implications of including number of days in a period as an offset term?\nInterpret model parameters from fit1.\nIs there any evidence of lack of fit in fit1? What factors may lead to lack of fit?\nDoes full moon have an effect above and beyond the linear cycle trend? Interpret the coefficient and a 95% confidence interval for the fullmoon term in fit3.\nWhat is fit4 doing? Does it offer an improvement over fit3?\nWhat evidence is there that fit5 (which uses bitesbyday.csv) is the analysis performed by the authors of this paper? See the article HERE.\nBased on this analysis, does it make you more wary of animal bites during full moons?\n\n\nAddressing issues of overdispersion in fit5.\n\nIs there evidence of lack of fit in fit5? Cite evidence both from a goodness of fit test and from comparing means and variances by period.\nIf overdispersion goes uncorrected, what are implications for p-values and CIs for model coefficients?\n\n\nOverdispersion parameter adjustment.\nOne solution is to simply add a second parameter to inflate variances, so that \\(Var(Y_i)=\\phi\\lambda_i\\). This is called a “quasi-Poisson” or, in general, a “quasi-likelihood” approach, because our data no longer follows a true Poisson distribution.\n\n\nNegative binomial modeling.\nAnother solution is to model response using a negative binomial distribution, so that \\(Y\\sim NegBinom(\\theta,p)\\). This distribution comes about if \\(Y|λ\\sim Poisson(λ)\\), but the \\(\\lambda\\)’s themselves are randomly chosen according to a gamma distribution: \\(\\lambda \\sim gamma(θ,\\frac{(1-p)}{p})\\). In this case, $E(Y)==$ and \\(Var(Y)=\\theta \\frac{p}{(1-p)^2} =\\mu+\\frac{\\mu^2}{\\theta}\\), so that \\(\\frac{\\mu^2}{\\theta}\\) is the amount of overdispersion.\n\nCompare the following estimates, tests, and intervals under usual Poisson regression, quasi-Poisson regression, and negative binomial regression:\n\n\n\n\n\n\n\n\n\n\n\nPoisson\nQuasi-poisson\nNegative Binomial\n\n\n\n\n\\(\\phi\\)\n\n\n\n\n\n\\(SE(\\hat{\\beta}_4\\))\n\n\n\n\n\nWald-type test stat\n\n\n\n\n\nWald-type p-value\n\n\n\n\n\nLRT-type test stat\n\n\n\n\n\nLRT-type test stat\n\n\n\n\n\nLRT-type p-value\n\n\n\n\n\nCI - profile for \\(e^{\\beta_4}\\)\n\n\n\n\n\nCI-Wald-type for \\(e^{\\beta_4}\\)"
  },
  {
    "objectID": "labs/03_poisson/03_FullMoon.html#code",
    "href": "labs/03_poisson/03_FullMoon.html#code",
    "title": "Chapter 4 - Poisson Regression",
    "section": "Code",
    "text": "Code\n\nSetup\n\nlibrary(MASS)\nlibrary(dplyr)\nlibrary(maxLik)   # will have to install first\nlibrary(ggplot2)\nlibrary(gridExtra) \n\nbites.data &lt;- read.csv(\"data/bites.csv\")\nbites.day &lt;- read.csv(\"data/bitesbyday.csv\")\n\n\n\nbites.csv: analysis and offsets\n\n# Exploratory Data Analysis\nbites.data\n\n# Average n.bites by day for each cycle to make them comparable\nbites.data &lt;- mutate(bites.data, avg.bites=n.bites/n.days, \n                     fullmoon = ifelse(lunar.cycle==10,1,0))\nwith(bites.data, summary(avg.bites))\nwith(bites.data, summary(n.bites))\nwith(bites.data, by(avg.bites,fullmoon,summary))\n# t.test(avg.bites~fullmoon, data=bites.data)    # produces an error -why?\n\nggplot(data = bites.data, aes(x = avg.bites)) + \n  geom_histogram(bins = 4)\nggplot(data = bites.data, aes(x = n.bites)) + \n  geom_histogram(bins = 4)\nggplot(data = bites.data, aes(x = lunar.cycle, y = avg.bites)) + \n  geom_point() + geom_smooth(method=\"lm\") \n\n# Now try modeling as Poisson counts for each cycle.\n# However, Cycle 5 contains only 2 days whereas all others have 3.  \n\n# The simplest model (Model 1) uses only an indicator for the full moon cycle\n#   plus an offset for the number of days so the counts are comparable.\nfit1 &lt;-  glm(n.bites ~ fullmoon, family=poisson, offset=log(n.days),\n             data=bites.data)\nsummary(fit1)\nexp(fit1$coef)\nexp(confint(fit1))\n\n# Alternative (Wald) confidence intervals\nfit1coef &lt;- summary(fit1)$coefficients[,1]\nfit1se &lt;- summary(fit1)$coefficients[,2]\nlb &lt;- fit1coef - qnorm(.975)*fit1se\nub &lt;- fit1coef + qnorm(.975)*fit1se\ncbind(exp(lb),exp(ub))\n\n#Signs of lack-of-fit with Simple Model\ngof &lt;- 1-pchisq(fit1$deviance, fit1$df.residual)\ngof \n\n# Model 2:  Linear cycle trend\nfit2 &lt;-  glm(n.bites ~ lunar.cycle, family=poisson, offset=log(n.days),\n             data=bites.data)\nsummary(fit2)\ngof &lt;- 1-pchisq(fit2$deviance, fit2$df.residual)\ngof \n\n# Model 3: Does full moon have effect above and beyond linear cycle trend?\nfit3 &lt;- glm(n.bites ~ lunar.cycle + fullmoon, family=poisson, \n            offset=log(n.days), data=bites.data)\nsummary(fit3)\nexp(fit3$coef)\nexp(confint(fit3))\nanova(fit2, fit3, test = \"Chisq\")\ngof &lt;- 1-pchisq(fit3$deviance, fit3$df.residual)\ngof \n\n# Model 4: What does this model do?\nbites.data4 &lt;- mutate(bites.data, dist = c(3,6,9,12,14.5,12,9,6,3,0))\nfit2a &lt;- glm(n.bites ~ dist, family=poisson, offset=log(n.days), data=bites.data4)\nsummary(fit2a)\n\nfit4 &lt;- glm(n.bites ~ dist + fullmoon, family=poisson, offset=log(n.days),\n            data=bites.data4)\nsummary(fit4)\n\n\n\nbitesbyday.csv: analysis and overdispersion\n\n# Model 5: Try to replicate model from paper, using daily data I transcribed \n#          from Figure 1 and matched to period totals\nbites.day &lt;- bites.day %&gt;%\n  mutate(period = factor(period, levels = c(5,1:4,6:10)),\n         period_no4 = factor(period_no4, levels = c(5, 1:3, 6:10))\n)\n\nfit5 &lt;- glm(bites ~ period, family=poisson, data=bites.day)\nsummary(fit5)\nexp(fit5$coef)\nexp(confint(fit5))\ngof &lt;- 1-pchisq(fit5$deviance, fit5$df.residual)\ngof \n\nfit5reduced &lt;- glm(bites ~ period_no4, family = poisson, data = bites.day)\nsummary(fit5reduced)\nanova(fit5reduced, fit5, test = \"Chisq\")\n\n# Alternative (Wald) confidence intervals\nfit5coef &lt;- summary(fit5)$coefficients[,1]\nfit5se &lt;- summary(fit5)$coefficients[,2]\nlb &lt;- fit5coef - qnorm(.975)*fit5se\nub &lt;- fit5coef + qnorm(.975)*fit5se\ncbind(exp(lb),exp(ub))\n\n\n# Might overdispersion be a factor in lack of fit?\nbites.day %&gt;%\n  group_by(period) %&gt;%\n  summarise(count = n(),\n            mean_bites = mean(bites),\n            var_bites = var(bites))\n\n\n# Adjust fit1 for overdispersion\nfit5a &lt;- glm(bites ~ period, family=quasipoisson, data=bites.day)\nsummary(fit5a)\nexp(confint(fit5a))\n\nfit5areduced &lt;- glm(bites ~ period_no4, family = quasipoisson, data = bites.day)\nsummary(fit5areduced)\nanova(fit5areduced, fit5a, test = \"F\")\n\nphi &lt;- sum(resid(fit5, type='pearson')^2) / fit5$df.residual\nphi\ndrop.in.dev &lt;- fit5reduced$deviance - fit5$deviance\ndiff.in.df &lt;- fit5reduced$df.residual - fit5$df.residual\nFstat &lt;- drop.in.dev / summary(fit5a)$dispersion\nFstat\n1-pf(Fstat, diff.in.df, fit5$df.residual)\n\n# Alternative (Wald) confidence intervals with QL\nfit5acoef &lt;- summary(fit5a)$coefficients[,1]\nfit5ase &lt;- summary(fit5a)$coefficients[,2]\nlb &lt;- fit5acoef - qt(.975, fit5a$df.residual)*fit5ase\nub &lt;- fit5acoef + qt(.975, fit5a$df.residual)*fit5ase\ncbind(exp(lb),exp(ub))\n\n\n# Model fit1 using negative binomial\nfit5b &lt;- glm.nb(bites ~ period, data=bites.day)\nsummary(fit5b)\nexp(confint(fit5b))\n\nfit5breduced &lt;- glm.nb(bites ~ period_no4, data = bites.day)\nsummary(fit5breduced)\nanova(fit5breduced, fit5b, test = \"Chisq\")\n\n# Alternative (Wald) confidence intervals with NB\nfit5bcoef &lt;- summary(fit5b)$coefficients[,1]\nfit5bse &lt;- summary(fit5b)$coefficients[,2]\nlb &lt;- fit5bcoef - qnorm(.975)*fit5bse\nub &lt;- fit5bcoef + qnorm(.975)*fit5bse\ncbind(exp(lb),exp(ub))"
  },
  {
    "objectID": "labs/04_logistic/04_moths.html",
    "href": "labs/04_logistic/04_moths.html",
    "title": "Chapter 6 - Binomial Regression",
    "section": "",
    "text": "Copy the project lab folder located at Home -&gt; STA363_inst_files-&gt; labs. If you check the box next to the folder name, then click the small gear icon you can “copy to” and put a copy of the folder in your newly created folder.\nNow, click File-&gt; Open Project and navigate to the project file in the folder you just copied.\nYou can place your responses in the file qmd file included."
  },
  {
    "objectID": "labs/04_logistic/04_moths.html#lab-setup",
    "href": "labs/04_logistic/04_moths.html#lab-setup",
    "title": "Chapter 6 - Binomial Regression",
    "section": "",
    "text": "Copy the project lab folder located at Home -&gt; STA363_inst_files-&gt; labs. If you check the box next to the folder name, then click the small gear icon you can “copy to” and put a copy of the folder in your newly created folder.\nNow, click File-&gt; Open Project and navigate to the project file in the folder you just copied.\nYou can place your responses in the file qmd file included."
  },
  {
    "objectID": "labs/04_logistic/04_moths.html#introduction",
    "href": "labs/04_logistic/04_moths.html#introduction",
    "title": "Chapter 6 - Binomial Regression",
    "section": "Introduction",
    "text": "Introduction\nAn article in the Journal of Animal Ecology by Bishop (1972) investigated whether moths provide evidence of “survival of the fittest” with their camouflage traits. Researchers glued equal numbers of light and dark morph moths in lifelike positions on tree trunks at 7 locations from 0 to 51.2 km from Liverpool. They then recorded the numbers of moths removed after 24 hours, presumably by predators. The hypothesis was that, since tree trunks near Liverpool were blackened by pollution, light morph moths would be more likely to be removed near Liverpool. The data is found in moth.csv, and relevant R code can be found under Moths.Rmd. Variables include:\n\nMORPH = light or dark\nDISTANCE = kilometers from Liverpool\nPLACED = number of moths of a specific morph glued to trees at that location\nREMOVED = number of moths of a specific morph removed after 24 hours"
  },
  {
    "objectID": "labs/04_logistic/04_moths.html#questions",
    "href": "labs/04_logistic/04_moths.html#questions",
    "title": "Chapter 6 - Binomial Regression",
    "section": "Questions",
    "text": "Questions\n\nWhat do you think of the study design? Any suggestions for improvement?\nWhat are logits, and why would we want to plot logits vs. distance (rather than, say, proportion removed vs. distance)?\nWhat can we conclude from the empirical logit plots?\n\n\nModels breg2 and breg2a\n\nInterpret the 3 coefficient estimates from model “breg2”. Note that breg2 and breg2a provide two alternative ways to express the same model…\nWhat are the implications to using MORPH rather than “dark” in breg2b?\nHow do the predicted logits from breg2 fit the actual data? Note that the predict() function with type=”link” returns predicted logits, while type=”response” returns predicted probabilities…\n\n\n\nModels breg3\n\nInterpret the 4 coefficients estimates from model breg3.\nTest the significance of the interaction term in breg3 in two ways. Do both methods agree?\nTest the goodness of fit for model breg3. What can we conclude about this model?\nIs there evidence of extra-binomial variation (overdispersion) in breg3?\n\n\n\nModels breg4 and breg4a\n\nRegardless of your answer to (9), repeat (7) after adjusting for overdispersion (breg4).\nCompare confidence intervals for the interaction term in breg3 with and without adjusting for overdispersion, and with and without using profile likelihoods.\n\n\n\nModel breg5 and breg5a\n\nWhat are the implications in breg5 of including DISTANCE as a factor variable? How does this change model interpretations? Does it lead to an improved model?\nWhat happens if we expand the data set to contain one row per moth (968 rows)? Now we can run a logistic regression model. How does the logistic regression model “lreg1” compare to the binomial regression model breg3? What are similarities and differences? Would there be any reason to run a binomial regression rather than a logistic regression in a case like this?"
  },
  {
    "objectID": "labs/05_multilevel_mod/05_happy.html",
    "href": "labs/05_multilevel_mod/05_happy.html",
    "title": "Chapter 8 - Multilevel Models",
    "section": "",
    "text": "Copy the project lab folder located at Home -&gt; STA363_inst_files-&gt; labs. If you check the box next to the folder name, then click the small gear icon you can “copy to” and put a copy of the folder in your newly created folder.\nNow, click File-&gt; Open Project and navigate to the project file in the folder you just copied.\nYou can place your responses in the file qmd file included."
  },
  {
    "objectID": "labs/05_multilevel_mod/05_happy.html#lab-setup",
    "href": "labs/05_multilevel_mod/05_happy.html#lab-setup",
    "title": "Chapter 8 - Multilevel Models",
    "section": "",
    "text": "Copy the project lab folder located at Home -&gt; STA363_inst_files-&gt; labs. If you check the box next to the folder name, then click the small gear icon you can “copy to” and put a copy of the folder in your newly created folder.\nNow, click File-&gt; Open Project and navigate to the project file in the folder you just copied.\nYou can place your responses in the file qmd file included."
  },
  {
    "objectID": "labs/05_multilevel_mod/05_happy.html#introduction",
    "href": "labs/05_multilevel_mod/05_happy.html#introduction",
    "title": "Chapter 8 - Multilevel Models",
    "section": "Introduction",
    "text": "Introduction\nIn this chapter, we studied models for predicting music performance anxiety, as measured by the negative affect scale from the PANAS instrument. Now we will examine models for predicting the happiness of musicians prior to performances, as measured by the positive affect scale from the PANAS instrument.\nThe data musicdata.csv come from the Sadler and Miller (2010) study of the emotional state of musicians before performances. The dataset contains information collected from 37 undergraduate music majors who completed the Positive Affect Negative Affect Schedule (PANAS), an instrument produces a measure of anxiety (negative affect) and a measure of happiness (positive affect). This analysis will focus on negative affect as a measure of performance anxiety.\nThe primary variables we’ll use are\n\nid = unique musician identification number\ndiary = cumulative total of diaries filled out by musician\nperf_type = type of performance (Solo, Large Ensemble, or Small Ensemble)\naudience = who attended (Instructor, Public, Students, or Juried)\nmemory = performed from Memory, using Score, or Unspecified\nna = negative affect score from PANAS\npa = positive affect score from PANAS\ngender = musician gender\ninstrument = Voice, Orchestral, or Piano\nmpqab = absorption subscale from MPQ\nmpqpem = positive emotionality (PEM) composite scale from MPQ\nmpqnem = negative emotionality (NEM) composite scale from MPQ\n\n\nModels\nTo begin, run the following models:\n\nModel A = unconditional means model \nModel B = indicator for instructor audience type and indicator for student audience type at Level One; no Level Two predictors \nModel C = indicator for instructor audience type and indicator for student audience type at Level One; centered MPQ absorption subscale as Level Two predictor for intercept and all slope terms \nModel D = indicator for instructor audience type and indicator for student audience type at Level One; centered MPQ absorption subscale and a male indicator as Level Two predictors for intercept and all slope terms\n\n\n\nQuestions\n\nPerform an exploratory data analysis by comparing positive affect (happiness) to Level One and Level Two covariates using appropriate graphs. Comment on interesting trends, supporting your comments with appropriate summary statistics.\nReport estimated fixed effects and variance components from Model A, using proper notation from this chapter (no interpretations required). Also report and interpret an intraclass correlation coefficient.\nReport estimated fixed effects and variance components from Model B, using proper notation from this chapter. Interpret your MLE estimates for \\(\\hat{\\alpha}_{0}\\) (the intercept), and \\(\\hat{\\sigma}_{u}\\) (the Level Two standard deviation for the intercept). Also report and interpret an appropriate pseudo R-squared value for model A vs model B.\nWrite out Model C, using both separate Level One and Level Two models as well as a composite model. Be sure to express distributions for error terms. How many parameters must be estimated in Model C?\nReport and interpret the following parameter estimates from Model C: \\(\\hat{\\alpha}_{0}\\), \\(\\hat{\\alpha}_{1}\\), \\(\\hat{\\gamma}_{0}\\), \\(\\hat{\\beta}_{1}\\), \\(\\hat{\\sigma}_{u}\\), \\(\\hat{\\sigma}_{v}\\), and \\(\\hat{\\rho}_{uv}\\). Interpretations for variance components should be done in terms of standard deviations and correlation coefficients.\nReport and interpret the same parameter estimates listed above from Model D. In each case, the new interpretation should involve a small modification of your interpretation from Model C. Use underlines or highlights to denote the part of the Model D interpretation that differs from the Model C interpretation.\nAlso report and interpret the following parameter estimates from Model D: \\(\\hat{\\alpha}_{2}\\) and \\(\\hat{\\beta}_{2}\\).\nUse a drop in deviance statistic (likelihood ratio test) to compare Model C vs. Model D. Give a test statistic and p-value, then state a conclusion. Also compare Models C and D with appropriate pseudo R-squared value(s) and with AIC and BIC statistics."
  },
  {
    "objectID": "labs/06_non_linear/06_wage.html",
    "href": "labs/06_non_linear/06_wage.html",
    "title": "Moving Beyond Linearity - Chapter 7 (ISLR)",
    "section": "",
    "text": "Copy the project lab folder located at Home -&gt; STA363_inst_files-&gt; labs. If you check the box next to the folder name, then click the small gear icon you can “copy to” and put a copy of the folder in your newly created folder.\nNow, click File-&gt; Open Project and navigate to the project file in the folder you just copied.\nYou can place your responses in the file qmd file included."
  },
  {
    "objectID": "labs/06_non_linear/06_wage.html#lab-setup",
    "href": "labs/06_non_linear/06_wage.html#lab-setup",
    "title": "Moving Beyond Linearity - Chapter 7 (ISLR)",
    "section": "",
    "text": "Copy the project lab folder located at Home -&gt; STA363_inst_files-&gt; labs. If you check the box next to the folder name, then click the small gear icon you can “copy to” and put a copy of the folder in your newly created folder.\nNow, click File-&gt; Open Project and navigate to the project file in the folder you just copied.\nYou can place your responses in the file qmd file included."
  },
  {
    "objectID": "labs/06_non_linear/06_wage.html#packages",
    "href": "labs/06_non_linear/06_wage.html#packages",
    "title": "Moving Beyond Linearity - Chapter 7 (ISLR)",
    "section": "Packages",
    "text": "Packages\nIn this lab we will work with three packages: tidyverse which is a collection of packages for doing data analysis in a “tidy” way, tidymodels for statistical model coefficients, and splines for our splines models.\nIf you’d like to run your code in the Console as well you’ll also need to load the packages there. To do so, run the following in the console.\n\nlibrary(tidyverse) \nlibrary(tidymodels)\nlibrary(splines)"
  },
  {
    "objectID": "labs/06_non_linear/06_wage.html#data",
    "href": "labs/06_non_linear/06_wage.html#data",
    "title": "Moving Beyond Linearity - Chapter 7 (ISLR)",
    "section": "Data",
    "text": "Data\nFor this lab, we are using wage data adapted from the ISLR package. This can be loaded in by running:\n\nload(\"data/wage.rda\")\n\nBelow is a data dictionary for this dataset:\n\n\n\n\n\n\n\nvariable\ndefinition\n\n\n\n\nyear\nYear that wage information was recorded\n\n\nage\nAge of worker\n\n\nmaritl\nA factor with levels 1. Never Married 2. Married 3. Widowed 4. Divorced and 5. Separated indicating marital status\n\n\nrace\nA factor with levels 1. White 2. Black 3. Asian and 4. Other indicating race\n\n\neducation\nA factor with levels 1. &lt; HS Grad 2. HS Grad 3. Some College 4. College Grad and 5. Advanced Degree indicating education level\n\n\nregion\nRegion of the country (mid-atlantic only)\n\n\njobclass\nA factor with levels 1. Industrial and 2. Information indicating type of job\n\n\nhealth\nA factor with levels 1. &lt;=Good and 2. &gt;=Very Good indicating health level of worker\n\n\nhealth_ins\nA factor with levels 1. Yes and 2. No indicating whether worker has health insurance\n\n\nlogwage\nLog of workers wage\n\n\nwage\nWorkers raw wage"
  },
  {
    "objectID": "mini_project/02_project_GLM.html",
    "href": "mini_project/02_project_GLM.html",
    "title": "Mini Project 2",
    "section": "",
    "text": "Pick a partner.\nFind an appropriate data set.\n\nGLM’s are broadly applicable and there are no requirements on the type of data the response needs to follow.\nThe data you choose should have a few predictors that may be useful when modeling (ideally at least 1 quantitative and 1 categorical).\n\n\nData Options. Check out Data Links on the Useful Links part of the course website. TidyTuesday has quickly accessible data.\n\nWrite out the anticipated cleaning and/or feature engineering steps you will need to take. Some examples:\n\nCreating simplified categorical variables or transforming a continuous variable into categorical.\nAggregating data\nConverting date columns\netc.\n\nSetup your R Project(s) on the server.\nRead your data into R. This likely will require you to download the data to your computer and upload the data the server."
  },
  {
    "objectID": "mini_project/02_project_GLM.html#timing",
    "href": "mini_project/02_project_GLM.html#timing",
    "title": "Mini Project 2",
    "section": "Timing",
    "text": "Timing\nThis project will be in a workshop style. The intention is for you to start and finish by the end of class time. We will follow a timeline:\n\n\n\n\n\n\n\nTask\nTiming\n\n\n\n\nClean Data\n9:00 am - 9:30 am\n\n\nPerform EDA\n9:30 am - 10:30 am\n\n\nFit, Assess, and Compare Regression Models\n10:30 am - 11:00 am, 1:00 pm - 2:00 pm\n\n\nPrepare presentation\n2:00 pm - 2:20 pm\n\n\nPresent your findings\n2:20 pm - 2:40 pm\n\n\nSubmit your Final Report\nSubmit HTML Sunday 9/8 at 11:59 pm\n\n\n\n\nGrading\nEach Mini Project is worth 50 points (Labs are 10 points each).\n\n\n\n\n\n\n\nCategory\nPoints\n\n\n\n\nThe data chosen is appropriate, and the cleaning steps are correct and explained.\n5\n\n\nEDA is thorough. All included graphs and tables are paired with a discussion. EDA supports the choice of modeling technique.\n15\n\n\nThe model fitting process has a logical flow. Multiple models are considered and compared using statistical tests and multiple metrics. Any model that is interpreted has been assessed using residual plots and appropriate statistical tests.\n15\n\n\nThe code follows a sensible order and has been appropriately commented on.\n5\n\n\nThe presentation is concise, describes the data, highlights key parts of the EDA, describes minimally the final model, gives at least 1 interpretation in the context of a coefficient, and discusses limitations and potential future work.\n5\n\n\nThe report is well written, with correct spelling and grammar. The used code is included either inline or in an appendix at the end.\n5\n\n\n\n\n\nSubmission\nAdd format part of your final report document and then re-render:\n```{r}\n#| label: yaml_example\n#| eval: false\n---\ntitle: \"Document title\"\nauthor: \"my name\"\nformat:\n  html:\n    embed-resources: true\n---\n```\nWhen you are finished with your homework, be sure to Render the final document. Once rendered, you can download your file by:\n\nFinding the .html file in your File pane (on the bottom right of the screen)\nClick the check box next to the file\nClick the blue gear above and then click “Export” to download\nSubmit your final html document to the respective assignment on Moodle"
  },
  {
    "objectID": "project.html",
    "href": "project.html",
    "title": "Class Project",
    "section": "",
    "text": "You will be analyzing a dataset using regression analysis and a classification analysis.\nCollaboration: You can work individually or in groups of up to 3.\nAs one of my past professors liked to say, “Keep it Simple Stupid (KISS)”"
  },
  {
    "objectID": "project.html#part-1-research-questions-and-data",
    "href": "project.html#part-1-research-questions-and-data",
    "title": "Class Project",
    "section": "Part 1: Research Questions and Data",
    "text": "Part 1: Research Questions and Data\n\nDetermine a topic\nFind data that goes with that topic. See HERE.\n\n\nMust have at least 5 variables that may be useful in a classification and/or regression model.\n\n\nClean and prepare that data.\nWrite two research questions that you will use that data to investigate.\n\n\nOne question that can be answered with regression; make clear the outcome variable and its units.\nOne question that can be answered with classification; make clear the outcome variable and its possible categories.\n\n\nSubmit a project-proposal_last_names.html document that includes\n\n\nsource or sources of your data\na glimpse of your data (run glimpse())\nvariable definitions\nYour research questions\n\nMoodle link: HERE"
  },
  {
    "objectID": "project.html#part-2-eda",
    "href": "project.html#part-2-eda",
    "title": "Class Project",
    "section": "Part 2: EDA",
    "text": "Part 2: EDA\n\nConduct a full EDA on your dataset including any variables being considered for either research question. Minimally this should include the following. All included graphs/tables must have comments and discussion.\n\n\nClearly describe what the cases in the final clean dataset represent.\nWho collected the data? When, why, and how? Answer as much of this as the available information allows.\nUnivariate distributions of all variables.\nGraphs that specifically compare potential predictors to the variables that will be your response variables.\n\n\nSubmit a eda_last_names.html document to Moodle HERE."
  },
  {
    "objectID": "project.html#part-3-modeling",
    "href": "project.html#part-3-modeling",
    "title": "Class Project",
    "section": "Part 3: Modeling",
    "text": "Part 3: Modeling\nThe following should be included in your modeling report (generated from a Quarto file, or files).\n\nRegression\n\nRegression: Methods\n\nDescribe the models used.\nDescribe what you did to evaluate models.\n\nIndicate how you estimated quantitative evaluation metrics.\nIndicate what plots you used to evaluate models.\n\nDescribe the goals / purpose of the methods used in the overall context of your research investigations.\n\n\n\nRegression: Results\n\nSummarize your final model and justify your model choice (see below for ways to justify your choice).\n\nCompare the different models in light of evaluation metrics, plots, variable importance, and data context.\nDisplay evaluation metrics for different models in a clean, organized way. This display should include both the estimated CV metric as well as its standard deviation.\nBroadly summarize conclusions from looking at these CV evaluation metrics and their measures of uncertainty.\nSummarize conclusions from residual plots from initial models (don’t have to display them though).\n\nShow and interpret some representative examples of residual plots for your final model. Does the model show acceptable results in terms of any systematic biases?\n\n\n\nRegression: Conclusions\n\nInterpret you final model (show plots of estimated non-linear functions, or slope coefficients) for important predictors, and provide some general interpretations of what you learn from these\nInterpret evaluation metric(s) for the final model in context with units. Does the model show an acceptable amount of error?\nSummarization should show evidence of acknowledging the data context in thinking about the sensibility of these results.\n\n\n\n\nClassification\n\nClassification: Methods\n\nIndicate at least 2 different methods used to answer your classification research question.\n\nDescribe what you did to evaluate the models explored.\nIndicate how you estimated quantitative evaluation metrics.\n\nDescribe the goals / purpose of the methods used in the overall context of your research investigations.\n\n\n\nClassification: Results\n\nSummarize your final model and justify your model choice (see below for ways to justify your choice).\n\nCompare the different classification models tried in light of evaluation metrics, variable importance, and data context.\nDisplay evaluation metrics for different models in a clean, organized way. This display should include both the estimated metric as well as its standard deviation. (This won’t be available from OOB error estimation. If using OOB, don’t worry about reporting the SD.)\nBroadly summarize conclusions from looking at these evaluation metrics and their measures of uncertainty.\n\n\n\n\nClassification: Conclusions\n\nInterpret evaluation metric(s) for the final model in context. Does the model show an acceptable amount of error?\n\nIf using OOB error estimation, display the test (OOB) confusion matrix, and use it to interpret the strengths and weaknesses of the final model.\n\nSummarization should show evidence of acknowledging the data context in thinking about the sensibility of these results.\n\n\n\n\nSubmit\nSubmit one (or two) model_last_names.html document(s) with your analysis from the parts above. Submit on Moodle HERE."
  },
  {
    "objectID": "project.html#part-4-tell-people-about-it.",
    "href": "project.html#part-4-tell-people-about-it.",
    "title": "Class Project",
    "section": "Part 4: Tell people about it.",
    "text": "Part 4: Tell people about it.\nA 5-10 minute video presentation of your project. (Recording the presentation over Zoom is a good option for creating the video. You can record to your computer or to the cloud.)\n\nUpload the video to Google Drive.\nAll team members should have an equal speaking role in the presentation.\n\nIn order to record your presentation,\n\nStart a Zoom meeting and invite your project mates.\nOne of your share your screen with presentation slides (recommended: Quarto Presentation, Google Slides, or Powerpoint).\nPlease have everyone turn your video on so that we can see who is speaking.\nWhen you are ready to start, the host of the meeting (who ever started the meeting) can click Record on this Computer. I highly recommend that someone else start a timer so that you can make sure you keep the presentation to 10 minutes max.\nStart presenting!\nYou can Pause the recording, as needed, and then press start recording again.\nWhen you have finished recording, you can press Stop Recording. When you end the meeting, the recording (an mp4 file) will be downloaded to the computer of the individual who pressed Record.\nUpload the video to Google Drive.\n\nWe will watch these in class!\nSource: Brianna Heggeseth, STA 253"
  },
  {
    "objectID": "slides/02_likelihoods_ch2_o.html",
    "href": "slides/02_likelihoods_ch2_o.html",
    "title": "Beyond Least Squares: Using Likelihoods",
    "section": "",
    "text": "library(tidyverse)\nlibrary(GGally)\nlibrary(knitr)\nlibrary(patchwork)\nlibrary(viridis)\nlibrary(ggfortify)"
  },
  {
    "objectID": "slides/02_likelihoods_ch2_o.html#setup",
    "href": "slides/02_likelihoods_ch2_o.html#setup",
    "title": "Beyond Least Squares: Using Likelihoods",
    "section": "",
    "text": "library(tidyverse)\nlibrary(GGally)\nlibrary(knitr)\nlibrary(patchwork)\nlibrary(viridis)\nlibrary(ggfortify)"
  },
  {
    "objectID": "slides/02_likelihoods_ch2_o.html#learning-goals",
    "href": "slides/02_likelihoods_ch2_o.html#learning-goals",
    "title": "Beyond Least Squares: Using Likelihoods",
    "section": "Learning goals",
    "text": "Learning goals\n\nDescribe the concept of a likelihood\nConstruct the likelihood for a simple model\nDefine the Maximum Likelihood Estimate (MLE) and use it to answer an analysis question\nIdentify three ways to calculate or approximate the MLE and apply these methods to find the MLE for a simple model\nUse likelihoods to compare models (next week)"
  },
  {
    "objectID": "slides/02_likelihoods_ch2_o.html#what-is-the-likelihood",
    "href": "slides/02_likelihoods_ch2_o.html#what-is-the-likelihood",
    "title": "Beyond Least Squares: Using Likelihoods",
    "section": "What is the likelihood?",
    "text": "What is the likelihood?\nA likelihood is a function that tells us how likely we are to observe our data for a given parameter value (or values).\n\nUnlike Ordinary Least Squares (OLS), they do not require the responses be independent, identically distributed, and normal (iidN)\nThey are not the same as probability functions\n\nProbability function: Fixed parameter value(s) + input possible outcomes \\(\\Rightarrow\\) probability of seeing the different outcomes given the parameter value(s)\nLikelihood: Fixed data + input possible parameter values \\(\\Rightarrow\\) probability of seeing the fixed data for each parameter value"
  },
  {
    "objectID": "slides/02_likelihoods_ch2_o.html#fouls-in-college-basketball-games",
    "href": "slides/02_likelihoods_ch2_o.html#fouls-in-college-basketball-games",
    "title": "Beyond Least Squares: Using Likelihoods",
    "section": "Fouls in college basketball games",
    "text": "Fouls in college basketball games\nThe data set 04-refs.csv includes 30 randomly selected NCAA men’s basketball games played in the 2009 - 2010 season.\nWe will focus on the variables foul1, foul2, and foul3, which indicate which team had a foul called them for the 1st, 2nd, and 3rd fouls, respectively.\n\nH: Foul was called on the home team\nV: Foul was called on the visiting team\n\n. . .\nWe are focusing on the first three fouls for this analysis, but this could easily be extended to include all fouls in a game.\n\n[The dataset was derived from basektball0910.csv used in BMLR Section 11.2"
  },
  {
    "objectID": "slides/02_likelihoods_ch2_o.html#fouls-in-college-basketball-games-1",
    "href": "slides/02_likelihoods_ch2_o.html#fouls-in-college-basketball-games-1",
    "title": "Beyond Least Squares: Using Likelihoods",
    "section": "Fouls in college basketball games",
    "text": "Fouls in college basketball games\n\nrefs &lt;- read_csv(\"data/04-refs.csv\")\nrefs %&gt;% slice(1:5) %&gt;% kable()\n\n\n\n\ngame\ndate\nvisitor\nhometeam\nfoul1\nfoul2\nfoul3\n\n\n\n\n166\n20100126\nCLEM\nBC\nV\nV\nV\n\n\n224\n20100224\nDEPAUL\nCIN\nH\nH\nV\n\n\n317\n20100109\nMARQET\nNOVA\nH\nH\nH\n\n\n214\n20100228\nMARQET\nSETON\nV\nV\nH\n\n\n278\n20100128\nSETON\nSFL\nH\nV\nV\n\n\n\n\n\nWe will treat the games as independent in this analysis."
  },
  {
    "objectID": "slides/02_likelihoods_ch2_o.html#different-likelihood-models",
    "href": "slides/02_likelihoods_ch2_o.html#different-likelihood-models",
    "title": "Beyond Least Squares: Using Likelihoods",
    "section": "Different likelihood models",
    "text": "Different likelihood models\nModel 1 (Unconditional Model): What is the probability the referees call a foul on the home team, assuming foul calls within a game are independent?\n\nModel 2 (Conditional Model):\n\nIs there a tendency for the referees to call more fouls on the visiting team or home team?\nIs there a tendency for referees to call a foul on the team that already has more fouls?\n\n. . .\nUltimately we want to decide which model is better."
  },
  {
    "objectID": "slides/02_likelihoods_ch2_o.html#exploratory-data-analysis",
    "href": "slides/02_likelihoods_ch2_o.html#exploratory-data-analysis",
    "title": "Beyond Least Squares: Using Likelihoods",
    "section": "Exploratory data analysis",
    "text": "Exploratory data analysis\n\n\n\nrefs %&gt;%\ncount(foul1, foul2, foul3) %&gt;% kable()\n\n\n\n\nfoul1\nfoul2\nfoul3\nn\n\n\n\n\nH\nH\nH\n3\n\n\nH\nH\nV\n2\n\n\nH\nV\nH\n3\n\n\nH\nV\nV\n7\n\n\nV\nH\nH\n7\n\n\nV\nH\nV\n1\n\n\nV\nV\nH\n5\n\n\nV\nV\nV\n2\n\n\n\n\n\n\nThere are\n\n46 total fouls on the home team\n44 total fouls on the visiting team"
  },
  {
    "objectID": "slides/02_likelihoods_ch2_o.html#model-1-unconditional-model",
    "href": "slides/02_likelihoods_ch2_o.html#model-1-unconditional-model",
    "title": "Beyond Least Squares: Using Likelihoods",
    "section": "Model 1: Unconditional model",
    "text": "Model 1: Unconditional model\nWhat is the probability the referees call a foul on the home team, assuming foul calls within a game are independent?"
  },
  {
    "objectID": "slides/02_likelihoods_ch2_o.html#likelihood-1",
    "href": "slides/02_likelihoods_ch2_o.html#likelihood-1",
    "title": "Beyond Least Squares: Using Likelihoods",
    "section": "Likelihood",
    "text": "Likelihood\nLet \\(p_H\\) be the probability the referees call a foul on the home team.\nThe likelihood for a single observation\n\\[Lik(p_H) = p_H^{y_i}(1 - p_H)^{n_i - y_i}\\]\nWhere \\(y_i\\) is the number of fouls called on the home team.\n(In this example, we know \\(n_i = 3\\) for all observations.)\n. . .\nExample\nFor a single game where the first three fouls are \\(H, H, V\\), then\n\\[Lik(p_H) = p_H^{2}(1 - p_H)^{3 - 2} = p_H^{2}(1 - p_H)\\]"
  },
  {
    "objectID": "slides/02_likelihoods_ch2_o.html#model-1-likelihood-contribution",
    "href": "slides/02_likelihoods_ch2_o.html#model-1-likelihood-contribution",
    "title": "Beyond Least Squares: Using Likelihoods",
    "section": "Model 1: Likelihood contribution",
    "text": "Model 1: Likelihood contribution\n\n\n\nFoul1\nFoul2\nFoul3\nn\nLikelihood Contribution\n\n\n\n\nH\nH\nH\n3\n\\(p_H^3\\)\n\n\nH\nH\nV\n2\n\\(p_H^2(1 - p_H)\\)\n\n\nH\nV\nH\n3\n\\(p_H^2(1 - p_H)\\)\n\n\nH\nV\nV\n7\nA\n\n\nV\nH\nH\n7\nB\n\n\nV\nH\nV\n1\n\\(p_H(1 - p_H)^2\\)\n\n\nV\nV\nH\n5\n\\(p_H(1 - p_H)^2\\)\n\n\nV\nV\nV\n2\n\\((1 - p_H)^3\\)\n\n\n\n. . .\nFill in A and B."
  },
  {
    "objectID": "slides/02_likelihoods_ch2_o.html#model-1-likelihood-function",
    "href": "slides/02_likelihoods_ch2_o.html#model-1-likelihood-function",
    "title": "Beyond Least Squares: Using Likelihoods",
    "section": "Model 1: Likelihood function",
    "text": "Model 1: Likelihood function\nBecause the observations (the games) are independent, the likelihood is\n\\[Lik(p_H) = \\prod_{i=1}^{n}p_H^{y_i}(1 - p_H)^{3 - y_i}\\]\nWe will use this function to find the maximum likelihood estimate (MLE). The MLE is the value between 0 and 1 where we are most likely to see the observed data."
  },
  {
    "objectID": "slides/02_likelihoods_ch2_o.html#visualizing-the-likelihood",
    "href": "slides/02_likelihoods_ch2_o.html#visualizing-the-likelihood",
    "title": "Beyond Least Squares: Using Likelihoods",
    "section": "Visualizing the likelihood",
    "text": "Visualizing the likelihood\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\np &lt;- seq(0,1, length.out = 100) #sequence of 100 values between 0 and 100\nlik &lt;- p^46 *(1 -p)^44\n\nx &lt;- tibble(p = p, lik = lik)\nggplot(data = x, aes(x = p, y = lik)) + \n  geom_point() + \n  geom_line() +\n  labs(y = \"Likelihood\",\n       title = \"Likelihood of p_H\")"
  },
  {
    "objectID": "slides/02_likelihoods_ch2_o.html#q-what-is-your-best-guess-for-the-mle-hatp_h",
    "href": "slides/02_likelihoods_ch2_o.html#q-what-is-your-best-guess-for-the-mle-hatp_h",
    "title": "Beyond Least Squares: Using Likelihoods",
    "section": "Q: What is your best guess for the MLE, \\(\\hat{p}_H\\)?",
    "text": "Q: What is your best guess for the MLE, \\(\\hat{p}_H\\)?\nA. 0.489\nB. 0.500\nC. 0.511\nD. 0.556"
  },
  {
    "objectID": "slides/02_likelihoods_ch2_o.html#finding-the-maximum-likelihood-estimate",
    "href": "slides/02_likelihoods_ch2_o.html#finding-the-maximum-likelihood-estimate",
    "title": "Beyond Least Squares: Using Likelihoods",
    "section": "Finding the maximum likelihood estimate",
    "text": "Finding the maximum likelihood estimate\nThere are three primary ways to find the MLE\n. . .\n✅ Approximate using a graph\n. . .\n✅ Numerical approximation\n. . .\n✅ Using calculus"
  },
  {
    "objectID": "slides/02_likelihoods_ch2_o.html#approximate-mle-from-a-graph",
    "href": "slides/02_likelihoods_ch2_o.html#approximate-mle-from-a-graph",
    "title": "Beyond Least Squares: Using Likelihoods",
    "section": "Approximate MLE from a graph",
    "text": "Approximate MLE from a graph"
  },
  {
    "objectID": "slides/02_likelihoods_ch2_o.html#find-the-mle-using-numerical-approximation",
    "href": "slides/02_likelihoods_ch2_o.html#find-the-mle-using-numerical-approximation",
    "title": "Beyond Least Squares: Using Likelihoods",
    "section": "Find the MLE using numerical approximation",
    "text": "Find the MLE using numerical approximation\nSpecify a finite set of possible values the for \\(p_H\\) and calculate the likelihood for each value\n\n# write an R function for the likelihood\nref_lik &lt;- function(ph) {\n  ph^46 *(1 - ph)^44\n}\n\n\n# use the optimize function to find the MLE\noptimize(ref_lik, interval = c(0,1), maximum = TRUE)\n\n$maximum\n[1] 0.5111132\n\n$objective\n[1] 8.25947e-28"
  },
  {
    "objectID": "slides/02_likelihoods_ch2_o.html#find-mle-using-calculus",
    "href": "slides/02_likelihoods_ch2_o.html#find-mle-using-calculus",
    "title": "Beyond Least Squares: Using Likelihoods",
    "section": "Find MLE using calculus",
    "text": "Find MLE using calculus\n\nFind the MLE by taking the first derivative of the likelihood function.\nThis can be tricky because of the Product Rule, so we can maximize the log(Likelihood) instead. The same value maximizes the likelihood and log(Likelihood)\n\n. . .\n\n\n\n\n\n\n\n\n\n. . .\nSince calculus is not a pre-req, we will forgo this quest."
  },
  {
    "objectID": "slides/02_likelihoods_ch2_o.html#model-2-conditional-model",
    "href": "slides/02_likelihoods_ch2_o.html#model-2-conditional-model",
    "title": "Beyond Least Squares: Using Likelihoods",
    "section": "Model 2: Conditional model",
    "text": "Model 2: Conditional model\n\nIs there a tendency for the referees to call more fouls on the visiting team or home team?\nIs there a tendency for referees to call a foul on the team that already has more fouls?"
  },
  {
    "objectID": "slides/02_likelihoods_ch2_o.html#model-2-likelihood-contributions",
    "href": "slides/02_likelihoods_ch2_o.html#model-2-likelihood-contributions",
    "title": "Beyond Least Squares: Using Likelihoods",
    "section": "Model 2: Likelihood contributions",
    "text": "Model 2: Likelihood contributions\n\nNow let’s assume fouls are not independent within each game. We will specify this dependence using conditional probabilities.\n\nConditional probability: \\(P(A|B) =\\) Probability of \\(A\\) given \\(B\\) has occurred\n\n\n. . .\nDefine new parameters:\n\n\\(p_{H|N}\\): Probability referees call foul on home team given there are equal numbers of fouls on the home and visiting teams\n\\(p_{H|H Bias}\\): Probability referees call foul on home team given there are more prior fouls on the home team\n\\(p_{H|V Bias}\\): Probability referees call foul on home team given there are more prior fouls on the visiting team"
  },
  {
    "objectID": "slides/02_likelihoods_ch2_o.html#model-2-likelihood-contributions-1",
    "href": "slides/02_likelihoods_ch2_o.html#model-2-likelihood-contributions-1",
    "title": "Beyond Least Squares: Using Likelihoods",
    "section": "Model 2: Likelihood contributions",
    "text": "Model 2: Likelihood contributions\n\n\n\n\n\n\n\n\n\n\nFoul1\nFoul2\nFoul3\nn\nLikelihood Contribution\n\n\n\n\nH\nH\nH\n3\n\\((p_{H\\vert N})(p_{H\\vert H Bias})(p_{H\\vert H Bias}) = (p_{H\\vert N})(p_{H\\vert H Bias})^2\\)\n\n\nH\nH\nV\n2\n\\((p_{H\\vert N})(p_{H\\vert H Bias})(1 - p_{H\\vert H Bias})\\)\n\n\nH\nV\nH\n3\n\\((p_{H\\vert N})(1 - p_{H\\vert H Bias})(p_{H\\vert N}) = (p_{H\\vert N})^2(1 - p_{H\\vert H Bias})\\)\n\n\nH\nV\nV\n7\nA\n\n\nV\nH\nH\n7\nB\n\n\nV\nH\nV\n1\n\\((1 - p_{H\\vert N})(p_{H\\vert V Bias})(1 - p_{H\\vert N}) = (1 - p_{H\\vert N})^2(p_{H\\vert V Bias})\\)\n\n\nV\nV\nH\n5\n\\((1 - p_{H\\vert N})(1-p_{H\\vert V Bias})(p_{H\\vert V Bias})\\)\n\n\nV\nV\nV\n2\n\\(\\begin{aligned}&(1 - p_{H\\vert N})(1-p_{H\\vert V Bias})(1-p_{H\\vert V Bias})\\\\ &=(1 - p_{H\\vert N})(1-p_{H\\vert V Bias})^2\\end{aligned}\\)\n\n\n\nFill in A and B"
  },
  {
    "objectID": "slides/02_likelihoods_ch2_o.html#likelihood-function",
    "href": "slides/02_likelihoods_ch2_o.html#likelihood-function",
    "title": "Beyond Least Squares: Using Likelihoods",
    "section": "Likelihood function",
    "text": "Likelihood function\n\\[\\begin{aligned}Lik(p_{H| N}, p_{H|H Bias}, p_{H |V Bias}) &= [(p_{H| N})^{25}(1 - p_{H|N})^{23}(p_{H| H Bias})^8 \\\\ &(1 - p_{H| H Bias})^{12}(p_{H| V Bias})^{13}(1-p_{H|V Bias})^9]\\end{aligned}\\]\n(Note: The exponents sum to 90, the total number of fouls in the data)\n. . .\n\\[\\begin{aligned}\\log (Lik(p_{H| N}, p_{H|H Bias}, p_{H |V Bias})) &= 25 \\log(p_{H| N}) + 23 \\log(1 - p_{H|N}) \\\\ & + 8 \\log(p_{H| H Bias}) + 12 \\log(1 - p_{H| H Bias})\\\\ &+ 13 \\log(p_{H| V Bias}) + 9 \\log(1-p_{H|V Bias})\\end{aligned}\\]"
  },
  {
    "objectID": "slides/02_likelihoods_ch2_o.html#q-if-fouls-within-a-game-are-independent-how-would-you-expect-hatp_h-hatp_hvert-h-bias-and-hatp_hvert-v-bias-to-compare",
    "href": "slides/02_likelihoods_ch2_o.html#q-if-fouls-within-a-game-are-independent-how-would-you-expect-hatp_h-hatp_hvert-h-bias-and-hatp_hvert-v-bias-to-compare",
    "title": "Beyond Least Squares: Using Likelihoods",
    "section": "Q: If fouls within a game are independent, how would you expect \\(\\hat{p}_H\\), \\(\\hat{p}_{H\\vert H Bias}\\) and \\(\\hat{p}_{H\\vert V Bias}\\) to compare?",
    "text": "Q: If fouls within a game are independent, how would you expect \\(\\hat{p}_H\\), \\(\\hat{p}_{H\\vert H Bias}\\) and \\(\\hat{p}_{H\\vert V Bias}\\) to compare?\n\n\\(\\hat{p}_H\\) is greater than \\(\\hat{p}_{H\\vert H Bias}\\) and \\(\\hat{p}_{H \\vert V Bias}\\)\n\\(\\hat{p}_{H\\vert H Bias}\\) is greater than \\(\\hat{p}_H\\) and \\(\\hat{p}_{H \\vert V Bias}\\)\n\\(\\hat{p}_{H\\vert V Bias}\\) is greater than \\(\\hat{p}_H\\) and \\(\\hat{p}_{H \\vert V Bias}\\)\nThey are all approximately equal."
  },
  {
    "objectID": "slides/02_likelihoods_ch2_o.html#q-if-there-is-a-tendency-for-referees-to-call-a-foul-on-the-team-that-already-has-more-fouls-how-would-you-expect-hatp_h-and-hatp_hvert-h-bias-to-compare",
    "href": "slides/02_likelihoods_ch2_o.html#q-if-there-is-a-tendency-for-referees-to-call-a-foul-on-the-team-that-already-has-more-fouls-how-would-you-expect-hatp_h-and-hatp_hvert-h-bias-to-compare",
    "title": "Beyond Least Squares: Using Likelihoods",
    "section": "Q: If there is a tendency for referees to call a foul on the team that already has more fouls, how would you expect \\(\\hat{p}_H\\) and \\(\\hat{p}_{H\\vert H Bias}\\) to compare?",
    "text": "Q: If there is a tendency for referees to call a foul on the team that already has more fouls, how would you expect \\(\\hat{p}_H\\) and \\(\\hat{p}_{H\\vert H Bias}\\) to compare?\n\n\\(\\hat{p}_H\\) is greater than \\(\\hat{p}_{H\\vert H Bias}\\)\n\\(\\hat{p}_{H\\vert H Bias}\\) is greater than \\(\\hat{p}_H\\)\nThey are approximately equal."
  },
  {
    "objectID": "slides/02_likelihoods_ch2_o.html#likelihoods",
    "href": "slides/02_likelihoods_ch2_o.html#likelihoods",
    "title": "Beyond Least Squares: Using Likelihoods",
    "section": "Likelihoods",
    "text": "Likelihoods\nModel 1 (Unconditional Model)\n\n\\(p_H\\): probability of a foul being called on the home team\n\n. . .\nModel 2 (Conditional Model)\n\n\\(p_{H|N}\\): Probability referees call foul on home team given there are equal numbers of fouls on the home and visiting teams\n\\(p_{H|H Bias}\\): Probability referees call foul on home team given there are more prior fouls on the home team\n\\(p_{H|V Bias}\\): Probability referees call foul on home team given there are more prior fouls on the visiting team"
  },
  {
    "objectID": "slides/02_likelihoods_ch2_o.html#likelihoods-1",
    "href": "slides/02_likelihoods_ch2_o.html#likelihoods-1",
    "title": "Beyond Least Squares: Using Likelihoods",
    "section": "Likelihoods",
    "text": "Likelihoods\nModel 1 (Unconditional Model)\n\\[Lik(p_H) = p_H^{46}(1 - p_H)^{44}\\]\n. . .\nModel 2 (Conditional Model)\n\\[\\begin{aligned}Lik(p_{H| N}, p_{H|H Bias}, p_{H |V Bias}) &= [(p_{H| N})^{25}(1 - p_{H|N})^{23}(p_{H| H Bias})^8 \\\\ &(1 - p_{H| H Bias})^{12}(p_{H| V Bias})^{13}(1-p_{H|V Bias})^9]\\end{aligned}\\]"
  },
  {
    "objectID": "slides/02_likelihoods_ch2_o.html#maximum-likelihood-estimates",
    "href": "slides/02_likelihoods_ch2_o.html#maximum-likelihood-estimates",
    "title": "Beyond Least Squares: Using Likelihoods",
    "section": "Maximum likelihood estimates",
    "text": "Maximum likelihood estimates\nThe maximum likelihood estimate (MLE) is the value between 0 and 1 where we are most likely to see the observed data.\n. . .\n\n\nModel 1 (Unconditional Model)\n\n\\(\\hat{p}_H = 46/90 = 0.511\\)\n\nModel 2 (Conditional Model)\n\n\\(\\hat{p}_{H|N} = 25 / 48 = 0.521\\)\n\\(\\hat{p}_{H|H Bias} = 8 /20 = 0.4\\)\n\\(\\hat{p}_{H|V Bias} = 13/ 22 = 0.591\\)\n\n\n\nWhat is the probability the referees call a foul on the home team, assuming foul calls within a game are independent?\nIs there a tendency for the referees to call more fouls on the visiting team or home team?\nIs there a tendency for referees to call a foul on the team that already has more fouls?"
  },
  {
    "objectID": "slides/02_likelihoods_ch2_o.html#finding-the-mles-for-model-2",
    "href": "slides/02_likelihoods_ch2_o.html#finding-the-mles-for-model-2",
    "title": "Beyond Least Squares: Using Likelihoods",
    "section": "Finding the MLEs for model 2",
    "text": "Finding the MLEs for model 2\nThe likelihood is\n\\[\\begin{aligned}Lik(p_{H| N}, p_{H|H Bias}, p_{H |V Bias}) &= [(p_{H| N})^{25}(1 - p_{H|N})^{23}(p_{H| H Bias})^8 \\\\ &(1 - p_{H| H Bias})^{12}(p_{H| V Bias})^{13}(1-p_{H|V Bias})^9]\\end{aligned}\\]\n. . .\nThe log-likelihood is\n\\[\\begin{aligned}\\log (Lik(p_{H| N}, p_{H|H Bias}, p_{H |V Bias})) &= 25 \\log(p_{H| N}) + 23 \\log(1 - p_{H|N}) \\\\ & + 8 \\log(p_{H| H Bias}) + 12 \\log(1 - p_{H| H Bias})\\\\ &+ 13 \\log(p_{H| V Bias}) + 9 \\log(1-p_{H|V Bias})\\end{aligned}\\]\n. . .\nWe would like to find the MLEs for \\(p_{H| N}, p_{H|H Bias}, \\text{ and }p_{H |V Bias}\\)."
  },
  {
    "objectID": "slides/02_likelihoods_ch2_o.html#finding-mles-using-graphs",
    "href": "slides/02_likelihoods_ch2_o.html#finding-mles-using-graphs",
    "title": "Beyond Least Squares: Using Likelihoods",
    "section": "Finding MLEs using graphs",
    "text": "Finding MLEs using graphs\n\nWe need to find the MLEs for three parameters, therefore we would need to visualize a 4-dimensional object to find the MLEs from a graph. Given the difficulty of this task and the lack of precision in the estimates from this approach, we should rely on other approaches to find the MLEs in this instance.\n\n. . .\n\nWe also can’t use calculus… that leaves only 1 approach…. optimization via grid search or optim in R"
  },
  {
    "objectID": "slides/02_likelihoods_ch2_o.html#finding-the-mles-using-r-13",
    "href": "slides/02_likelihoods_ch2_o.html#finding-the-mles-using-r-13",
    "title": "Beyond Least Squares: Using Likelihoods",
    "section": "Finding the MLEs using R (1/3)",
    "text": "Finding the MLEs using R (1/3)\nWe can write a function and do a grid search to find the values that maximize the log-likelihood.\n. . .\n\nmaxloglik&lt;- function(nvals){\n  #nvals specifies the number of values\n  phn &lt;- seq(0, 1, length = nvals)\n  phh &lt;- seq(0, 1, length = nvals)\n  phv &lt;- seq(0, 1, length = nvals)\n  \n  loglik &lt;- expand.grid(phn, phh, phv) \n  colnames(loglik) &lt;- c(\"phn\", \"phh\", \"phv\")\n  \n  loglik &lt;- loglik %&gt;%\n    mutate(loglik  = log(phn^25 * (1 - phn)^23 * phh^8 * (1 - phh)^12 * \n                           phv^13 * (1 - phv)^9))\n  \n  loglik %&gt;%\n    arrange(desc(loglik)) %&gt;%\n    slice(1)\n}\n\n\nmaxloglik(100)\n\n        phn       phh       phv    loglik\n1 0.5252525 0.4040404 0.5858586 -61.57691"
  },
  {
    "objectID": "slides/02_likelihoods_ch2_o.html#finding-the-mles-using-r-23",
    "href": "slides/02_likelihoods_ch2_o.html#finding-the-mles-using-r-23",
    "title": "Beyond Least Squares: Using Likelihoods",
    "section": "Finding the MLEs using R (2/3)",
    "text": "Finding the MLEs using R (2/3)\n\nDepending on the number of parameters, it may be hard to conduct a granular enough search to find the exact values of the MLEs.\nTherefore, one could use the function above to conduct a crude search to find starting values for R’s optim function.\nThe function optim differs from optimize in that it can optimize over multiple parameter values (The optimize function can only optimize over a single parameter value).\n\n. . .\n\n# Function to calculate log-likelihood that will be used in the optim function\nloglik &lt;- function(params){\n  phn &lt;- params[1]\n  phh &lt;- params[2]\n  phv &lt;- params[3]\n\n  log(phn^25 * (1 - phn)^23 * phh^8 * (1 - phh)^12 * \n                           phv^13 * (1 - phv)^9)\n}"
  },
  {
    "objectID": "slides/02_likelihoods_ch2_o.html#finding-the-mles-using-r-33",
    "href": "slides/02_likelihoods_ch2_o.html#finding-the-mles-using-r-33",
    "title": "Beyond Least Squares: Using Likelihoods",
    "section": "Finding the MLEs using R (3/3)",
    "text": "Finding the MLEs using R (3/3)\n\n# use manual search to get starting values \nstart_vals &lt;- maxloglik(50) %&gt;% select(-loglik)\n\n\n# Use optim function in R to find the values to maximize the log-likelihood\n#set fnscale = -1 to maximize (the default is minimize)\noptim(par = start_vals, fn = loglik, control=list(fnscale=-1))\n\n$par\n      phn       phh       phv \n0.5208272 0.4000361 0.5909793 \n\n$value\n[1] -61.57319\n\n$counts\nfunction gradient \n      66       NA \n\n$convergence\n[1] 0\n\n$message\nNULL"
  },
  {
    "objectID": "slides/02_likelihoods_ch2_o.html#model-comparisons-1",
    "href": "slides/02_likelihoods_ch2_o.html#model-comparisons-1",
    "title": "Beyond Least Squares: Using Likelihoods",
    "section": "Model comparisons",
    "text": "Model comparisons\n\nNested models\nNon-nested models"
  },
  {
    "objectID": "slides/02_likelihoods_ch2_o.html#nested-models-12",
    "href": "slides/02_likelihoods_ch2_o.html#nested-models-12",
    "title": "Beyond Least Squares: Using Likelihoods",
    "section": "Nested Models (1/2)",
    "text": "Nested Models (1/2)\nNested models: Models such that the parameters of the reduced model are a subset of the parameters for a larger model\nExample:\n\\[\\begin{aligned}&\\text{Model A: }y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\epsilon\\\\\n&\\text{Model B: }y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3 + \\beta_4 x_4 + \\epsilon\\end{aligned}\\]\n. . .\nModel A is nested in Model B. We could use likelihoods to test whether it is useful to add \\(x_3\\) and \\(x_4\\) to the model.\n. . .\n\\[\\begin{aligned}&H_0: \\beta_3 = \\beta_4 = 0 \\\\\n&H_a: \\text{ at least one }\\beta_j \\text{ is not equal to 0}\\end{aligned}\\]"
  },
  {
    "objectID": "slides/02_likelihoods_ch2_o.html#nested-models-22",
    "href": "slides/02_likelihoods_ch2_o.html#nested-models-22",
    "title": "Beyond Least Squares: Using Likelihoods",
    "section": "Nested models (2/2)",
    "text": "Nested models (2/2)\nAnother way to think about nested models: Parameters in larger model can be equated to get the simpler model or if some parameters can be set to constants\nExample:\n\\[\\begin{aligned}&\\text{Model 1: }p_H \\\\\n&\\text{Model 2: }p_{H| N}, p_{H| H Bias}, p_{H| V Bias}\\end{aligned}\\]\n. . .\nModel 1 is nested in Model 2. The parameters \\(p_{H| N}\\), \\(p_{H|H Bias}\\), and \\(p_{H |V Bias}\\) can be set equal to \\(p_H\\) to get Model 1.\n. . .\n\\[\\begin{aligned}&H_0: p_{H| N} = p_{H| H Bias} = p_{H| V Bias} = p_H \\\\\n&H_a: \\text{At least one of }p_{H| N}, p_{H| H Bias}, p_{H| V Bias} \\text{ differs from the others}\\end{aligned}\\]"
  },
  {
    "objectID": "slides/02_likelihoods_ch2_o.html#steps-to-compare-models",
    "href": "slides/02_likelihoods_ch2_o.html#steps-to-compare-models",
    "title": "Beyond Least Squares: Using Likelihoods",
    "section": "Steps to compare models",
    "text": "Steps to compare models\n1️⃣ Find the MLEs for each model.\n2️⃣ Plug the MLEs into the log-likelihood function for each model to get the maximum value of the log-likelihood for each model.\n3️⃣ Find the difference in the maximum log-likelihoods\n4️⃣ Use the Likelihood Ratio Test to determine if the difference is statistically significant"
  },
  {
    "objectID": "slides/02_likelihoods_ch2_o.html#steps-1---2",
    "href": "slides/02_likelihoods_ch2_o.html#steps-1---2",
    "title": "Beyond Least Squares: Using Likelihoods",
    "section": "Steps 1 - 2",
    "text": "Steps 1 - 2\nFind the MLEs for each model and plug them into the log-likelihood functions.\n\n\nModel 1:\n\n\\(\\hat{p}_H = 46/90 = 0.511\\)\n\n\nloglik1 &lt;- function(ph){\n log(ph^46 * (1 - ph)^44)\n}\nloglik1(46/90)\n\n[1] -62.36102\n\n\n. . .\nModel 2\n\n\\(\\hat{p}_{H|N} = 25 / 48 = 0.521\\)\n\\(\\hat{p}_{H|H Bias} = 8 /20 = 0.4\\)\n\\(\\hat{p}_{H|V Bias} = 13/ 22 = 0.591\\)\n\n. . .\n\nloglik2 &lt;- function(phn, phh, phv) {\n  log(phn^25 * (1 - phn)^23 * phh^8 * \n        (1 - phh)^12 * phv^13 * (1 - phv)^9)\n}\nloglik2(25/48, 8/20, 13/22)\n\n[1] -61.57319"
  },
  {
    "objectID": "slides/02_likelihoods_ch2_o.html#step-3",
    "href": "slides/02_likelihoods_ch2_o.html#step-3",
    "title": "Beyond Least Squares: Using Likelihoods",
    "section": "Step 3",
    "text": "Step 3\nFind the difference in the log-likelihoods\n\n(diff &lt;- loglik2(25/48, 8/20, 13/22) - loglik1(46/90))\n\n[1] 0.7878318\n\n\n\n. . .\nIs the difference in the maximum log-likelihoods statistically significant?"
  },
  {
    "objectID": "slides/02_likelihoods_ch2_o.html#likelihood-ratio-test",
    "href": "slides/02_likelihoods_ch2_o.html#likelihood-ratio-test",
    "title": "Beyond Least Squares: Using Likelihoods",
    "section": "Likelihood Ratio Test",
    "text": "Likelihood Ratio Test\nTest statistic\n\\[\\begin{aligned} LRT &= 2[\\max\\{\\log(Lik(\\text{larger model}))\\} - \\max\\{\\log(Lik(\\text{reduced model}))\\}]\\\\[10pt]\n&= 2\\log\\Bigg(\\frac{\\max\\{(Lik(\\text{larger model})\\}}{\\max\\{(Lik(\\text{reduced model})\\}}\\Bigg)\\end{aligned}\\]\n\n. . .\nLRT follows a \\(\\chi^2\\) distribution where the degrees of freedom equal the difference in the number of parameters between the two models"
  },
  {
    "objectID": "slides/02_likelihoods_ch2_o.html#step-4",
    "href": "slides/02_likelihoods_ch2_o.html#step-4",
    "title": "Beyond Least Squares: Using Likelihoods",
    "section": "Step 4",
    "text": "Step 4\n\n(LRT &lt;- 2 * (loglik2(25/48, 8/20, 13/22) - loglik1(46/90)))\n\n[1] 1.575664\n\n\n. . .\nThe test statistic follows a \\(\\chi^2\\) distribution with 2 degrees of freedom. Therefore, the p-value is \\(P(\\chi^2 &gt; LRT)\\).\n\npchisq(LRT, 2, lower.tail = FALSE)\n\n[1] 0.4548299\n\n\n. . .\nThe p-value is very large, so we fail to reject \\(H_0\\). We do not have convincing evidence that the conditional model is an improvement over the unconditional model. Therefore, we can stick with the unconditional model."
  },
  {
    "objectID": "slides/02_likelihoods_ch2_o.html#comparing-non-nested-models",
    "href": "slides/02_likelihoods_ch2_o.html#comparing-non-nested-models",
    "title": "Beyond Least Squares: Using Likelihoods",
    "section": "Comparing non-nested models",
    "text": "Comparing non-nested models\n\n\nAIC = -2(max log-likelihood) + 2p\n\n(Model1_AIC &lt;- 2 * loglik1(46/90) + 2 * 1)\n\n[1] -122.722\n\n(Model2_AIC &lt;-2 * loglik2(25/48, 8/20, 13/22) + 2 * 3)\n\n[1] -117.1464\n\n\n. . .\nBIC = -2(max log-likelihood) + plog(n)\n\n(Model1_BIC &lt;- 2 * loglik1(46/90) + 1 * log(30))\n\n[1] -121.3208\n\n(Model2_BIC &lt;-2 * loglik2(25/48, 8/20, 13/22) + 3 * log(30))\n\n[1] -112.9428\n\n\nChoose Model 1, the unconditional model, based on AIC and BIC"
  },
  {
    "objectID": "slides/02_likelihoods_ch2_o.html#looking-ahead",
    "href": "slides/02_likelihoods_ch2_o.html#looking-ahead",
    "title": "Beyond Least Squares: Using Likelihoods",
    "section": "Looking ahead",
    "text": "Looking ahead\n\nLikelihoods help us answer the question of how likely we are to observe the data given different parameters\nIn this example, we did not consider covariates, so in practice the parameters we want to estimate will look more similar to this\n\n. . .\n\\[p_H = \\frac{e^{\\beta_0 + \\beta_1x_1 + \\dots + \\beta_px_p}}{1 + e^{\\beta_0 + \\beta_1x_1 + \\dots + \\beta_px_p}}\\]\n. . .\n\nFinding the MLE becomes much more complex and numerical methods may be required.\n\nWe will primarily rely on software to find the MLE, but the conceptual ideas will be the same"
  },
  {
    "objectID": "slides/02_likelihoods_ch2_o.html#acknowledgements",
    "href": "slides/02_likelihoods_ch2_o.html#acknowledgements",
    "title": "Beyond Least Squares: Using Likelihoods",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThese slides are based on content in BMLR: Chapter 1 - Review of Multiple Linear Regression\nInitial versions of the slides are by Dr. Maria Tackett, Duke University"
  },
  {
    "objectID": "slides/02_likelihoods_ch2.html#setup",
    "href": "slides/02_likelihoods_ch2.html#setup",
    "title": "Beyond Least Squares: Using Likelihoods",
    "section": "Setup",
    "text": "Setup\n\nlibrary(tidyverse)\nlibrary(GGally)\nlibrary(knitr)\nlibrary(patchwork)\nlibrary(viridis)\nlibrary(ggfortify)"
  },
  {
    "objectID": "slides/02_likelihoods_ch2.html#learning-goals",
    "href": "slides/02_likelihoods_ch2.html#learning-goals",
    "title": "Beyond Least Squares: Using Likelihoods",
    "section": "Learning goals",
    "text": "Learning goals\n\nDescribe the concept of a likelihood\nConstruct the likelihood for a simple model\nDefine the Maximum Likelihood Estimate (MLE) and use it to answer an analysis question\nIdentify three ways to calculate or approximate the MLE and apply these methods to find the MLE for a simple model\nUse likelihoods to compare models (next week)"
  },
  {
    "objectID": "slides/02_likelihoods_ch2.html#what-is-the-likelihood",
    "href": "slides/02_likelihoods_ch2.html#what-is-the-likelihood",
    "title": "Beyond Least Squares: Using Likelihoods",
    "section": "What is the likelihood?",
    "text": "What is the likelihood?\nA likelihood is a function that tells us how likely we are to observe our data for a given parameter value (or values).\n\nUnlike Ordinary Least Squares (OLS), they do not require the responses be independent, identically distributed, and normal (iidN)\nThey are not the same as probability functions\n\nProbability function: Fixed parameter value(s) + input possible outcomes \\(\\Rightarrow\\) probability of seeing the different outcomes given the parameter value(s)\nLikelihood: Fixed data + input possible parameter values \\(\\Rightarrow\\) probability of seeing the fixed data for each parameter value"
  },
  {
    "objectID": "slides/02_likelihoods_ch2.html#fouls-in-college-basketball-games",
    "href": "slides/02_likelihoods_ch2.html#fouls-in-college-basketball-games",
    "title": "Beyond Least Squares: Using Likelihoods",
    "section": "Fouls in college basketball games",
    "text": "Fouls in college basketball games\nThe data set 04-refs.csv includes 30 randomly selected NCAA men’s basketball games played in the 2009 - 2010 season.\nWe will focus on the variables foul1, foul2, and foul3, which indicate which team had a foul called them for the 1st, 2nd, and 3rd fouls, respectively.\n\nH: Foul was called on the home team\nV: Foul was called on the visiting team\n\n\nWe are focusing on the first three fouls for this analysis, but this could easily be extended to include all fouls in a game.\n\n[The dataset was derived from basektball0910.csv used in BMLR Section 11.2"
  },
  {
    "objectID": "slides/02_likelihoods_ch2.html#fouls-in-college-basketball-games-1",
    "href": "slides/02_likelihoods_ch2.html#fouls-in-college-basketball-games-1",
    "title": "Beyond Least Squares: Using Likelihoods",
    "section": "Fouls in college basketball games",
    "text": "Fouls in college basketball games\n\nrefs &lt;- read_csv(\"data/04-refs.csv\")\nrefs %&gt;% slice(1:5) %&gt;% kable()\n\n\n\n\ngame\ndate\nvisitor\nhometeam\nfoul1\nfoul2\nfoul3\n\n\n\n\n166\n20100126\nCLEM\nBC\nV\nV\nV\n\n\n224\n20100224\nDEPAUL\nCIN\nH\nH\nV\n\n\n317\n20100109\nMARQET\nNOVA\nH\nH\nH\n\n\n214\n20100228\nMARQET\nSETON\nV\nV\nH\n\n\n278\n20100128\nSETON\nSFL\nH\nV\nV\n\n\n\n\n\nWe will treat the games as independent in this analysis."
  },
  {
    "objectID": "slides/02_likelihoods_ch2.html#different-likelihood-models",
    "href": "slides/02_likelihoods_ch2.html#different-likelihood-models",
    "title": "Beyond Least Squares: Using Likelihoods",
    "section": "Different likelihood models",
    "text": "Different likelihood models\nModel 1 (Unconditional Model): What is the probability the referees call a foul on the home team, assuming foul calls within a game are independent?\n\nModel 2 (Conditional Model):\n\nIs there a tendency for the referees to call more fouls on the visiting team or home team?\nIs there a tendency for referees to call a foul on the team that already has more fouls?\n\n\nUltimately we want to decide which model is better."
  },
  {
    "objectID": "slides/02_likelihoods_ch2.html#exploratory-data-analysis",
    "href": "slides/02_likelihoods_ch2.html#exploratory-data-analysis",
    "title": "Beyond Least Squares: Using Likelihoods",
    "section": "Exploratory data analysis",
    "text": "Exploratory data analysis\n\n\n\nrefs %&gt;%\ncount(foul1, foul2, foul3) %&gt;% kable()\n\n\n\n\nfoul1\nfoul2\nfoul3\nn\n\n\n\n\nH\nH\nH\n3\n\n\nH\nH\nV\n2\n\n\nH\nV\nH\n3\n\n\nH\nV\nV\n7\n\n\nV\nH\nH\n7\n\n\nV\nH\nV\n1\n\n\nV\nV\nH\n5\n\n\nV\nV\nV\n2\n\n\n\n\n\n\nThere are\n\n46 total fouls on the home team\n44 total fouls on the visiting team"
  },
  {
    "objectID": "slides/02_likelihoods_ch2.html#model-1-unconditional-model",
    "href": "slides/02_likelihoods_ch2.html#model-1-unconditional-model",
    "title": "Beyond Least Squares: Using Likelihoods",
    "section": "Model 1: Unconditional model",
    "text": "Model 1: Unconditional model\nWhat is the probability the referees call a foul on the home team, assuming foul calls within a game are independent?"
  },
  {
    "objectID": "slides/02_likelihoods_ch2.html#likelihood-1",
    "href": "slides/02_likelihoods_ch2.html#likelihood-1",
    "title": "Beyond Least Squares: Using Likelihoods",
    "section": "Likelihood",
    "text": "Likelihood\nLet \\(p_H\\) be the probability the referees call a foul on the home team.\nThe likelihood for a single observation\n\\[Lik(p_H) = p_H^{y_i}(1 - p_H)^{n_i - y_i}\\]\nWhere \\(y_i\\) is the number of fouls called on the home team.\n(In this example, we know \\(n_i = 3\\) for all observations.)\n\nExample\nFor a single game where the first three fouls are \\(H, H, V\\), then\n\\[Lik(p_H) = p_H^{2}(1 - p_H)^{3 - 2} = p_H^{2}(1 - p_H)\\]"
  },
  {
    "objectID": "slides/02_likelihoods_ch2.html#model-1-likelihood-contribution",
    "href": "slides/02_likelihoods_ch2.html#model-1-likelihood-contribution",
    "title": "Beyond Least Squares: Using Likelihoods",
    "section": "Model 1: Likelihood contribution",
    "text": "Model 1: Likelihood contribution\n\n\n\nFoul1\nFoul2\nFoul3\nn\nLikelihood Contribution\n\n\n\n\nH\nH\nH\n3\n\\(p_H^3\\)\n\n\nH\nH\nV\n2\n\\(p_H^2(1 - p_H)\\)\n\n\nH\nV\nH\n3\n\\(p_H^2(1 - p_H)\\)\n\n\nH\nV\nV\n7\nA\n\n\nV\nH\nH\n7\nB\n\n\nV\nH\nV\n1\n\\(p_H(1 - p_H)^2\\)\n\n\nV\nV\nH\n5\n\\(p_H(1 - p_H)^2\\)\n\n\nV\nV\nV\n2\n\\((1 - p_H)^3\\)\n\n\n\n\nFill in A and B."
  },
  {
    "objectID": "slides/02_likelihoods_ch2.html#model-1-likelihood-function",
    "href": "slides/02_likelihoods_ch2.html#model-1-likelihood-function",
    "title": "Beyond Least Squares: Using Likelihoods",
    "section": "Model 1: Likelihood function",
    "text": "Model 1: Likelihood function\nBecause the observations (the games) are independent, the likelihood is\n\\[Lik(p_H) = \\prod_{i=1}^{n}p_H^{y_i}(1 - p_H)^{3 - y_i}\\]\nWe will use this function to find the maximum likelihood estimate (MLE). The MLE is the value between 0 and 1 where we are most likely to see the observed data."
  },
  {
    "objectID": "slides/02_likelihoods_ch2.html#visualizing-the-likelihood",
    "href": "slides/02_likelihoods_ch2.html#visualizing-the-likelihood",
    "title": "Beyond Least Squares: Using Likelihoods",
    "section": "Visualizing the likelihood",
    "text": "Visualizing the likelihood\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\np &lt;- seq(0,1, length.out = 100) #sequence of 100 values between 0 and 100\nlik &lt;- p^46 *(1 -p)^44\n\nx &lt;- tibble(p = p, lik = lik)\nggplot(data = x, aes(x = p, y = lik)) + \n  geom_point() + \n  geom_line() +\n  labs(y = \"Likelihood\",\n       title = \"Likelihood of p_H\")"
  },
  {
    "objectID": "slides/02_likelihoods_ch2.html#q-what-is-your-best-guess-for-the-mle-hatp_h",
    "href": "slides/02_likelihoods_ch2.html#q-what-is-your-best-guess-for-the-mle-hatp_h",
    "title": "Beyond Least Squares: Using Likelihoods",
    "section": "Q: What is your best guess for the MLE, \\(\\hat{p}_H\\)?",
    "text": "Q: What is your best guess for the MLE, \\(\\hat{p}_H\\)?\nA. 0.489\nB. 0.500\nC. 0.511\nD. 0.556"
  },
  {
    "objectID": "slides/02_likelihoods_ch2.html#finding-the-maximum-likelihood-estimate",
    "href": "slides/02_likelihoods_ch2.html#finding-the-maximum-likelihood-estimate",
    "title": "Beyond Least Squares: Using Likelihoods",
    "section": "Finding the maximum likelihood estimate",
    "text": "Finding the maximum likelihood estimate\nThere are three primary ways to find the MLE\n\n✅ Approximate using a graph\n\n\n✅ Numerical approximation\n\n\n✅ Using calculus"
  },
  {
    "objectID": "slides/02_likelihoods_ch2.html#approximate-mle-from-a-graph",
    "href": "slides/02_likelihoods_ch2.html#approximate-mle-from-a-graph",
    "title": "Beyond Least Squares: Using Likelihoods",
    "section": "Approximate MLE from a graph",
    "text": "Approximate MLE from a graph"
  },
  {
    "objectID": "slides/02_likelihoods_ch2.html#find-the-mle-using-numerical-approximation",
    "href": "slides/02_likelihoods_ch2.html#find-the-mle-using-numerical-approximation",
    "title": "Beyond Least Squares: Using Likelihoods",
    "section": "Find the MLE using numerical approximation",
    "text": "Find the MLE using numerical approximation\nSpecify a finite set of possible values the for \\(p_H\\) and calculate the likelihood for each value\n\n# write an R function for the likelihood\nref_lik &lt;- function(ph) {\n  ph^46 *(1 - ph)^44\n}\n\n\n# use the optimize function to find the MLE\noptimize(ref_lik, interval = c(0,1), maximum = TRUE)\n\n$maximum\n[1] 0.5111132\n\n$objective\n[1] 8.25947e-28"
  },
  {
    "objectID": "slides/02_likelihoods_ch2.html#find-mle-using-calculus",
    "href": "slides/02_likelihoods_ch2.html#find-mle-using-calculus",
    "title": "Beyond Least Squares: Using Likelihoods",
    "section": "Find MLE using calculus",
    "text": "Find MLE using calculus\n\nFind the MLE by taking the first derivative of the likelihood function.\nThis can be tricky because of the Product Rule, so we can maximize the log(Likelihood) instead. The same value maximizes the likelihood and log(Likelihood)\n\n\n\n\n\n\n\n\n\n\n\n\n\nSince calculus is not a pre-req, we will forgo this quest."
  },
  {
    "objectID": "slides/02_likelihoods_ch2.html#model-2-conditional-model",
    "href": "slides/02_likelihoods_ch2.html#model-2-conditional-model",
    "title": "Beyond Least Squares: Using Likelihoods",
    "section": "Model 2: Conditional model",
    "text": "Model 2: Conditional model\n\nIs there a tendency for the referees to call more fouls on the visiting team or home team?\nIs there a tendency for referees to call a foul on the team that already has more fouls?"
  },
  {
    "objectID": "slides/02_likelihoods_ch2.html#model-2-likelihood-contributions",
    "href": "slides/02_likelihoods_ch2.html#model-2-likelihood-contributions",
    "title": "Beyond Least Squares: Using Likelihoods",
    "section": "Model 2: Likelihood contributions",
    "text": "Model 2: Likelihood contributions\n\nNow let’s assume fouls are not independent within each game. We will specify this dependence using conditional probabilities.\n\nConditional probability: \\(P(A|B) =\\) Probability of \\(A\\) given \\(B\\) has occurred\n\n\n\nDefine new parameters:\n\n\\(p_{H|N}\\): Probability referees call foul on home team given there are equal numbers of fouls on the home and visiting teams\n\\(p_{H|H Bias}\\): Probability referees call foul on home team given there are more prior fouls on the home team\n\\(p_{H|V Bias}\\): Probability referees call foul on home team given there are more prior fouls on the visiting team"
  },
  {
    "objectID": "slides/02_likelihoods_ch2.html#model-2-likelihood-contributions-1",
    "href": "slides/02_likelihoods_ch2.html#model-2-likelihood-contributions-1",
    "title": "Beyond Least Squares: Using Likelihoods",
    "section": "Model 2: Likelihood contributions",
    "text": "Model 2: Likelihood contributions\n\n\n\n\n\n\n\n\n\n\nFoul1\nFoul2\nFoul3\nn\nLikelihood Contribution\n\n\n\n\nH\nH\nH\n3\n\\((p_{H\\vert N})(p_{H\\vert H Bias})(p_{H\\vert H Bias}) = (p_{H\\vert N})(p_{H\\vert H Bias})^2\\)\n\n\nH\nH\nV\n2\n\\((p_{H\\vert N})(p_{H\\vert H Bias})(1 - p_{H\\vert H Bias})\\)\n\n\nH\nV\nH\n3\n\\((p_{H\\vert N})(1 - p_{H\\vert H Bias})(p_{H\\vert N}) = (p_{H\\vert N})^2(1 - p_{H\\vert H Bias})\\)\n\n\nH\nV\nV\n7\nA\n\n\nV\nH\nH\n7\nB\n\n\nV\nH\nV\n1\n\\((1 - p_{H\\vert N})(p_{H\\vert V Bias})(1 - p_{H\\vert N}) = (1 - p_{H\\vert N})^2(p_{H\\vert V Bias})\\)\n\n\nV\nV\nH\n5\n\\((1 - p_{H\\vert N})(1-p_{H\\vert V Bias})(p_{H\\vert V Bias})\\)\n\n\nV\nV\nV\n2\n\\(\\begin{aligned}&(1 - p_{H\\vert N})(1-p_{H\\vert V Bias})(1-p_{H\\vert V Bias})\\\\ &=(1 - p_{H\\vert N})(1-p_{H\\vert V Bias})^2\\end{aligned}\\)\n\n\n\nFill in A and B"
  },
  {
    "objectID": "slides/02_likelihoods_ch2.html#likelihood-function",
    "href": "slides/02_likelihoods_ch2.html#likelihood-function",
    "title": "Beyond Least Squares: Using Likelihoods",
    "section": "Likelihood function",
    "text": "Likelihood function\n\\[\\begin{aligned}Lik(p_{H| N}, p_{H|H Bias}, p_{H |V Bias}) &= [(p_{H| N})^{25}(1 - p_{H|N})^{23}(p_{H| H Bias})^8 \\\\ &(1 - p_{H| H Bias})^{12}(p_{H| V Bias})^{13}(1-p_{H|V Bias})^9]\\end{aligned}\\]\n(Note: The exponents sum to 90, the total number of fouls in the data)\n\n\\[\\begin{aligned}\\log (Lik(p_{H| N}, p_{H|H Bias}, p_{H |V Bias})) &= 25 \\log(p_{H| N}) + 23 \\log(1 - p_{H|N}) \\\\ & + 8 \\log(p_{H| H Bias}) + 12 \\log(1 - p_{H| H Bias})\\\\ &+ 13 \\log(p_{H| V Bias}) + 9 \\log(1-p_{H|V Bias})\\end{aligned}\\]"
  },
  {
    "objectID": "slides/02_likelihoods_ch2.html#q-if-fouls-within-a-game-are-independent-how-would-you-expect-hatp_h-hatp_hvert-h-bias-and-hatp_hvert-v-bias-to-compare",
    "href": "slides/02_likelihoods_ch2.html#q-if-fouls-within-a-game-are-independent-how-would-you-expect-hatp_h-hatp_hvert-h-bias-and-hatp_hvert-v-bias-to-compare",
    "title": "Beyond Least Squares: Using Likelihoods",
    "section": "Q: If fouls within a game are independent, how would you expect \\(\\hat{p}_H\\), \\(\\hat{p}_{H\\vert H Bias}\\) and \\(\\hat{p}_{H\\vert V Bias}\\) to compare?",
    "text": "Q: If fouls within a game are independent, how would you expect \\(\\hat{p}_H\\), \\(\\hat{p}_{H\\vert H Bias}\\) and \\(\\hat{p}_{H\\vert V Bias}\\) to compare?\n\n\\(\\hat{p}_H\\) is greater than \\(\\hat{p}_{H\\vert H Bias}\\) and \\(\\hat{p}_{H \\vert V Bias}\\)\n\\(\\hat{p}_{H\\vert H Bias}\\) is greater than \\(\\hat{p}_H\\) and \\(\\hat{p}_{H \\vert V Bias}\\)\n\\(\\hat{p}_{H\\vert V Bias}\\) is greater than \\(\\hat{p}_H\\) and \\(\\hat{p}_{H \\vert V Bias}\\)\nThey are all approximately equal."
  },
  {
    "objectID": "slides/02_likelihoods_ch2.html#q-if-there-is-a-tendency-for-referees-to-call-a-foul-on-the-team-that-already-has-more-fouls-how-would-you-expect-hatp_h-and-hatp_hvert-h-bias-to-compare",
    "href": "slides/02_likelihoods_ch2.html#q-if-there-is-a-tendency-for-referees-to-call-a-foul-on-the-team-that-already-has-more-fouls-how-would-you-expect-hatp_h-and-hatp_hvert-h-bias-to-compare",
    "title": "Beyond Least Squares: Using Likelihoods",
    "section": "Q: If there is a tendency for referees to call a foul on the team that already has more fouls, how would you expect \\(\\hat{p}_H\\) and \\(\\hat{p}_{H\\vert H Bias}\\) to compare?",
    "text": "Q: If there is a tendency for referees to call a foul on the team that already has more fouls, how would you expect \\(\\hat{p}_H\\) and \\(\\hat{p}_{H\\vert H Bias}\\) to compare?\n\n\\(\\hat{p}_H\\) is greater than \\(\\hat{p}_{H\\vert H Bias}\\)\n\\(\\hat{p}_{H\\vert H Bias}\\) is greater than \\(\\hat{p}_H\\)\nThey are approximately equal."
  },
  {
    "objectID": "slides/02_likelihoods_ch2.html#likelihoods",
    "href": "slides/02_likelihoods_ch2.html#likelihoods",
    "title": "Beyond Least Squares: Using Likelihoods",
    "section": "Likelihoods",
    "text": "Likelihoods\nModel 1 (Unconditional Model)\n\n\\(p_H\\): probability of a foul being called on the home team\n\n\nModel 2 (Conditional Model)\n\n\\(p_{H|N}\\): Probability referees call foul on home team given there are equal numbers of fouls on the home and visiting teams\n\\(p_{H|H Bias}\\): Probability referees call foul on home team given there are more prior fouls on the home team\n\\(p_{H|V Bias}\\): Probability referees call foul on home team given there are more prior fouls on the visiting team"
  },
  {
    "objectID": "slides/02_likelihoods_ch2.html#likelihoods-1",
    "href": "slides/02_likelihoods_ch2.html#likelihoods-1",
    "title": "Beyond Least Squares: Using Likelihoods",
    "section": "Likelihoods",
    "text": "Likelihoods\nModel 1 (Unconditional Model)\n\\[Lik(p_H) = p_H^{46}(1 - p_H)^{44}\\]\n\nModel 2 (Conditional Model)\n\\[\\begin{aligned}Lik(p_{H| N}, p_{H|H Bias}, p_{H |V Bias}) &= [(p_{H| N})^{25}(1 - p_{H|N})^{23}(p_{H| H Bias})^8 \\\\ &(1 - p_{H| H Bias})^{12}(p_{H| V Bias})^{13}(1-p_{H|V Bias})^9]\\end{aligned}\\]"
  },
  {
    "objectID": "slides/02_likelihoods_ch2.html#maximum-likelihood-estimates",
    "href": "slides/02_likelihoods_ch2.html#maximum-likelihood-estimates",
    "title": "Beyond Least Squares: Using Likelihoods",
    "section": "Maximum likelihood estimates",
    "text": "Maximum likelihood estimates\nThe maximum likelihood estimate (MLE) is the value between 0 and 1 where we are most likely to see the observed data.\n\n\n\nModel 1 (Unconditional Model)\n\n\\(\\hat{p}_H = 46/90 = 0.511\\)\n\nModel 2 (Conditional Model)\n\n\\(\\hat{p}_{H|N} = 25 / 48 = 0.521\\)\n\\(\\hat{p}_{H|H Bias} = 8 /20 = 0.4\\)\n\\(\\hat{p}_{H|V Bias} = 13/ 22 = 0.591\\)\n\n\n\nWhat is the probability the referees call a foul on the home team, assuming foul calls within a game are independent?\nIs there a tendency for the referees to call more fouls on the visiting team or home team?\nIs there a tendency for referees to call a foul on the team that already has more fouls?"
  },
  {
    "objectID": "slides/02_likelihoods_ch2.html#finding-the-mles-for-model-2",
    "href": "slides/02_likelihoods_ch2.html#finding-the-mles-for-model-2",
    "title": "Beyond Least Squares: Using Likelihoods",
    "section": "Finding the MLEs for model 2",
    "text": "Finding the MLEs for model 2\nThe likelihood is\n\\[\\begin{aligned}Lik(p_{H| N}, p_{H|H Bias}, p_{H |V Bias}) &= [(p_{H| N})^{25}(1 - p_{H|N})^{23}(p_{H| H Bias})^8 \\\\ &(1 - p_{H| H Bias})^{12}(p_{H| V Bias})^{13}(1-p_{H|V Bias})^9]\\end{aligned}\\]\n\nThe log-likelihood is\n\\[\\begin{aligned}\\log (Lik(p_{H| N}, p_{H|H Bias}, p_{H |V Bias})) &= 25 \\log(p_{H| N}) + 23 \\log(1 - p_{H|N}) \\\\ & + 8 \\log(p_{H| H Bias}) + 12 \\log(1 - p_{H| H Bias})\\\\ &+ 13 \\log(p_{H| V Bias}) + 9 \\log(1-p_{H|V Bias})\\end{aligned}\\]\n\n\nWe would like to find the MLEs for \\(p_{H| N}, p_{H|H Bias}, \\text{ and }p_{H |V Bias}\\)."
  },
  {
    "objectID": "slides/02_likelihoods_ch2.html#finding-mles-using-graphs",
    "href": "slides/02_likelihoods_ch2.html#finding-mles-using-graphs",
    "title": "Beyond Least Squares: Using Likelihoods",
    "section": "Finding MLEs using graphs",
    "text": "Finding MLEs using graphs\n\nWe need to find the MLEs for three parameters, therefore we would need to visualize a 4-dimensional object to find the MLEs from a graph. Given the difficulty of this task and the lack of precision in the estimates from this approach, we should rely on other approaches to find the MLEs in this instance.\n\n\n\nWe also can’t use calculus… that leaves only 1 approach…. optimization via grid search or optim in R"
  },
  {
    "objectID": "slides/02_likelihoods_ch2.html#finding-the-mles-using-r-13",
    "href": "slides/02_likelihoods_ch2.html#finding-the-mles-using-r-13",
    "title": "Beyond Least Squares: Using Likelihoods",
    "section": "Finding the MLEs using R (1/3)",
    "text": "Finding the MLEs using R (1/3)\nWe can write a function and do a grid search to find the values that maximize the log-likelihood.\n\n\nmaxloglik&lt;- function(nvals){\n  #nvals specifies the number of values\n  phn &lt;- seq(0, 1, length = nvals)\n  phh &lt;- seq(0, 1, length = nvals)\n  phv &lt;- seq(0, 1, length = nvals)\n  \n  loglik &lt;- expand.grid(phn, phh, phv) \n  colnames(loglik) &lt;- c(\"phn\", \"phh\", \"phv\")\n  \n  loglik &lt;- loglik %&gt;%\n    mutate(loglik  = log(phn^25 * (1 - phn)^23 * phh^8 * (1 - phh)^12 * \n                           phv^13 * (1 - phv)^9))\n  \n  loglik %&gt;%\n    arrange(desc(loglik)) %&gt;%\n    slice(1)\n}\n\n\nmaxloglik(100)\n\n        phn       phh       phv    loglik\n1 0.5252525 0.4040404 0.5858586 -61.57691"
  },
  {
    "objectID": "slides/02_likelihoods_ch2.html#finding-the-mles-using-r-23",
    "href": "slides/02_likelihoods_ch2.html#finding-the-mles-using-r-23",
    "title": "Beyond Least Squares: Using Likelihoods",
    "section": "Finding the MLEs using R (2/3)",
    "text": "Finding the MLEs using R (2/3)\n\nDepending on the number of parameters, it may be hard to conduct a granular enough search to find the exact values of the MLEs.\nTherefore, one could use the function above to conduct a crude search to find starting values for R’s optim function.\nThe function optim differs from optimize in that it can optimize over multiple parameter values (The optimize function can only optimize over a single parameter value).\n\n\n\n# Function to calculate log-likelihood that will be used in the optim function\nloglik &lt;- function(params){\n  phn &lt;- params[1]\n  phh &lt;- params[2]\n  phv &lt;- params[3]\n\n  log(phn^25 * (1 - phn)^23 * phh^8 * (1 - phh)^12 * \n                           phv^13 * (1 - phv)^9)\n}"
  },
  {
    "objectID": "slides/02_likelihoods_ch2.html#finding-the-mles-using-r-33",
    "href": "slides/02_likelihoods_ch2.html#finding-the-mles-using-r-33",
    "title": "Beyond Least Squares: Using Likelihoods",
    "section": "Finding the MLEs using R (3/3)",
    "text": "Finding the MLEs using R (3/3)\n\n# use manual search to get starting values \nstart_vals &lt;- maxloglik(50) %&gt;% select(-loglik)\n\n\n# Use optim function in R to find the values to maximize the log-likelihood\n#set fnscale = -1 to maximize (the default is minimize)\noptim(par = start_vals, fn = loglik, control=list(fnscale=-1))\n\n$par\n      phn       phh       phv \n0.5208272 0.4000361 0.5909793 \n\n$value\n[1] -61.57319\n\n$counts\nfunction gradient \n      66       NA \n\n$convergence\n[1] 0\n\n$message\nNULL"
  },
  {
    "objectID": "slides/02_likelihoods_ch2.html#model-comparisons-1",
    "href": "slides/02_likelihoods_ch2.html#model-comparisons-1",
    "title": "Beyond Least Squares: Using Likelihoods",
    "section": "Model comparisons",
    "text": "Model comparisons\n\nNested models\nNon-nested models"
  },
  {
    "objectID": "slides/02_likelihoods_ch2.html#nested-models-12",
    "href": "slides/02_likelihoods_ch2.html#nested-models-12",
    "title": "Beyond Least Squares: Using Likelihoods",
    "section": "Nested Models (1/2)",
    "text": "Nested Models (1/2)\nNested models: Models such that the parameters of the reduced model are a subset of the parameters for a larger model\nExample:\n\\[\\begin{aligned}&\\text{Model A: }y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\epsilon\\\\\n&\\text{Model B: }y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3 + \\beta_4 x_4 + \\epsilon\\end{aligned}\\]\n\nModel A is nested in Model B. We could use likelihoods to test whether it is useful to add \\(x_3\\) and \\(x_4\\) to the model.\n\n\n\\[\\begin{aligned}&H_0: \\beta_3 = \\beta_4 = 0 \\\\\n&H_a: \\text{ at least one }\\beta_j \\text{ is not equal to 0}\\end{aligned}\\]"
  },
  {
    "objectID": "slides/02_likelihoods_ch2.html#nested-models-22",
    "href": "slides/02_likelihoods_ch2.html#nested-models-22",
    "title": "Beyond Least Squares: Using Likelihoods",
    "section": "Nested models (2/2)",
    "text": "Nested models (2/2)\nAnother way to think about nested models: Parameters in larger model can be equated to get the simpler model or if some parameters can be set to constants\nExample:\n\\[\\begin{aligned}&\\text{Model 1: }p_H \\\\\n&\\text{Model 2: }p_{H| N}, p_{H| H Bias}, p_{H| V Bias}\\end{aligned}\\]\n\nModel 1 is nested in Model 2. The parameters \\(p_{H| N}\\), \\(p_{H|H Bias}\\), and \\(p_{H |V Bias}\\) can be set equal to \\(p_H\\) to get Model 1.\n\n\n\\[\\begin{aligned}&H_0: p_{H| N} = p_{H| H Bias} = p_{H| V Bias} = p_H \\\\\n&H_a: \\text{At least one of }p_{H| N}, p_{H| H Bias}, p_{H| V Bias} \\text{ differs from the others}\\end{aligned}\\]"
  },
  {
    "objectID": "slides/02_likelihoods_ch2.html#steps-to-compare-models",
    "href": "slides/02_likelihoods_ch2.html#steps-to-compare-models",
    "title": "Beyond Least Squares: Using Likelihoods",
    "section": "Steps to compare models",
    "text": "Steps to compare models\n1️⃣ Find the MLEs for each model.\n2️⃣ Plug the MLEs into the log-likelihood function for each model to get the maximum value of the log-likelihood for each model.\n3️⃣ Find the difference in the maximum log-likelihoods\n4️⃣ Use the Likelihood Ratio Test to determine if the difference is statistically significant"
  },
  {
    "objectID": "slides/02_likelihoods_ch2.html#steps-1---2",
    "href": "slides/02_likelihoods_ch2.html#steps-1---2",
    "title": "Beyond Least Squares: Using Likelihoods",
    "section": "Steps 1 - 2",
    "text": "Steps 1 - 2\nFind the MLEs for each model and plug them into the log-likelihood functions.\n\n\nModel 1:\n\n\\(\\hat{p}_H = 46/90 = 0.511\\)\n\n\nloglik1 &lt;- function(ph){\n log(ph^46 * (1 - ph)^44)\n}\nloglik1(46/90)\n\n[1] -62.36102\n\n\n. . .\nModel 2\n\n\\(\\hat{p}_{H|N} = 25 / 48 = 0.521\\)\n\\(\\hat{p}_{H|H Bias} = 8 /20 = 0.4\\)\n\\(\\hat{p}_{H|V Bias} = 13/ 22 = 0.591\\)\n\n. . .\n\nloglik2 &lt;- function(phn, phh, phv) {\n  log(phn^25 * (1 - phn)^23 * phh^8 * \n        (1 - phh)^12 * phv^13 * (1 - phv)^9)\n}\nloglik2(25/48, 8/20, 13/22)\n\n[1] -61.57319"
  },
  {
    "objectID": "slides/02_likelihoods_ch2.html#step-3",
    "href": "slides/02_likelihoods_ch2.html#step-3",
    "title": "Beyond Least Squares: Using Likelihoods",
    "section": "Step 3",
    "text": "Step 3\nFind the difference in the log-likelihoods\n\n(diff &lt;- loglik2(25/48, 8/20, 13/22) - loglik1(46/90))\n\n[1] 0.7878318\n\n\n\n\nIs the difference in the maximum log-likelihoods statistically significant?"
  },
  {
    "objectID": "slides/02_likelihoods_ch2.html#likelihood-ratio-test",
    "href": "slides/02_likelihoods_ch2.html#likelihood-ratio-test",
    "title": "Beyond Least Squares: Using Likelihoods",
    "section": "Likelihood Ratio Test",
    "text": "Likelihood Ratio Test\nTest statistic\n\\[\\begin{aligned} LRT &= 2[\\max\\{\\log(Lik(\\text{larger model}))\\} - \\max\\{\\log(Lik(\\text{reduced model}))\\}]\\\\[10pt]\n&= 2\\log\\Bigg(\\frac{\\max\\{(Lik(\\text{larger model})\\}}{\\max\\{(Lik(\\text{reduced model})\\}}\\Bigg)\\end{aligned}\\]\n\n\nLRT follows a \\(\\chi^2\\) distribution where the degrees of freedom equal the difference in the number of parameters between the two models"
  },
  {
    "objectID": "slides/02_likelihoods_ch2.html#step-4",
    "href": "slides/02_likelihoods_ch2.html#step-4",
    "title": "Beyond Least Squares: Using Likelihoods",
    "section": "Step 4",
    "text": "Step 4\n\n(LRT &lt;- 2 * (loglik2(25/48, 8/20, 13/22) - loglik1(46/90)))\n\n[1] 1.575664\n\n\n\nThe test statistic follows a \\(\\chi^2\\) distribution with 2 degrees of freedom. Therefore, the p-value is \\(P(\\chi^2 &gt; LRT)\\).\n\npchisq(LRT, 2, lower.tail = FALSE)\n\n[1] 0.4548299\n\n\n\n\nThe p-value is very large, so we fail to reject \\(H_0\\). We do not have convincing evidence that the conditional model is an improvement over the unconditional model. Therefore, we can stick with the unconditional model."
  },
  {
    "objectID": "slides/02_likelihoods_ch2.html#comparing-non-nested-models",
    "href": "slides/02_likelihoods_ch2.html#comparing-non-nested-models",
    "title": "Beyond Least Squares: Using Likelihoods",
    "section": "Comparing non-nested models",
    "text": "Comparing non-nested models\n\n\nAIC = -2(max log-likelihood) + 2p\n\n(Model1_AIC &lt;- 2 * loglik1(46/90) + 2 * 1)\n\n[1] -122.722\n\n(Model2_AIC &lt;-2 * loglik2(25/48, 8/20, 13/22) + 2 * 3)\n\n[1] -117.1464\n\n\n. . .\nBIC = -2(max log-likelihood) + plog(n)\n\n(Model1_BIC &lt;- 2 * loglik1(46/90) + 1 * log(30))\n\n[1] -121.3208\n\n(Model2_BIC &lt;-2 * loglik2(25/48, 8/20, 13/22) + 3 * log(30))\n\n[1] -112.9428\n\n\nChoose Model 1, the unconditional model, based on AIC and BIC"
  },
  {
    "objectID": "slides/02_likelihoods_ch2.html#looking-ahead",
    "href": "slides/02_likelihoods_ch2.html#looking-ahead",
    "title": "Beyond Least Squares: Using Likelihoods",
    "section": "Looking ahead",
    "text": "Looking ahead\n\nLikelihoods help us answer the question of how likely we are to observe the data given different parameters\nIn this example, we did not consider covariates, so in practice the parameters we want to estimate will look more similar to this\n\n\n\\[p_H = \\frac{e^{\\beta_0 + \\beta_1x_1 + \\dots + \\beta_px_p}}{1 + e^{\\beta_0 + \\beta_1x_1 + \\dots + \\beta_px_p}}\\]\n\n\n\nFinding the MLE becomes much more complex and numerical methods may be required.\n\nWe will primarily rely on software to find the MLE, but the conceptual ideas will be the same"
  },
  {
    "objectID": "slides/02_likelihoods_ch2.html#acknowledgements",
    "href": "slides/02_likelihoods_ch2.html#acknowledgements",
    "title": "Beyond Least Squares: Using Likelihoods",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThese slides are based on content in BMLR: Chapter 1 - Review of Multiple Linear Regression\nInitial versions of the slides are by Dr. Maria Tackett, Duke University"
  },
  {
    "objectID": "slides/04_poisson_ch4_o.html",
    "href": "slides/04_poisson_ch4_o.html",
    "title": "Poisson Regression",
    "section": "",
    "text": "library(tidyverse)\nlibrary(tidymodels)\nlibrary(GGally)\nlibrary(knitr)\nlibrary(patchwork)\nlibrary(viridis)\nlibrary(ggfortify)\nlibrary(gridExtra)"
  },
  {
    "objectID": "slides/04_poisson_ch4_o.html#setup",
    "href": "slides/04_poisson_ch4_o.html#setup",
    "title": "Poisson Regression",
    "section": "",
    "text": "library(tidyverse)\nlibrary(tidymodels)\nlibrary(GGally)\nlibrary(knitr)\nlibrary(patchwork)\nlibrary(viridis)\nlibrary(ggfortify)\nlibrary(gridExtra)"
  },
  {
    "objectID": "slides/04_poisson_ch4_o.html#learning-goals-12",
    "href": "slides/04_poisson_ch4_o.html#learning-goals-12",
    "title": "Poisson Regression",
    "section": "Learning goals (1/2)",
    "text": "Learning goals (1/2)\n\nDescribe properties of the Poisson random variable\nWrite the Poisson regression model\nDescribe how the Poisson regression differs from least-squares regression\nInterpret the coefficients for the Poisson regression model\nCompare two Poisson regression models\nDefine and calculate residuals for the Poisson regression model\nUse Goodness-of-fit to assess model fit"
  },
  {
    "objectID": "slides/04_poisson_ch4_o.html#learning-goals-22",
    "href": "slides/04_poisson_ch4_o.html#learning-goals-22",
    "title": "Poisson Regression",
    "section": "Learning goals (2/2)",
    "text": "Learning goals (2/2)\n\nIdentify overdispersion\nApply modeling approaches to deal with overdispersion\nExplore properties of negative binomial versus Poisson response\nFit and interpret models with offset to adjust for differences in sampling effort\nFit and interpret Zero-inflated Poisson models\nWrite likelihood for Poisson and Zero-inflated Poisson model"
  },
  {
    "objectID": "slides/04_poisson_ch4_o.html#scenarios-to-use-poisson-regression",
    "href": "slides/04_poisson_ch4_o.html#scenarios-to-use-poisson-regression",
    "title": "Poisson Regression",
    "section": "Scenarios to use Poisson regression",
    "text": "Scenarios to use Poisson regression\n\nDoes the number of employers conducting on-campus interviews during a year differ for public and private colleges?\nDoes the daily number of asthma-related visits to an Emergency Room differ depending on air pollution indices?\nDoes the number of paint defects per square foot of wall differ based on the years of experience of the painter?\n\n. . .\nEach response variable is a count per a unit of time or space."
  },
  {
    "objectID": "slides/04_poisson_ch4_o.html#poisson-distribution",
    "href": "slides/04_poisson_ch4_o.html#poisson-distribution",
    "title": "Poisson Regression",
    "section": "Poisson distribution",
    "text": "Poisson distribution\nLet \\(Y\\) be the number of events in a given unit of time or space. Then \\(Y\\) can be modeled using a Poisson distribution\n. . .\n\\[P(Y=y) = \\frac{e^{-\\lambda}\\lambda^y}{y!} \\hspace{10mm} y=0,1,2,\\ldots, \\infty\\]"
  },
  {
    "objectID": "slides/04_poisson_ch4_o.html#poisson-features",
    "href": "slides/04_poisson_ch4_o.html#poisson-features",
    "title": "Poisson Regression",
    "section": "Poisson Features",
    "text": "Poisson Features\n\n\\(E(Y) = Var(Y) = \\lambda\\)\nThe distribution is typically skewed right, particularly if \\(\\lambda\\) is small\nThe distribution becomes more symmetric as \\(\\lambda\\) increases\n\nIf \\(\\lambda\\) is sufficiently large, it can be approximated using a normal distribution (Click here for an example.)"
  },
  {
    "objectID": "slides/04_poisson_ch4_o.html#poisson-graphs",
    "href": "slides/04_poisson_ch4_o.html#poisson-graphs",
    "title": "Poisson Regression",
    "section": "Poisson Graphs",
    "text": "Poisson Graphs\n\nGraphsTable\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMean\nVariance\n\n\n\n\nlambda = 1\n0.99351\n0.9902178\n\n\nlambda = 5\n4.99367\n4.9865798\n\n\nlambda = 50\n49.99288\n49.8962683"
  },
  {
    "objectID": "slides/04_poisson_ch4_o.html#example",
    "href": "slides/04_poisson_ch4_o.html#example",
    "title": "Poisson Regression",
    "section": "Example",
    "text": "Example\nThe annual number of earthquakes registering at least 2.5 on the Richter Scale and having an epicenter within 40 miles of downtown Memphis follows a Poisson distribution with mean 6.5. What is the probability there will be at 3 or fewer such earthquakes next year?\n. . .\n\\[P(Y \\leq 3) = P(Y = 0) + P(Y = 1) + P(Y = 2) + P(Y = 3)\\]\n\\[ = \\frac{e^{-6.5}6.5^0}{0!} + \\frac{e^{-6.5}6.5^1}{1!} + \\frac{e^{-6.5}6.5^2}{2!} + \\frac{e^{-6.5}6.5^3}{3!}\\]\n\\[ = 0.112\\]\n. . .\n\nppois(3, 6.5)\n\n[1] 0.1118496\n\n\n\n\nExample adapted from Introduction to Probability Theory Example 28-2"
  },
  {
    "objectID": "slides/04_poisson_ch4_o.html#poisson-regression-1",
    "href": "slides/04_poisson_ch4_o.html#poisson-regression-1",
    "title": "Poisson Regression",
    "section": "Poisson regression",
    "text": "Poisson regression\nThe data: Household size in the Philippines\n\nThe data fHH1.csv come from the 2015 Family Income and Expenditure Survey conducted by the Philippine Statistics Authority.\nGoal: Understand the association between household size and various characteristics of the household\nResponse: - total: Number of people in the household other than the head\n\n\nPredictors: - location: Where the house is located - age: Age of the head of household - roof: Type of roof on the residence (proxy for wealth)\n\nOther variables: - numLT5: Number in the household under 5 years old"
  },
  {
    "objectID": "slides/04_poisson_ch4_o.html#the-data",
    "href": "slides/04_poisson_ch4_o.html#the-data",
    "title": "Poisson Regression",
    "section": "The data",
    "text": "The data\n\nhh_data &lt;- read_csv(\"data/fHH1.csv\")\nhh_data |&gt; slice(1:5) |&gt; kable()\n\n\n\n\nlocation\nage\ntotal\nnumLT5\nroof\n\n\n\n\nCentralLuzon\n65\n0\n0\nPredominantly Strong Material\n\n\nMetroManila\n75\n3\n0\nPredominantly Strong Material\n\n\nDavaoRegion\n54\n4\n0\nPredominantly Strong Material\n\n\nVisayas\n49\n3\n0\nPredominantly Strong Material\n\n\nMetroManila\n74\n3\n0\nPredominantly Strong Material"
  },
  {
    "objectID": "slides/04_poisson_ch4_o.html#response-variable",
    "href": "slides/04_poisson_ch4_o.html#response-variable",
    "title": "Poisson Regression",
    "section": "Response variable",
    "text": "Response variable\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmean\nvar\n\n\n\n\n3.685\n5.534"
  },
  {
    "objectID": "slides/04_poisson_ch4_o.html#why-the-least-squares-model-doesnt-work",
    "href": "slides/04_poisson_ch4_o.html#why-the-least-squares-model-doesnt-work",
    "title": "Poisson Regression",
    "section": "Why the least-squares model doesn’t work",
    "text": "Why the least-squares model doesn’t work\nThe goal is to model \\(\\lambda\\), the expected number of people in the household (other than the head), as a function of the predictors (covariates)\n. . .\nWe might be tempted to try a linear model \\[\\lambda_i = \\beta_0 + \\beta_1x_{i1} + \\beta_2x_{i2} + \\dots + \\beta_px_{ip}\\]\n. . .\nThis model won’t work because…\n\nIt could produce negative values of \\(\\lambda\\) for certain values of the predictors\nThe equal variance assumption required to conduct inference for linear regression is violated."
  },
  {
    "objectID": "slides/04_poisson_ch4_o.html#poisson-regression-model",
    "href": "slides/04_poisson_ch4_o.html#poisson-regression-model",
    "title": "Poisson Regression",
    "section": "Poisson regression model",
    "text": "Poisson regression model\nIf \\(Y_i \\sim Poisson\\) with \\(\\lambda = \\lambda_i\\) for the given values \\(x_{i1}, \\ldots, x_{ip}\\), then\n\\[\\log(\\lambda_i) = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\dots + \\beta_p x_{ip}\\]\n\nEach observation can have a different value of \\(\\lambda\\) based on its value of the predictors \\(x_1, \\ldots, x_p\\)\n\\(\\lambda\\) determines the mean and variance, so we don’t need to estimate a separate error term"
  },
  {
    "objectID": "slides/04_poisson_ch4_o.html#poisson-vs.-multiple-linear-regression",
    "href": "slides/04_poisson_ch4_o.html#poisson-vs.-multiple-linear-regression",
    "title": "Poisson Regression",
    "section": "Poisson vs. multiple linear regression",
    "text": "Poisson vs. multiple linear regression\n\n\n\n\n\nRegression models: Linear regression (left) and Poisson regression (right).\n\n\n\n\n\n\nFrom BMLR Figure 4.1"
  },
  {
    "objectID": "slides/04_poisson_ch4_o.html#assumptions-for-poisson-regression",
    "href": "slides/04_poisson_ch4_o.html#assumptions-for-poisson-regression",
    "title": "Poisson Regression",
    "section": "Assumptions for Poisson regression",
    "text": "Assumptions for Poisson regression\n\n\nPoisson response: The response variable is a count per unit of time or space, described by a Poisson distribution, at each level of the predictor(s)\nIndependence: The observations must be independent of one another\nMean = Variance: The mean must equal the variance\nLinearity: The log of the mean rate, \\(\\log(\\lambda)\\), must be a linear function of the predictor(s)"
  },
  {
    "objectID": "slides/04_poisson_ch4_o.html#model-1-number-in-household-vs.-age",
    "href": "slides/04_poisson_ch4_o.html#model-1-number-in-household-vs.-age",
    "title": "Poisson Regression",
    "section": "Model 1: Number in household vs. age",
    "text": "Model 1: Number in household vs. age\n\nmodel1 &lt;- glm(total ~ age, data = hh_data, family = poisson)\n\ntidy(model1) |&gt; \n  kable(digits = 4)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n1.5499\n0.0503\n30.8290\n0\n\n\nage\n-0.0047\n0.0009\n-5.0258\n0\n\n\n\n\n\n\\[\\log(\\hat{\\lambda}) = 1.5499  - 0.0047 ~ age\\]\n. . .\nQuestion: The coefficient for age is -0.0047. Interpret this coefficient in context."
  },
  {
    "objectID": "slides/04_poisson_ch4_o.html#answers",
    "href": "slides/04_poisson_ch4_o.html#answers",
    "title": "Poisson Regression",
    "section": "Answers",
    "text": "Answers\n\nEach additional year older the head of household is, the estimated average log of the number of people in the household is .0047 lower.\nEach additional year older the head of household is, the estimated average number of people in the household reduces by 0.5%.\nEach additional year older the head of household is the estimated average number of people in the household changes by a factor of .995.\nFor every 10 years older the head of household is, the estimated average number of people in the household reduces by 5%."
  },
  {
    "objectID": "slides/04_poisson_ch4_o.html#understanding-the-interpretation",
    "href": "slides/04_poisson_ch4_o.html#understanding-the-interpretation",
    "title": "Poisson Regression",
    "section": "Understanding the interpretation",
    "text": "Understanding the interpretation\nLet’s derive the change in predicted mean when we go from \\(x\\) to \\(x+1\\)\n(see boardwork)"
  },
  {
    "objectID": "slides/04_poisson_ch4_o.html#is-the-coefficient-of-age-statistically-significant",
    "href": "slides/04_poisson_ch4_o.html#is-the-coefficient-of-age-statistically-significant",
    "title": "Poisson Regression",
    "section": "Is the coefficient of age statistically significant?",
    "text": "Is the coefficient of age statistically significant?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n1.5499\n0.0503\n30.8290\n0\n1.4512\n1.6482\n\n\nage\n-0.0047\n0.0009\n-5.0258\n0\n-0.0065\n-0.0029\n\n\n\n\n\n\\[H_0: \\beta_1 = 0 \\hspace{2mm} \\text{ vs. } \\hspace{2mm} H_a: \\beta_1 \\neq 0\\]\n. . .\nTest statistic\n\\[Z = \\frac{\\hat{\\beta}_1 - 0}{SE(\\hat{\\beta}_1)} = \\frac{-0.0047 - 0}{0.0009} = -5.026 \\text{ (using exact values)}\\]\n. . .\nP-value\n\\[P(|Z| &gt; |-5.026|) = 5.01 \\times 10^{-7} \\approx 0\\]"
  },
  {
    "objectID": "slides/04_poisson_ch4_o.html#what-are-plausible-values-for-the-coefficient-of-age",
    "href": "slides/04_poisson_ch4_o.html#what-are-plausible-values-for-the-coefficient-of-age",
    "title": "Poisson Regression",
    "section": "What are plausible values for the coefficient of age?",
    "text": "What are plausible values for the coefficient of age?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n1.5499\n0.0503\n30.8290\n0\n1.4512\n1.6482\n\n\nage\n-0.0047\n0.0009\n-5.0258\n0\n-0.0065\n-0.0029\n\n\n\n\n\n95% confidence interval for the coefficient of age\n\\[\\hat{\\beta}_1 \\pm Z^{*}\\times SE(\\hat{\\beta}_1)\\] \\[-0.0047 \\pm 1.96 \\times 0.0009 = \\mathbf{(-.0065, -0.0029)}\\]\n. . .\nQuestion: Interpret the interval in terms of the change in mean household size."
  },
  {
    "objectID": "slides/04_poisson_ch4_o.html#which-can-best-help-us-determine-whether-model-1-is-a-good-fit",
    "href": "slides/04_poisson_ch4_o.html#which-can-best-help-us-determine-whether-model-1-is-a-good-fit",
    "title": "Poisson Regression",
    "section": "Which can best help us determine whether Model 1 is a good fit?",
    "text": "Which can best help us determine whether Model 1 is a good fit?\n\nData set pulled from BMLR Section 4.11.3.\n\nCase study in BMLR - Section 4.10\n\nPlotsCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\np1 &lt;- ggplot(data = hh_data, aes(x = age, y = total)) + \n  geom_point() + \n  labs(y = \"Total household size\", \n       title = \"Plot A\")\n\np2 &lt;- hh_data |&gt;\n  group_by(age) |&gt; \n  summarise(mean = mean(total)) |&gt;\n  ggplot(aes(x = age, y = mean))+ \n  geom_point() + \n  labs(y = \"Empirical mean household size\", \n       title = \"Plot B\")\n\np3 &lt;- hh_data |&gt;\n  group_by(age) |&gt; \n  summarise(log_mean = log(mean(total))) |&gt;\n  ggplot(aes(x = age, y = log_mean)) + \n  geom_point() + \n  labs(y = \"Log empirical mean household size\", \n       title = \"Plot C\")\n\np1 + p2 + p3 + plot_annotation(tag_levels = 'A')\n\n\nModel 2: Add a quadratic effect for age\n\nhh_data &lt;- hh_data |&gt; \n  mutate(age2 = age*age)\n\nmodel2 &lt;- glm(total ~ age + age2, data = hh_data, family = poisson)\ntidy(model2, conf.int = T) |&gt; \n  kable(digits = 4)\n\n\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n-0.3325\n0.1788\n-1.8594\n0.063\n-0.6863\n0.0148\n\n\nage\n0.0709\n0.0069\n10.2877\n0.000\n0.0575\n0.0845\n\n\nage2\n-0.0007\n0.0001\n-11.0578\n0.000\n-0.0008\n-0.0006\n\n\n\n\n\n\n\nModel 2: Add a quadratic effect for age\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n-0.3325\n0.1788\n-1.8594\n0.063\n-0.6863\n0.0148\n\n\nage\n0.0709\n0.0069\n10.2877\n0.000\n0.0575\n0.0845\n\n\nage2\n-0.0007\n0.0001\n-11.0578\n0.000\n-0.0008\n-0.0006\n\n\n\n\n\nWe can determine whether to keep \\(age^2\\) in the model in two ways:\n1️⃣ Use the p-value (or confidence interval) for the coefficient (since we are adding a single term to the model)\n2️⃣ Conduct a drop-in-deviance test\n\n\nDeviance\nA deviance is a way to measure how the observed data deviates from the model predictions.\n\nIt’s a measure unexplained variability in the response variable (similar to SSE in linear regression )\nLower deviance means the model is a better fit to the data\n\n. . .\nWe can calculate the “deviance residual” for each observation in the data (more on the formula later). Let \\((\\text{deviance residual}_i\\) be the deviance residual for the \\(i^{th}\\) observation, then\n\\[\\text{deviance} = \\sum(\\text{deviance residual})_i^2\\]\n. . .\nNote: Deviance is also known as the “residual deviance”\n\n\nDrop-in-Deviance Test\nWe can use a drop-in-deviance test to compare two models. To conduct the test\n1️⃣ Compute the deviance for each model\n2️⃣ Calculate the drop in deviance\n\\[\\text{drop-in-deviance =  Deviance(reduced model) - Deviance(larger model)}\\]\n. . .\n3️⃣ Given the reduced model is the true model \\((H_0 \\text{ true})\\), then \\[\\text{drop-in-deviance} \\sim \\chi_d^2\\]\nwhere \\(d\\) is the difference in degrees of freedom between the two models (i.e., the difference in number of terms)\n\n\nDrop-in-deviance - Model1 and Model2\n\nanova(model1, model2, test = \"Chisq\") |&gt;\n  kable(digits = 3)\n\n\n\n\nResid. Df\nResid. Dev\nDf\nDeviance\nPr(&gt;Chi)\n\n\n\n\n1498\n2337.089\nNA\nNA\nNA\n\n\n1497\n2200.944\n1\n136.145\n0\n\n\n\n\n\n. . .\nQuestions:\n\nWrite the null and alternative hypotheses.\nWhat does the value 2337.089 tell you?\nWhat does the value 1 tell you?\nWhat is your conclusion?\n\n\n\nAdd location to the model?\nSuppose we want to add location to the model, so we compare the following models:\nModel A: \\(\\lambda_i = \\beta_0 + \\beta_1 ~ age_i + \\beta_2 ~ age_i^2\\)\nModel B: \\(\\lambda_i =  \\beta_0 + \\beta_1 ~ age_i + \\beta_2 ~ age_i^2 + \\beta_3 ~ Loc1_i + \\beta_4 ~ Loc2_i + \\beta_5 ~ Loc3_i + \\beta_6 ~ Loc4_i\\)\n\n\nQuestion\nWhich of the following are reliable ways to determine if location should be added to the model?\n. . .\n\nDrop-in-deviance test\nUse the p-value for each coefficient\nLikelihood ratio test\nNested F Test\nBIC\n\n\n\nAdd location to the model?\n\nmodel3 &lt;- glm(total ~ age + age2 + location, data = hh_data, family = poisson)\n\n. . .\nUse a drop-in-deviance test to determine if Model 2 or Model 3 (with location) is a better fit for the data.\n. . .\n\nanova(model2, model3, test = \"Chisq\") |&gt;\n  kable(digits = 3)\n\n\n\n\nResid. Df\nResid. Dev\nDf\nDeviance\nPr(&gt;Chi)\n\n\n\n\n1497\n2200.944\nNA\nNA\nNA\n\n\n1493\n2187.800\n4\n13.144\n0.011\n\n\n\n\n\nThe p-value is small (0.01 &lt; 0.05), so we conclude that Model 3 is a better fit for the data.\n\n\nModel 3\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n-0.3843\n0.1821\n-2.1107\n0.0348\n-0.7444\n-0.0306\n\n\nage\n0.0704\n0.0069\n10.1900\n0.0000\n0.0569\n0.0840\n\n\nage2\n-0.0007\n0.0001\n-10.9437\n0.0000\n-0.0008\n-0.0006\n\n\nlocationDavaoRegion\n-0.0194\n0.0538\n-0.3605\n0.7185\n-0.1250\n0.0859\n\n\nlocationIlocosRegion\n0.0610\n0.0527\n1.1580\n0.2468\n-0.0423\n0.1641\n\n\nlocationMetroManila\n0.0545\n0.0472\n1.1542\n0.2484\n-0.0378\n0.1473\n\n\nlocationVisayas\n0.1121\n0.0417\n2.6853\n0.0072\n0.0308\n0.1945\n\n\n\n\n\n. . .\nDoes this model sufficiently explain the variability in the mean household size?\n\n\nGoodness-of-fit\n\nPearson residuals\nWe can calculate two types of residuals for Poisson regression: Pearson residuals and deviance residuals\n. . .\n\\[\\text{Pearson residual}_i = \\frac{\\text{observed} - \\text{predicted}}{\\text{std. error}} = \\frac{y_i - \\hat{\\lambda}_i}{\\sqrt{\\hat{\\lambda}_i}}\\]\n. . .\n\nSimilar interpretation as standardized residuals from linear regression\nExpect most to fall between -2 and 2\nUsed to calculate overdispersion parameter\n\n\n\nDeviance residuals\nThe deviance residual indicates how much the observed data deviates from the fitted model\n\\[\\text{deviance residual}_i = \\text{sign}(y_i - \\hat{\\lambda}_i)\\sqrt{2\\Bigg[y_i\\log\\bigg(\\frac{y_i}{\\hat{\\lambda}_i}\\bigg) - (y_i - \\hat{\\lambda}_i)\\Bigg]}\\]\nwhere\n\\[\\text{sign}(y_i - \\hat{\\lambda}_i)  =  \\begin{cases}\n1 & \\text{ if }(y_i - \\hat{\\lambda}_i) &gt; 0 \\\\\n-1 & \\text{ if }(y_i - \\hat{\\lambda}_i) &lt; 0 \\\\\n0 & \\text{ if }(y_i - \\hat{\\lambda}_i) = 0\n\\end{cases}\\]\n\n\nModel 3: Residual plots\n\nmodel3_aug_pearson &lt;- augment(model3, type.residuals = \"pearson\") \nmodel3_aug_deviance &lt;- augment(model3, type.residuals = \"deviance\")\n\n\nPlotsCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\np1 &lt;- ggplot(data = model3_aug_pearson, aes(x = .fitted, y = .resid)) + \n  geom_point()  + \n  geom_smooth() + \n  labs(x = \"Fitted values\", \n       y = \"Pearson residuals\", \n       title = \"Pearson residuals vs. fitted\")\n\np2 &lt;-  ggplot(data = model3_aug_deviance, aes(x = .fitted, y = .resid)) + \n  geom_point()  + \n  geom_smooth() + \n  labs(x = \"Fitted values\", \n       y = \"Deviance residuals\", \n       title = \"Deviance residuals vs. fitted\")\n\np1 + p2\n\n\n\n\n\n\nGoodness-of-fit\n\nGoal: Use the (residual) deviance to assess how much the predicted values differ from the observed values. Recall \\((\\text{deviance}) = \\sum_{i=1}^{n}(\\text{deviance residual})_i^2\\)\nIf the model sufficiently fits the data, then\n\n\\[\\text{deviance} \\sim \\chi^2_{df}\\]\nwhere \\(df\\) is the model’s residual degrees of freedom\n. . .\n\nQuestion: What is the probability of observing a deviance larger than the one we’ve observed, given this model sufficiently fits the data?\n\n. . .\n\\[P(\\chi^2_{df} &gt; \\text{ deviance})\\]\n\n\nModel 3: Goodness-of-fit calculations\n\nmodel3$deviance\n\n[1] 2187.8\n\nmodel3$df.residual\n\n[1] 1493\n\n\n\npchisq(model3$deviance, model3$df.residual, lower.tail = FALSE)\n\n[1] 3.153732e-29\n\n\nThe probability of observing a deviance greater than 2187.8 is \\(\\approx 0\\), so there is significant evidence of lack-of-fit.\n\n\nLack-of-fit\nThere are a few potential reasons for lack-of-fit:\n\nMissing important interactions or higher-order terms\nMissing important variables (perhaps this means a more comprehensive data set is required)\nThere could be extreme observations causing the deviance to be larger than expected (assess based on the residual plots)\nThere could be a problem with the Poisson model\n\nMay need more flexibility in the model to handle overdispersion\n\n\n\n\nOverdispersion\nOverdispersion: There is more variability in the response than what is implied by the Poisson model\n\nTablesCode\n\n\n\n\nOverall\n\n\n\n\n\nmean\nvar\n\n\n\n\n3.685\n5.534\n\n\n\n\n\n\nby Location\n\n\n\n\n\nlocation\nmean\nvar\n\n\n\n\nCentralLuzon\n3.402\n4.152\n\n\nDavaoRegion\n3.390\n4.723\n\n\nIlocosRegion\n3.586\n5.402\n\n\nMetroManila\n3.707\n4.863\n\n\nVisayas\n3.902\n6.602\n\n\n\n\n\n\n\n\n\n\nhh_data |&gt;\n  summarise(mean = mean(total), var = var(total)) |&gt;\n  kable(digits = 3)\n\n\nhh_data |&gt;\n  group_by(location) |&gt;\n  summarise(mean = mean(total), var = var(total)) |&gt;\n  kable(digits = 3)\n\n\n\n\n\n\nWhy overdispersion matters\nIf there is overdispersion, then there is more variation in the response than what’s implied by a Poisson model. This means\n❌ The standard errors of the model coefficients are artificially small\n❌ The p-values are artificially small\n❌ This could lead to models that are more complex than what is needed\n. . .\nWe can take overdispersion into account by\n\ninflating standard errors by multiplying them by a dispersion factor\nusing a negative-binomial regression model\n\n\n\n\nQuasi-poission\n\nDispersion parameter\nThe dispersion parameter is represented by \\(\\phi\\)\n\\[\\hat{\\phi} =\\frac{\\text{deviance}}{\\text{residual df}} = \\frac{\\sum_{i=1}^{n}(\\text{Pearson residuals})^2}{n - p}\\]\nwhere \\(p\\) is the number of terms in the model (including the intercept)\n\nIf there is no overdispersion \\(\\hat{\\phi} = 1\\)\nIf there is overdispersion \\(\\hat{\\phi} &gt;  1\\)\n\n\n\nAccounting for dispersion in the model\nWe inflate the standard errors of the coefficient by multiplying the variance by \\(\\hat{\\phi}\\)\n. . .\n\\[SE_{Q}(\\hat{\\beta}) = \\sqrt{\\hat{\\phi}}  * SE(\\hat{\\beta})\\] - “Q” stands for quasi-Poisson, since this is an ad-hoc solution - The process for model building and model comparison is called quasilikelihood (similar to likelihood without exact underlying distributions)\n\n\nModel 3: Quasi-Poisson model\n\nmodel3_q &lt;- glm(total ~ age + age2 + location, data = hh_data, \n                family = quasipoisson) #&lt;&lt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n-0.3843\n0.2166\n-1.7744\n0.0762\n-0.8134\n0.0358\n\n\nage\n0.0704\n0.0082\n8.5665\n0.0000\n0.0544\n0.0866\n\n\nage2\n-0.0007\n0.0001\n-9.2000\n0.0000\n-0.0009\n-0.0006\n\n\nlocationDavaoRegion\n-0.0194\n0.0640\n-0.3030\n0.7619\n-0.1451\n0.1058\n\n\nlocationIlocosRegion\n0.0610\n0.0626\n0.9735\n0.3304\n-0.0620\n0.1837\n\n\nlocationMetroManila\n0.0545\n0.0561\n0.9703\n0.3320\n-0.0552\n0.1649\n\n\nlocationVisayas\n0.1121\n0.0497\n2.2574\n0.0241\n0.0156\n0.2103\n\n\n\n\n\n\n\nPoisson vs. Q-Poisson\n\n\nPoisson\n\n\n\n\n\nterm\nestimate\nstd.error\n\n\n\n\n(Intercept)\n-0.3843\n0.1821\n\n\nage\n0.0704\n0.0069\n\n\nage2\n-0.0007\n0.0001\n\n\nlocationDavaoRegion\n-0.0194\n0.0538\n\n\nlocationIlocosRegion\n0.0610\n0.0527\n\n\nlocationMetroManila\n0.0545\n0.0472\n\n\nlocationVisayas\n0.1121\n0.0417\n\n\n\n\n\n\nQuasi-Poisson\n\n\n\n\n\nestimate\nstd.error\n\n\n\n\n-0.3843\n0.2166\n\n\n0.0704\n0.0082\n\n\n-0.0007\n0.0001\n\n\n-0.0194\n0.0640\n\n\n0.0610\n0.0626\n\n\n0.0545\n0.0561\n\n\n0.1121\n0.0497\n\n\n\n\n\n\n\n\n\nQ-Poisson: Inference for coefficients\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\n\n\n\n\n(Intercept)\n-0.3843\n0.2166\n\n\nage\n0.0704\n0.0082\n\n\nage2\n-0.0007\n0.0001\n\n\nlocationDavaoRegion\n-0.0194\n0.0640\n\n\nlocationIlocosRegion\n0.0610\n0.0626\n\n\nlocationMetroManila\n0.0545\n0.0561\n\n\nlocationVisayas\n0.1121\n0.0497\n\n\n\n\n\n\nTest statistic \\[t = \\frac{\\hat{\\beta} - 0}{SE_{Q}(\\hat{\\beta})} \\sim t_{n-p}\\]\n\n\n\n\n\nNegative binomial regression model\n\nNegative binomial regression model\nAnother approach to handle overdispersion is to use a negative binomial regression model\n\nThis has more flexibility than the quasi-Poisson model, because there is a new parameter in addition to \\(\\lambda\\)\n\n\n. . .\nLet \\(Y\\) be a negative binomial random variable, \\(Y\\sim NegBinom(r, p)\\), then\n\\[P(Y = y_i) = {y_i + r - 1 \\choose r - 1}(1-p)^{y_i}p^r \\hspace{5mm} y_i = 0, 1, 2, \\ldots, \\infty\\]\n\n\nNegative binomial regression model\n\nMain idea: Generate a \\(\\lambda\\) for each observation (household) and generate a count using the Poisson random variable with parameter \\(\\lambda\\)\n\nMakes the counts more dispersed than with a single parameter\n\nThink of it as a Poisson model such that \\(\\lambda\\) is also random\n\n. . .\n\\(\\begin{aligned} &\\text{If }Y|\\lambda \\sim Poisson(\\lambda)\\\\\n&\\text{ and } \\lambda \\sim Gamma\\bigg(r, \\frac{1-p}{p}\\bigg)\\\\\n&\\text{ then } Y \\sim NegBinom(r, p)\\end{aligned}\\)\n\n\nNegative binomial regression in R\n\nlibrary(MASS)\nmodel3_nb &lt;- glm.nb(total ~ age + age2 + location, data = hh_data)\ntidy(model3_nb) |&gt; \n  kable(digits = 4)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-0.3753\n0.2076\n-1.8081\n0.0706\n\n\nage\n0.0699\n0.0079\n8.8981\n0.0000\n\n\nage2\n-0.0007\n0.0001\n-9.5756\n0.0000\n\n\nlocationDavaoRegion\n-0.0219\n0.0625\n-0.3501\n0.7262\n\n\nlocationIlocosRegion\n0.0577\n0.0615\n0.9391\n0.3477\n\n\nlocationMetroManila\n0.0562\n0.0551\n1.0213\n0.3071\n\n\nlocationVisayas\n0.1104\n0.0487\n2.2654\n0.0235\n\n\n\n\n\n\n\n\nOffsets\n\nData: Airbnbs in NYC\nThe data set NYCairbnb-sample.csv contains information about a random sample of 1000 Airbnbs in New York City. It is a subset of the data on 40628 Airbnbs scraped by Awad et al. (2017).\nVariables\n\nnumber_of_reviews: Number of reviews for the unit on Airbnb (proxy for number of rentals)\nprice: price per night in US dollars\nroom_type: Entire home/apartment, private room, or shared room\ndays: Number of days the unit has been listed (date when info scraped - date when unit first listed on Airbnb)\n\n\n\n\nData: Airbnbs in NYC\n\nairbnb &lt;- read_csv(\"data/NYCairbnb-sample.csv\") |&gt;\n  dplyr::select(id, number_of_reviews, days, room_type, price)\n\n\n\n\n\n\nid\nnumber_of_reviews\ndays\nroom_type\nprice\n\n\n\n\n15756544\n16\n1144\nPrivate room\n120\n\n\n14218251\n15\n471\nPrivate room\n89\n\n\n21644\n0\n2600\nPrivate room\n89\n\n\n13667835\n1\n283\nEntire home/apt\n150\n\n\n265912\n0\n1970\nEntire home/apt\n89\n\n\n\n\n\nGoal: Use the price and room type of Airbnbs to describe variation in the number of reviews (a proxy for number of rentals).\n\n\nEDA\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\np1 &lt;- ggplot(data = airbnb, aes(x = number_of_reviews)) + \n  geom_histogram() + \n   labs(x = \"Number of reviews\",\n    title = \"Distribution of number of reviews\")\n\np2 &lt;- airbnb |&gt;\n  filter(price &lt;= 2000) |&gt;\n  group_by(price) |&gt;\n  summarise(log_mean = log(mean(number_of_reviews))) |&gt;\n  ggplot(aes(x = price, y = log_mean)) + \n  geom_point(alpha= 0.7) + \n  geom_smooth() + \n  labs(x = \"Price in  US dollars\",\n    y = \"Log(mean # reviews)\", \n    title = \"Log mean #  of reviews vs. price\", \n    subtitle = \"Airbnbs $2000 or less\")\n\np3 &lt;- airbnb |&gt;\n  filter(price &lt;= 500) |&gt;\n  group_by(price) |&gt;\n  summarise(log_mean = log(mean(number_of_reviews))) |&gt;\n  ggplot(aes(x = price, y = log_mean)) + \n  geom_point(alpha= 0.7) + \n  geom_smooth() + \n  labs(x = \"Price in  US dollars\",\n    y = \"Log(mean # reviews)\", \n    title = \"Log mean # of reviews vs. price\", \n    subtitle = \"Airbnbs $500 or less\")\n\np1  / (p2 + p3) \n\n\n\n\n\n\nEDA\n\n\nOverall\n\n\n\n\n\nmean\nvar\n\n\n\n\n15.916\n765.969\n\n\n\n\n\n\nby Room type\n\n\n\n\n\nroom_type\nmean\nvar\n\n\n\n\nEntire home/apt\n16.283\n760.348\n\n\nPrivate room\n15.608\n786.399\n\n\nShared room\n15.028\n605.971\n\n\n\n\n\n\n\n\n\nConsiderations for modeling\nWe would like to fit the Poisson regression model\n\\[\\log(\\lambda) = \\beta_0 + \\beta_1 ~ price + \\beta_2 ~ room\\_type1 + \\beta_3 ~ room\\_type2\\]\n. . .\nQuestion: - Based on the EDA, what are some potential issues we may want to address in the model building?\n\nSuppose any model fit issues are addressed. What are some potential limitations to the conclusions and interpretations from the model?\n\n\n\nOffset\n\nSometimes counts are not directly comparable because the observations differ based on some characteristic directly related to the counts, i.e. the sampling effort.\nAn offset can be used to adjust for differences in sampling effort.\n\n. . .\n\nLet \\(x_{offset}\\) be the variable that accounts for differences in sampling effort, then \\(\\log(x_{offset})\\) will be added to the model.\n\n. . .\n\\(\\log(\\lambda_i) = \\beta_0 + \\beta_1 ~ x_{i1} + \\beta_2 ~ x_{i2} + ... + \\beta_p ~ x_{ip} + \\log(x_{offset_i})\\)\n\nThe offset is a term in the model with coefficient always equal to 1.\n\n\n\nAdding an offset to the Airbnb model\nWe will add the offset \\(\\log(days)\\) to the model. This accounts for the fact that we would expect Airbnbs that have been listed longer to have more reviews.\n\\[\\log(\\lambda_i) = \\beta_0 + \\beta_1 ~ price_i + \\beta_2 ~ room\\_type1_i + \\beta_3 ~ room\\_type2_i + \\log(days_i)\\] \nNote: The response variable for the model is still \\(\\log(\\lambda_i)\\), the log mean number of reviews\n\n\nDetail on the offset\nWe want to adjust for the number of days, so we are interested in \\(\\frac{reviews}{days}\\).\n. . .\nGiven \\(\\lambda\\) is the mean number of reviews\n\\[\\log\\Big(\\frac{\\lambda}{days}\\Big) = \\beta_0 + \\beta_1 ~ price + \\beta_2 ~ room\\_type1 + \\beta_3 ~ room\\_type2\\]\n. . .\n\\[\\Rightarrow \\log({\\lambda}) - \\log(days) = \\beta_0 + \\beta_1 ~ price + \\beta_2 ~ room\\_type1 + \\beta_3 ~ room\\_type2\\]\n. . .\n\\[\\Rightarrow \\log({\\lambda}) = \\beta_0 + \\beta_1 ~ price + \\beta_2 ~ room\\_type1 + \\beta_3 ~ room\\_type2 + \\log(days)\\]\n\n\nAirbnb model in R\n\nairbnb_model &lt;- glm(number_of_reviews ~ price + room_type, \n                    data = airbnb, family = poisson, \n                    offset = log(days)) #&lt;&lt;\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-4.1351\n0.0170\n-243.1397\n0\n\n\nprice\n-0.0005\n0.0001\n-7.0952\n0\n\n\nroom_typePrivate room\n-0.0994\n0.0174\n-5.6986\n0\n\n\nroom_typeShared room\n0.2436\n0.0452\n5.3841\n0\n\n\n\n\n\n. . .\nThe coefficient for \\(\\log(days)\\) is fixed at 1, so it is not in the model output.\n\n\nInterpretations\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-4.1351\n0.0170\n-243.1397\n0\n\n\nprice\n-0.0005\n0.0001\n-7.0952\n0\n\n\nroom_typePrivate room\n-0.0994\n0.0174\n-5.6986\n0\n\n\nroom_typeShared room\n0.2436\n0.0452\n5.3841\n0\n\n\n\n\n\n\nQuestion:\n\nInterpret the coefficient of price.\nInterpret the coefficient of room_typePrivate room\n\n\n\nGoodness-of-fit\n\\[\\begin{aligned}&H_0: \\text{ The model is a good fit for the data}\\\\\n&H_a: \\text{ There is significant lack of fit}\\end{aligned}\\]\n\npchisq(airbnb_model$deviance, airbnb_model$df.residual, lower.tail = F)\n\n[1] 0\n\n\n. . .\nThere is evidence of significant lack of fit in the model. Therefore, more models would need to be explored that address the issues mentioned earlier.\n. . .\nIn practice we would assess goodness-of-fit and finalize the model before any interpretations and conclusions.\n\n\n\nZero-inflated Poisson model\n\nData: Weekend drinking\nThe data weekend-drinks.csv contains information from a survey of 77 students in a introductory statistics course on a dry campus.\nVariables\n\ndrinks: Number of drinks they had in the past weekend\noff_campus: 1 - lives off campus, 0 otherwise\nfirst_year: 1 - student is a first-year, 0 otherwise\nsex: f - student identifies as female, m - student identifies as male\n\n. . .\nGoal: The goal is explore factors related to drinking behavior on a dry campus.\n\n\n\nEDA: Response variable\n\n\n\n\n\n\n\n\n\n\n\nObserved vs. expected response\n\n\n\n\n\n\n\n\n\n. . .\nWhat does it mean to be a “zero” in this data?\n\n\nTwo types of zeros\nThere are two types of zeros\n\nThose who happen to have a zero in the data set (people who drink but happened to not drink last weekend)\nThose who will always report a value of zero (non-drinkers)\n\nThese are called true zeros\n\n\n. . .\nWe introduce a new parameter \\(\\alpha\\) for the proportion of true zeros, then fit a model that has two components:\n. . .\n1️⃣ The association between mean number of drinks and various characteristics among those who drink\n2️⃣ The estimated proportion of non-drinkers\n\n\nZero-inflated Poisson model\nZero-inflated Poisson (ZIP) model has two parts\n. . .\n1️⃣ Association, among those who drink, between the mean number of drinks and predictors sex and off campus residence\n. . .\n\\[\\log(\\lambda) = \\beta_0 + \\beta_1 ~ off\\_campus + \\beta_2 ~ sex\\] where \\(\\lambda\\) is the mean number of drinks among those who drink\n. . .\n2️⃣ Probability that a student does not drink\n\\[\\text{logit}(\\alpha) = \\log\\Big(\\frac{\\alpha}{1- \\alpha}\\Big) = \\beta_0 + \\beta_1 ~ first\\_year\\]\nwhere \\(\\alpha\\) is the proportion of non-drinkers\n. . .\nNote: The same variables can be used in each component\n\n\nDetails of the ZIP model\n\nThe ZIP model is a special case of a latent variable model\n\nA type of mixture model where observations for one or more groups occur together but the group membership unknown\n\nZero-inflated models are a common type of mixture model; they apply beyond Poisson regression\n\n\n\nZIP model in R\nFit ZIP models using the zeroinfl function from the pscl R package.\n\nlibrary(pscl)\n\ndrinks_zip &lt;- zeroinfl(drinks ~ off_campus + sex | first_year,\n                data = drinks)\ndrinks_zip\n\n\nCall:\nzeroinfl(formula = drinks ~ off_campus + sex | first_year, data = drinks)\n\nCount model coefficients (poisson with log link):\n(Intercept)   off_campus         sexm  \n     0.7543       0.4159       1.0209  \n\nZero-inflation model coefficients (binomial with logit link):\n(Intercept)   first_year  \n    -0.6036       1.1364  \n\n\n\n\nTidy output\nUse the tidy function from the poissonreg package for tidy model output.\n\nlibrary(poissonreg)\n\n. . .\nMean number of drinks among those who drink\n\ntidy(drinks_zip, type = \"count\") %&gt;% kable(digits = 3)\n\n\n\n\nterm\ntype\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\ncount\n0.754\n0.144\n5.238\n0.000\n\n\noff_campus\ncount\n0.416\n0.206\n2.021\n0.043\n\n\nsexm\ncount\n1.021\n0.175\n5.827\n0.000\n\n\n\n\n\n\n\nTidy output\nProportion of non-drinkers\n\ntidy(drinks_zip, type = \"zero\") %&gt;% kable(digits = 3)\n\n\n\n\nterm\ntype\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\nzero\n-0.604\n0.311\n-1.938\n0.053\n\n\nfirst_year\nzero\n1.136\n0.610\n1.864\n0.062\n\n\n\n\n\n\n\nInterpreting the model coefficients\n\n\n\n\n\nterm\ntype\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\ncount\n0.754\n0.144\n5.238\n0.000\n\n\noff_campus\ncount\n0.416\n0.206\n2.021\n0.043\n\n\nsexm\ncount\n1.021\n0.175\n5.827\n0.000\n\n\n\n\n\n\nQuestions\n\nInterpret the intercept.\nInterpret the coefficients of off_campus and sexm.\n\n\n\nEstimated proportion zeros\n\n\n\n\n\nterm\ntype\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\nzero\n-0.604\n0.311\n-1.938\n0.053\n\n\nfirst_year\nzero\n1.136\n0.610\n1.864\n0.062\n\n\n\n\n\nQuestions:\nBased on the model…\n\nWhat is the probability a first-year student is a non-drinker?\nWhat is the probability a upperclass student (sophomore, junior, senior) is a non-drinker?\n\n\n\nThese are just a few of the many models…\n\nUse the Vuong Test to compare the fit of the ZIP model to a regular Poisson model\n\nWhy can’t we use a drop-in-deviance test?\n\nWe’ve just discussed the ZIP model here, but there are other model applications beyond the standard Poisson regression model (e.g., hurdle models, Zero-inflated Negative Binomial models, etc. )\n\n\n\n\nLikelihoods for Poisson models\n\nEstimating coefficients in Poisson model\n\nLeast squares estimation would not work because the normality and equal variance assumptions don’t hold for Poisson regression\nMaximum likelihood estimation is used to estimate the coefficients of Poisson regression.\nThe likelihood is the product of the probabilities for the \\(n\\) independent observations in the data\n\n\n\nLikelihood for regular Poisson model\nLet’s go back to example about household size in the Philippines. We will focus on the model using the main effect of age to understand variability in mean household size.\n. . .\nSuppose the first five observations have household sizes of 4, 2, 8, 6, and 1. Then the likelihood is\n. . .\n\\(L = \\frac{e^{-\\lambda_1}\\lambda_1^4}{4!} * \\frac{e^{-\\lambda_2}\\lambda_2^2}{2!} * \\frac{e^{-\\lambda_3}\\lambda_3^8}{8!} *\n\\frac{e^{-\\lambda_4}\\lambda_4^6}{6!} * \\frac{e^{-\\lambda_5}\\lambda_5^1}{1!}\\)\n\n\nLikelihood for regular Poisson model\nWe will use the log likelihood to make finding the MLE easier\n. . .\n\\(\\begin{aligned}\\log(L) &= -\\lambda_1 + 4\\log(\\lambda_1) - \\lambda_2 + 2\\log(\\lambda_2) - \\lambda_3 + 8\\log(\\lambda_3)\\\\ & -\\lambda_4 + 6 \\log(\\lambda_4) - \\lambda_5 + \\log(\\lambda_5) + C \\end{aligned}\\)\nwhere - \\(\\lambda\\) is the mean number in household depending on \\(x_i\\) - \\(C = -[\\log(4!) + \\log(2!) + \\log(8!) + \\log(6!)+ \\log(1!)]\\)\n\n\nLikelihood for regular Poisson model\nGiven the age of the head of the household, we fit the model\n\\[\\log(\\lambda_i) = \\beta_0 + \\beta_1~age_i\\]\nThen we replace each \\(\\lambda_i\\) in \\(\\log(L)\\) with \\(e^{\\beta_0 + \\beta_1~age_i}\\).\n. . .\nSuppose the first five observations have ages \\(X = (32, 21, 55, 44, 28)\\). Then\n. . .\n\\[\\begin{aligned} \\log(L) &= [-e^{\\beta_0 + \\beta_132}+ 4(\\beta_0 + \\beta_1 32)] + [ - e^{\\beta_0 + \\beta_121} + 2(\\beta_0 + \\beta_121)] \\\\ &+  [- e^{\\beta_0 + \\beta_155} + 8(\\beta_0 + \\beta_155)] +  [-e^{\\beta_0 + \\beta_144} + 6(\\beta_0 + \\beta_144)] \\\\ &+ [-e^{\\beta_0 + \\beta_128}(\\beta_0 + \\beta_128)] + C \\end{aligned}\\]\n. . .\nUse search algorithm to find the values of \\(\\beta_0\\) and \\(\\beta_1\\) that maximize the above equation.\n\n\nProbabilities under ZIP model\nThere are three different types of observations in the data:\n\nObserved zero and will always be 0 (true zeros)\nObserved 0 but will not always be 0\nObserved non-zero count and will not always be 0\n\n\n\nProbabilities under ZIP model\nTrue zeros\n\\[P(0 | \\text{true zero})= \\alpha\\]\n. . .\nObserved 0 but will not always be 0\n\\[P(0 | \\text{not always zero}) = (1 - \\alpha)\\frac{e^{-\\lambda}\\lambda^0}{0!}\\]\n. . .\nDid not observe 0 and will not always be 0\n\\[P(z_i | \\text{not always zero}) = (1 - \\alpha)\\frac{e^{-\\lambda}\\lambda^{z_i}}{z_i!}\\]\n\n\nProbabilities under ZIP model\nPutting this all together. Let \\(y_i\\) be an observed response then\n\\[P(Y_i = y_i | x_i) = \\begin{cases}\n\\alpha + (1 - \\alpha)e^{-\\lambda_i} && \\text{ if } y_i = 0 \\\\\n(1 - \\alpha)\\frac{e^{-\\lambda_i}\\lambda_i^{y_i}}{y_i!} && \\text{ if } y_i &gt; 0\n\\end{cases}\\]\n. . .\nRecall from our example,\n\\[\\lambda_i = e^{\\beta_0 + \\beta_1~off\\_campus_i + \\beta_2 ~ sex_i}\\]\n\\[\\alpha_i = \\frac{e^{\\beta_{0\\alpha} + \\beta_{1\\alpha} ~ first\\_year_i}}{1 + e^{\\beta_{0\\alpha} + \\beta_{1\\alpha} ~ first\\_year_i}}\\]\n\nPlug in \\(\\lambda_i\\) and \\(\\alpha_i\\) into the above equation obtain the likelihood function\n\n\n\nAcknowledgements\nThese slides are based on content in BMLR: Chapter 4\nInitial versions of the slides are by Dr. Maria Tackett, Duke University"
  },
  {
    "objectID": "slides/04_poisson_ch4_o.html#model-2-add-a-quadratic-effect-for-age",
    "href": "slides/04_poisson_ch4_o.html#model-2-add-a-quadratic-effect-for-age",
    "title": "Poisson Regression",
    "section": "Model 2: Add a quadratic effect for age",
    "text": "Model 2: Add a quadratic effect for age\n\nhh_data &lt;- hh_data |&gt; \n  mutate(age2 = age*age)\n\nmodel2 &lt;- glm(total ~ age + age2, data = hh_data, family = poisson)\ntidy(model2, conf.int = T) |&gt; \n  kable(digits = 4)\n\n\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n-0.3325\n0.1788\n-1.8594\n0.063\n-0.6863\n0.0148\n\n\nage\n0.0709\n0.0069\n10.2877\n0.000\n0.0575\n0.0845\n\n\nage2\n-0.0007\n0.0001\n-11.0578\n0.000\n-0.0008\n-0.0006"
  },
  {
    "objectID": "slides/04_poisson_ch4_o.html#model-2-add-a-quadratic-effect-for-age-1",
    "href": "slides/04_poisson_ch4_o.html#model-2-add-a-quadratic-effect-for-age-1",
    "title": "Poisson Regression",
    "section": "Model 2: Add a quadratic effect for age",
    "text": "Model 2: Add a quadratic effect for age\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n-0.3325\n0.1788\n-1.8594\n0.063\n-0.6863\n0.0148\n\n\nage\n0.0709\n0.0069\n10.2877\n0.000\n0.0575\n0.0845\n\n\nage2\n-0.0007\n0.0001\n-11.0578\n0.000\n-0.0008\n-0.0006\n\n\n\n\n\nWe can determine whether to keep \\(age^2\\) in the model in two ways:\n1️⃣ Use the p-value (or confidence interval) for the coefficient (since we are adding a single term to the model)\n2️⃣ Conduct a drop-in-deviance test"
  },
  {
    "objectID": "slides/04_poisson_ch4_o.html#deviance",
    "href": "slides/04_poisson_ch4_o.html#deviance",
    "title": "Poisson Regression",
    "section": "Deviance",
    "text": "Deviance\nA deviance is a way to measure how the observed data deviates from the model predictions.\n\nIt’s a measure unexplained variability in the response variable (similar to SSE in linear regression )\nLower deviance means the model is a better fit to the data\n\n. . .\nWe can calculate the “deviance residual” for each observation in the data (more on the formula later). Let \\((\\text{deviance residual}_i\\) be the deviance residual for the \\(i^{th}\\) observation, then\n\\[\\text{deviance} = \\sum(\\text{deviance residual})_i^2\\]\n. . .\nNote: Deviance is also known as the “residual deviance”"
  },
  {
    "objectID": "slides/04_poisson_ch4_o.html#drop-in-deviance-test",
    "href": "slides/04_poisson_ch4_o.html#drop-in-deviance-test",
    "title": "Poisson Regression",
    "section": "Drop-in-Deviance Test",
    "text": "Drop-in-Deviance Test\nWe can use a drop-in-deviance test to compare two models. To conduct the test\n1️⃣ Compute the deviance for each model\n2️⃣ Calculate the drop in deviance\n\\[\\text{drop-in-deviance =  Deviance(reduced model) - Deviance(larger model)}\\]\n. . .\n3️⃣ Given the reduced model is the true model \\((H_0 \\text{ true})\\), then \\[\\text{drop-in-deviance} \\sim \\chi_d^2\\]\nwhere \\(d\\) is the difference in degrees of freedom between the two models (i.e., the difference in number of terms)"
  },
  {
    "objectID": "slides/04_poisson_ch4_o.html#drop-in-deviance---model1-and-model2",
    "href": "slides/04_poisson_ch4_o.html#drop-in-deviance---model1-and-model2",
    "title": "Poisson Regression",
    "section": "Drop-in-deviance - Model1 and Model2",
    "text": "Drop-in-deviance - Model1 and Model2\n\nanova(model1, model2, test = \"Chisq\") |&gt;\n  kable(digits = 3)\n\n\n\n\nResid. Df\nResid. Dev\nDf\nDeviance\nPr(&gt;Chi)\n\n\n\n\n1498\n2337.089\nNA\nNA\nNA\n\n\n1497\n2200.944\n1\n136.145\n0\n\n\n\n\n\n. . .\nQuestions:\n\nWrite the null and alternative hypotheses.\nWhat does the value 2337.089 tell you?\nWhat does the value 1 tell you?\nWhat is your conclusion?"
  },
  {
    "objectID": "slides/04_poisson_ch4_o.html#add-location-to-the-model",
    "href": "slides/04_poisson_ch4_o.html#add-location-to-the-model",
    "title": "Poisson Regression",
    "section": "Add location to the model?",
    "text": "Add location to the model?\nSuppose we want to add location to the model, so we compare the following models:\nModel A: \\(\\lambda_i = \\beta_0 + \\beta_1 ~ age_i + \\beta_2 ~ age_i^2\\)\nModel B: \\(\\lambda_i =  \\beta_0 + \\beta_1 ~ age_i + \\beta_2 ~ age_i^2 + \\beta_3 ~ Loc1_i + \\beta_4 ~ Loc2_i + \\beta_5 ~ Loc3_i + \\beta_6 ~ Loc4_i\\)"
  },
  {
    "objectID": "slides/04_poisson_ch4_o.html#question",
    "href": "slides/04_poisson_ch4_o.html#question",
    "title": "Poisson Regression",
    "section": "Question",
    "text": "Question\nWhich of the following are reliable ways to determine if location should be added to the model?\n. . .\n\nDrop-in-deviance test\nUse the p-value for each coefficient\nLikelihood ratio test\nNested F Test\nBIC"
  },
  {
    "objectID": "slides/04_poisson_ch4_o.html#add-location-to-the-model-1",
    "href": "slides/04_poisson_ch4_o.html#add-location-to-the-model-1",
    "title": "Poisson Regression",
    "section": "Add location to the model?",
    "text": "Add location to the model?\n\nmodel3 &lt;- glm(total ~ age + age2 + location, data = hh_data, family = poisson)\n\n. . .\nUse a drop-in-deviance test to determine if Model 2 or Model 3 (with location) is a better fit for the data.\n. . .\n\nanova(model2, model3, test = \"Chisq\") |&gt;\n  kable(digits = 3)\n\n\n\n\nResid. Df\nResid. Dev\nDf\nDeviance\nPr(&gt;Chi)\n\n\n\n\n1497\n2200.944\nNA\nNA\nNA\n\n\n1493\n2187.800\n4\n13.144\n0.011\n\n\n\n\n\nThe p-value is small (0.01 &lt; 0.05), so we conclude that Model 3 is a better fit for the data."
  },
  {
    "objectID": "slides/04_poisson_ch4_o.html#model-3",
    "href": "slides/04_poisson_ch4_o.html#model-3",
    "title": "Poisson Regression",
    "section": "Model 3",
    "text": "Model 3\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n-0.3843\n0.1821\n-2.1107\n0.0348\n-0.7444\n-0.0306\n\n\nage\n0.0704\n0.0069\n10.1900\n0.0000\n0.0569\n0.0840\n\n\nage2\n-0.0007\n0.0001\n-10.9437\n0.0000\n-0.0008\n-0.0006\n\n\nlocationDavaoRegion\n-0.0194\n0.0538\n-0.3605\n0.7185\n-0.1250\n0.0859\n\n\nlocationIlocosRegion\n0.0610\n0.0527\n1.1580\n0.2468\n-0.0423\n0.1641\n\n\nlocationMetroManila\n0.0545\n0.0472\n1.1542\n0.2484\n-0.0378\n0.1473\n\n\nlocationVisayas\n0.1121\n0.0417\n2.6853\n0.0072\n0.0308\n0.1945\n\n\n\n\n\n. . .\nDoes this model sufficiently explain the variability in the mean household size?"
  },
  {
    "objectID": "slides/04_poisson_ch4_o.html#pearson-residuals",
    "href": "slides/04_poisson_ch4_o.html#pearson-residuals",
    "title": "Poisson Regression",
    "section": "Pearson residuals",
    "text": "Pearson residuals\nWe can calculate two types of residuals for Poisson regression: Pearson residuals and deviance residuals\n. . .\n\\[\\text{Pearson residual}_i = \\frac{\\text{observed} - \\text{predicted}}{\\text{std. error}} = \\frac{y_i - \\hat{\\lambda}_i}{\\sqrt{\\hat{\\lambda}_i}}\\]\n. . .\n\nSimilar interpretation as standardized residuals from linear regression\nExpect most to fall between -2 and 2\nUsed to calculate overdispersion parameter"
  },
  {
    "objectID": "slides/04_poisson_ch4_o.html#deviance-residuals",
    "href": "slides/04_poisson_ch4_o.html#deviance-residuals",
    "title": "Poisson Regression",
    "section": "Deviance residuals",
    "text": "Deviance residuals\nThe deviance residual indicates how much the observed data deviates from the fitted model\n\\[\\text{deviance residual}_i = \\text{sign}(y_i - \\hat{\\lambda}_i)\\sqrt{2\\Bigg[y_i\\log\\bigg(\\frac{y_i}{\\hat{\\lambda}_i}\\bigg) - (y_i - \\hat{\\lambda}_i)\\Bigg]}\\]\nwhere\n\\[\\text{sign}(y_i - \\hat{\\lambda}_i)  =  \\begin{cases}\n1 & \\text{ if }(y_i - \\hat{\\lambda}_i) &gt; 0 \\\\\n-1 & \\text{ if }(y_i - \\hat{\\lambda}_i) &lt; 0 \\\\\n0 & \\text{ if }(y_i - \\hat{\\lambda}_i) = 0\n\\end{cases}\\]"
  },
  {
    "objectID": "slides/04_poisson_ch4_o.html#model-3-residual-plots",
    "href": "slides/04_poisson_ch4_o.html#model-3-residual-plots",
    "title": "Poisson Regression",
    "section": "Model 3: Residual plots",
    "text": "Model 3: Residual plots\n\nmodel3_aug_pearson &lt;- augment(model3, type.residuals = \"pearson\") \nmodel3_aug_deviance &lt;- augment(model3, type.residuals = \"deviance\")\n\n\nPlotsCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\np1 &lt;- ggplot(data = model3_aug_pearson, aes(x = .fitted, y = .resid)) + \n  geom_point()  + \n  geom_smooth() + \n  labs(x = \"Fitted values\", \n       y = \"Pearson residuals\", \n       title = \"Pearson residuals vs. fitted\")\n\np2 &lt;-  ggplot(data = model3_aug_deviance, aes(x = .fitted, y = .resid)) + \n  geom_point()  + \n  geom_smooth() + \n  labs(x = \"Fitted values\", \n       y = \"Deviance residuals\", \n       title = \"Deviance residuals vs. fitted\")\n\np1 + p2"
  },
  {
    "objectID": "slides/04_poisson_ch4_o.html#goodness-of-fit-1",
    "href": "slides/04_poisson_ch4_o.html#goodness-of-fit-1",
    "title": "Poisson Regression",
    "section": "Goodness-of-fit",
    "text": "Goodness-of-fit\n\nGoal: Use the (residual) deviance to assess how much the predicted values differ from the observed values. Recall \\((\\text{deviance}) = \\sum_{i=1}^{n}(\\text{deviance residual})_i^2\\)\nIf the model sufficiently fits the data, then\n\n\\[\\text{deviance} \\sim \\chi^2_{df}\\]\nwhere \\(df\\) is the model’s residual degrees of freedom\n. . .\n\nQuestion: What is the probability of observing a deviance larger than the one we’ve observed, given this model sufficiently fits the data?\n\n. . .\n\\[P(\\chi^2_{df} &gt; \\text{ deviance})\\]"
  },
  {
    "objectID": "slides/04_poisson_ch4_o.html#model-3-goodness-of-fit-calculations",
    "href": "slides/04_poisson_ch4_o.html#model-3-goodness-of-fit-calculations",
    "title": "Poisson Regression",
    "section": "Model 3: Goodness-of-fit calculations",
    "text": "Model 3: Goodness-of-fit calculations\n\nmodel3$deviance\n\n[1] 2187.8\n\nmodel3$df.residual\n\n[1] 1493\n\n\n\npchisq(model3$deviance, model3$df.residual, lower.tail = FALSE)\n\n[1] 3.153732e-29\n\n\nThe probability of observing a deviance greater than 2187.8 is \\(\\approx 0\\), so there is significant evidence of lack-of-fit."
  },
  {
    "objectID": "slides/04_poisson_ch4_o.html#lack-of-fit",
    "href": "slides/04_poisson_ch4_o.html#lack-of-fit",
    "title": "Poisson Regression",
    "section": "Lack-of-fit",
    "text": "Lack-of-fit\nThere are a few potential reasons for lack-of-fit:\n\nMissing important interactions or higher-order terms\nMissing important variables (perhaps this means a more comprehensive data set is required)\nThere could be extreme observations causing the deviance to be larger than expected (assess based on the residual plots)\nThere could be a problem with the Poisson model\n\nMay need more flexibility in the model to handle overdispersion"
  },
  {
    "objectID": "slides/04_poisson_ch4_o.html#overdispersion",
    "href": "slides/04_poisson_ch4_o.html#overdispersion",
    "title": "Poisson Regression",
    "section": "Overdispersion",
    "text": "Overdispersion\nOverdispersion: There is more variability in the response than what is implied by the Poisson model\n\nTablesCode\n\n\n\n\nOverall\n\n\n\n\n\nmean\nvar\n\n\n\n\n3.685\n5.534\n\n\n\n\n\n\nby Location\n\n\n\n\n\nlocation\nmean\nvar\n\n\n\n\nCentralLuzon\n3.402\n4.152\n\n\nDavaoRegion\n3.390\n4.723\n\n\nIlocosRegion\n3.586\n5.402\n\n\nMetroManila\n3.707\n4.863\n\n\nVisayas\n3.902\n6.602\n\n\n\n\n\n\n\n\n\n\nhh_data |&gt;\n  summarise(mean = mean(total), var = var(total)) |&gt;\n  kable(digits = 3)\n\n\nhh_data |&gt;\n  group_by(location) |&gt;\n  summarise(mean = mean(total), var = var(total)) |&gt;\n  kable(digits = 3)"
  },
  {
    "objectID": "slides/04_poisson_ch4_o.html#why-overdispersion-matters",
    "href": "slides/04_poisson_ch4_o.html#why-overdispersion-matters",
    "title": "Poisson Regression",
    "section": "Why overdispersion matters",
    "text": "Why overdispersion matters\nIf there is overdispersion, then there is more variation in the response than what’s implied by a Poisson model. This means\n❌ The standard errors of the model coefficients are artificially small\n❌ The p-values are artificially small\n❌ This could lead to models that are more complex than what is needed\n. . .\nWe can take overdispersion into account by\n\ninflating standard errors by multiplying them by a dispersion factor\nusing a negative-binomial regression model"
  },
  {
    "objectID": "slides/04_poisson_ch4_o.html#dispersion-parameter",
    "href": "slides/04_poisson_ch4_o.html#dispersion-parameter",
    "title": "Poisson Regression",
    "section": "Dispersion parameter",
    "text": "Dispersion parameter\nThe dispersion parameter is represented by \\(\\phi\\)\n\\[\\hat{\\phi} =\\frac{\\text{deviance}}{\\text{residual df}} = \\frac{\\sum_{i=1}^{n}(\\text{Pearson residuals})^2}{n - p}\\]\nwhere \\(p\\) is the number of terms in the model (including the intercept)\n\nIf there is no overdispersion \\(\\hat{\\phi} = 1\\)\nIf there is overdispersion \\(\\hat{\\phi} &gt;  1\\)"
  },
  {
    "objectID": "slides/04_poisson_ch4_o.html#accounting-for-dispersion-in-the-model",
    "href": "slides/04_poisson_ch4_o.html#accounting-for-dispersion-in-the-model",
    "title": "Poisson Regression",
    "section": "Accounting for dispersion in the model",
    "text": "Accounting for dispersion in the model\nWe inflate the standard errors of the coefficient by multiplying the variance by \\(\\hat{\\phi}\\)\n. . .\n\\[SE_{Q}(\\hat{\\beta}) = \\sqrt{\\hat{\\phi}}  * SE(\\hat{\\beta})\\] - “Q” stands for quasi-Poisson, since this is an ad-hoc solution - The process for model building and model comparison is called quasilikelihood (similar to likelihood without exact underlying distributions)"
  },
  {
    "objectID": "slides/04_poisson_ch4_o.html#model-3-quasi-poisson-model",
    "href": "slides/04_poisson_ch4_o.html#model-3-quasi-poisson-model",
    "title": "Poisson Regression",
    "section": "Model 3: Quasi-Poisson model",
    "text": "Model 3: Quasi-Poisson model\n\nmodel3_q &lt;- glm(total ~ age + age2 + location, data = hh_data, \n                family = quasipoisson) #&lt;&lt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n-0.3843\n0.2166\n-1.7744\n0.0762\n-0.8134\n0.0358\n\n\nage\n0.0704\n0.0082\n8.5665\n0.0000\n0.0544\n0.0866\n\n\nage2\n-0.0007\n0.0001\n-9.2000\n0.0000\n-0.0009\n-0.0006\n\n\nlocationDavaoRegion\n-0.0194\n0.0640\n-0.3030\n0.7619\n-0.1451\n0.1058\n\n\nlocationIlocosRegion\n0.0610\n0.0626\n0.9735\n0.3304\n-0.0620\n0.1837\n\n\nlocationMetroManila\n0.0545\n0.0561\n0.9703\n0.3320\n-0.0552\n0.1649\n\n\nlocationVisayas\n0.1121\n0.0497\n2.2574\n0.0241\n0.0156\n0.2103"
  },
  {
    "objectID": "slides/04_poisson_ch4_o.html#poisson-vs.-q-poisson",
    "href": "slides/04_poisson_ch4_o.html#poisson-vs.-q-poisson",
    "title": "Poisson Regression",
    "section": "Poisson vs. Q-Poisson",
    "text": "Poisson vs. Q-Poisson\n\n\nPoisson\n\n\n\n\n\nterm\nestimate\nstd.error\n\n\n\n\n(Intercept)\n-0.3843\n0.1821\n\n\nage\n0.0704\n0.0069\n\n\nage2\n-0.0007\n0.0001\n\n\nlocationDavaoRegion\n-0.0194\n0.0538\n\n\nlocationIlocosRegion\n0.0610\n0.0527\n\n\nlocationMetroManila\n0.0545\n0.0472\n\n\nlocationVisayas\n0.1121\n0.0417\n\n\n\n\n\n\nQuasi-Poisson\n\n\n\n\n\nestimate\nstd.error\n\n\n\n\n-0.3843\n0.2166\n\n\n0.0704\n0.0082\n\n\n-0.0007\n0.0001\n\n\n-0.0194\n0.0640\n\n\n0.0610\n0.0626\n\n\n0.0545\n0.0561\n\n\n0.1121\n0.0497"
  },
  {
    "objectID": "slides/04_poisson_ch4_o.html#q-poisson-inference-for-coefficients",
    "href": "slides/04_poisson_ch4_o.html#q-poisson-inference-for-coefficients",
    "title": "Poisson Regression",
    "section": "Q-Poisson: Inference for coefficients",
    "text": "Q-Poisson: Inference for coefficients\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\n\n\n\n\n(Intercept)\n-0.3843\n0.2166\n\n\nage\n0.0704\n0.0082\n\n\nage2\n-0.0007\n0.0001\n\n\nlocationDavaoRegion\n-0.0194\n0.0640\n\n\nlocationIlocosRegion\n0.0610\n0.0626\n\n\nlocationMetroManila\n0.0545\n0.0561\n\n\nlocationVisayas\n0.1121\n0.0497\n\n\n\n\n\n\nTest statistic \\[t = \\frac{\\hat{\\beta} - 0}{SE_{Q}(\\hat{\\beta})} \\sim t_{n-p}\\]"
  },
  {
    "objectID": "slides/04_poisson_ch4_o.html#negative-binomial-regression-model-1",
    "href": "slides/04_poisson_ch4_o.html#negative-binomial-regression-model-1",
    "title": "Poisson Regression",
    "section": "Negative binomial regression model",
    "text": "Negative binomial regression model\nAnother approach to handle overdispersion is to use a negative binomial regression model\n\nThis has more flexibility than the quasi-Poisson model, because there is a new parameter in addition to \\(\\lambda\\)\n\n\n. . .\nLet \\(Y\\) be a negative binomial random variable, \\(Y\\sim NegBinom(r, p)\\), then\n\\[P(Y = y_i) = {y_i + r - 1 \\choose r - 1}(1-p)^{y_i}p^r \\hspace{5mm} y_i = 0, 1, 2, \\ldots, \\infty\\]"
  },
  {
    "objectID": "slides/04_poisson_ch4_o.html#negative-binomial-regression-model-2",
    "href": "slides/04_poisson_ch4_o.html#negative-binomial-regression-model-2",
    "title": "Poisson Regression",
    "section": "Negative binomial regression model",
    "text": "Negative binomial regression model\n\nMain idea: Generate a \\(\\lambda\\) for each observation (household) and generate a count using the Poisson random variable with parameter \\(\\lambda\\)\n\nMakes the counts more dispersed than with a single parameter\n\nThink of it as a Poisson model such that \\(\\lambda\\) is also random\n\n. . .\n\\(\\begin{aligned} &\\text{If }Y|\\lambda \\sim Poisson(\\lambda)\\\\\n&\\text{ and } \\lambda \\sim Gamma\\bigg(r, \\frac{1-p}{p}\\bigg)\\\\\n&\\text{ then } Y \\sim NegBinom(r, p)\\end{aligned}\\)"
  },
  {
    "objectID": "slides/04_poisson_ch4_o.html#negative-binomial-regression-in-r",
    "href": "slides/04_poisson_ch4_o.html#negative-binomial-regression-in-r",
    "title": "Poisson Regression",
    "section": "Negative binomial regression in R",
    "text": "Negative binomial regression in R\n\nlibrary(MASS)\nmodel3_nb &lt;- glm.nb(total ~ age + age2 + location, data = hh_data)\ntidy(model3_nb) |&gt; \n  kable(digits = 4)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-0.3753\n0.2076\n-1.8081\n0.0706\n\n\nage\n0.0699\n0.0079\n8.8981\n0.0000\n\n\nage2\n-0.0007\n0.0001\n-9.5756\n0.0000\n\n\nlocationDavaoRegion\n-0.0219\n0.0625\n-0.3501\n0.7262\n\n\nlocationIlocosRegion\n0.0577\n0.0615\n0.9391\n0.3477\n\n\nlocationMetroManila\n0.0562\n0.0551\n1.0213\n0.3071\n\n\nlocationVisayas\n0.1104\n0.0487\n2.2654\n0.0235"
  },
  {
    "objectID": "slides/04_poisson_ch4_o.html#data-airbnbs-in-nyc",
    "href": "slides/04_poisson_ch4_o.html#data-airbnbs-in-nyc",
    "title": "Poisson Regression",
    "section": "Data: Airbnbs in NYC",
    "text": "Data: Airbnbs in NYC\nThe data set NYCairbnb-sample.csv contains information about a random sample of 1000 Airbnbs in New York City. It is a subset of the data on 40628 Airbnbs scraped by Awad et al. (2017).\nVariables\n\nnumber_of_reviews: Number of reviews for the unit on Airbnb (proxy for number of rentals)\nprice: price per night in US dollars\nroom_type: Entire home/apartment, private room, or shared room\ndays: Number of days the unit has been listed (date when info scraped - date when unit first listed on Airbnb)"
  },
  {
    "objectID": "slides/04_poisson_ch4_o.html#data-airbnbs-in-nyc-1",
    "href": "slides/04_poisson_ch4_o.html#data-airbnbs-in-nyc-1",
    "title": "Poisson Regression",
    "section": "Data: Airbnbs in NYC",
    "text": "Data: Airbnbs in NYC\n\nairbnb &lt;- read_csv(\"data/NYCairbnb-sample.csv\") |&gt;\n  dplyr::select(id, number_of_reviews, days, room_type, price)\n\n\n\n\n\n\nid\nnumber_of_reviews\ndays\nroom_type\nprice\n\n\n\n\n15756544\n16\n1144\nPrivate room\n120\n\n\n14218251\n15\n471\nPrivate room\n89\n\n\n21644\n0\n2600\nPrivate room\n89\n\n\n13667835\n1\n283\nEntire home/apt\n150\n\n\n265912\n0\n1970\nEntire home/apt\n89\n\n\n\n\n\nGoal: Use the price and room type of Airbnbs to describe variation in the number of reviews (a proxy for number of rentals)."
  },
  {
    "objectID": "slides/04_poisson_ch4_o.html#eda",
    "href": "slides/04_poisson_ch4_o.html#eda",
    "title": "Poisson Regression",
    "section": "EDA",
    "text": "EDA\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\np1 &lt;- ggplot(data = airbnb, aes(x = number_of_reviews)) + \n  geom_histogram() + \n   labs(x = \"Number of reviews\",\n    title = \"Distribution of number of reviews\")\n\np2 &lt;- airbnb |&gt;\n  filter(price &lt;= 2000) |&gt;\n  group_by(price) |&gt;\n  summarise(log_mean = log(mean(number_of_reviews))) |&gt;\n  ggplot(aes(x = price, y = log_mean)) + \n  geom_point(alpha= 0.7) + \n  geom_smooth() + \n  labs(x = \"Price in  US dollars\",\n    y = \"Log(mean # reviews)\", \n    title = \"Log mean #  of reviews vs. price\", \n    subtitle = \"Airbnbs $2000 or less\")\n\np3 &lt;- airbnb |&gt;\n  filter(price &lt;= 500) |&gt;\n  group_by(price) |&gt;\n  summarise(log_mean = log(mean(number_of_reviews))) |&gt;\n  ggplot(aes(x = price, y = log_mean)) + \n  geom_point(alpha= 0.7) + \n  geom_smooth() + \n  labs(x = \"Price in  US dollars\",\n    y = \"Log(mean # reviews)\", \n    title = \"Log mean # of reviews vs. price\", \n    subtitle = \"Airbnbs $500 or less\")\n\np1  / (p2 + p3)"
  },
  {
    "objectID": "slides/04_poisson_ch4_o.html#eda-1",
    "href": "slides/04_poisson_ch4_o.html#eda-1",
    "title": "Poisson Regression",
    "section": "EDA",
    "text": "EDA\n\n\nOverall\n\n\n\n\n\nmean\nvar\n\n\n\n\n15.916\n765.969\n\n\n\n\n\n\nby Room type\n\n\n\n\n\nroom_type\nmean\nvar\n\n\n\n\nEntire home/apt\n16.283\n760.348\n\n\nPrivate room\n15.608\n786.399\n\n\nShared room\n15.028\n605.971"
  },
  {
    "objectID": "slides/04_poisson_ch4_o.html#considerations-for-modeling",
    "href": "slides/04_poisson_ch4_o.html#considerations-for-modeling",
    "title": "Poisson Regression",
    "section": "Considerations for modeling",
    "text": "Considerations for modeling\nWe would like to fit the Poisson regression model\n\\[\\log(\\lambda) = \\beta_0 + \\beta_1 ~ price + \\beta_2 ~ room\\_type1 + \\beta_3 ~ room\\_type2\\]\n. . .\nQuestion: - Based on the EDA, what are some potential issues we may want to address in the model building?\n\nSuppose any model fit issues are addressed. What are some potential limitations to the conclusions and interpretations from the model?"
  },
  {
    "objectID": "slides/04_poisson_ch4_o.html#offset",
    "href": "slides/04_poisson_ch4_o.html#offset",
    "title": "Poisson Regression",
    "section": "Offset",
    "text": "Offset\n\nSometimes counts are not directly comparable because the observations differ based on some characteristic directly related to the counts, i.e. the sampling effort.\nAn offset can be used to adjust for differences in sampling effort.\n\n. . .\n\nLet \\(x_{offset}\\) be the variable that accounts for differences in sampling effort, then \\(\\log(x_{offset})\\) will be added to the model.\n\n. . .\n\\(\\log(\\lambda_i) = \\beta_0 + \\beta_1 ~ x_{i1} + \\beta_2 ~ x_{i2} + ... + \\beta_p ~ x_{ip} + \\log(x_{offset_i})\\)\n\nThe offset is a term in the model with coefficient always equal to 1."
  },
  {
    "objectID": "slides/04_poisson_ch4_o.html#adding-an-offset-to-the-airbnb-model",
    "href": "slides/04_poisson_ch4_o.html#adding-an-offset-to-the-airbnb-model",
    "title": "Poisson Regression",
    "section": "Adding an offset to the Airbnb model",
    "text": "Adding an offset to the Airbnb model\nWe will add the offset \\(\\log(days)\\) to the model. This accounts for the fact that we would expect Airbnbs that have been listed longer to have more reviews.\n\\[\\log(\\lambda_i) = \\beta_0 + \\beta_1 ~ price_i + \\beta_2 ~ room\\_type1_i + \\beta_3 ~ room\\_type2_i + \\log(days_i)\\] \nNote: The response variable for the model is still \\(\\log(\\lambda_i)\\), the log mean number of reviews"
  },
  {
    "objectID": "slides/04_poisson_ch4_o.html#detail-on-the-offset",
    "href": "slides/04_poisson_ch4_o.html#detail-on-the-offset",
    "title": "Poisson Regression",
    "section": "Detail on the offset",
    "text": "Detail on the offset\nWe want to adjust for the number of days, so we are interested in \\(\\frac{reviews}{days}\\).\n. . .\nGiven \\(\\lambda\\) is the mean number of reviews\n\\[\\log\\Big(\\frac{\\lambda}{days}\\Big) = \\beta_0 + \\beta_1 ~ price + \\beta_2 ~ room\\_type1 + \\beta_3 ~ room\\_type2\\]\n. . .\n\\[\\Rightarrow \\log({\\lambda}) - \\log(days) = \\beta_0 + \\beta_1 ~ price + \\beta_2 ~ room\\_type1 + \\beta_3 ~ room\\_type2\\]\n. . .\n\\[\\Rightarrow \\log({\\lambda}) = \\beta_0 + \\beta_1 ~ price + \\beta_2 ~ room\\_type1 + \\beta_3 ~ room\\_type2 + \\log(days)\\]"
  },
  {
    "objectID": "slides/04_poisson_ch4_o.html#airbnb-model-in-r",
    "href": "slides/04_poisson_ch4_o.html#airbnb-model-in-r",
    "title": "Poisson Regression",
    "section": "Airbnb model in R",
    "text": "Airbnb model in R\n\nairbnb_model &lt;- glm(number_of_reviews ~ price + room_type, \n                    data = airbnb, family = poisson, \n                    offset = log(days)) #&lt;&lt;\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-4.1351\n0.0170\n-243.1397\n0\n\n\nprice\n-0.0005\n0.0001\n-7.0952\n0\n\n\nroom_typePrivate room\n-0.0994\n0.0174\n-5.6986\n0\n\n\nroom_typeShared room\n0.2436\n0.0452\n5.3841\n0\n\n\n\n\n\n. . .\nThe coefficient for \\(\\log(days)\\) is fixed at 1, so it is not in the model output."
  },
  {
    "objectID": "slides/04_poisson_ch4_o.html#interpretations",
    "href": "slides/04_poisson_ch4_o.html#interpretations",
    "title": "Poisson Regression",
    "section": "Interpretations",
    "text": "Interpretations\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-4.1351\n0.0170\n-243.1397\n0\n\n\nprice\n-0.0005\n0.0001\n-7.0952\n0\n\n\nroom_typePrivate room\n-0.0994\n0.0174\n-5.6986\n0\n\n\nroom_typeShared room\n0.2436\n0.0452\n5.3841\n0\n\n\n\n\n\n\nQuestion:\n\nInterpret the coefficient of price.\nInterpret the coefficient of room_typePrivate room"
  },
  {
    "objectID": "slides/04_poisson_ch4_o.html#goodness-of-fit-2",
    "href": "slides/04_poisson_ch4_o.html#goodness-of-fit-2",
    "title": "Poisson Regression",
    "section": "Goodness-of-fit",
    "text": "Goodness-of-fit\n\\[\\begin{aligned}&H_0: \\text{ The model is a good fit for the data}\\\\\n&H_a: \\text{ There is significant lack of fit}\\end{aligned}\\]\n\npchisq(airbnb_model$deviance, airbnb_model$df.residual, lower.tail = F)\n\n[1] 0\n\n\n. . .\nThere is evidence of significant lack of fit in the model. Therefore, more models would need to be explored that address the issues mentioned earlier.\n. . .\nIn practice we would assess goodness-of-fit and finalize the model before any interpretations and conclusions."
  },
  {
    "objectID": "slides/04_poisson_ch4_o.html#data-weekend-drinking",
    "href": "slides/04_poisson_ch4_o.html#data-weekend-drinking",
    "title": "Poisson Regression",
    "section": "Data: Weekend drinking",
    "text": "Data: Weekend drinking\nThe data weekend-drinks.csv contains information from a survey of 77 students in a introductory statistics course on a dry campus.\nVariables\n\ndrinks: Number of drinks they had in the past weekend\noff_campus: 1 - lives off campus, 0 otherwise\nfirst_year: 1 - student is a first-year, 0 otherwise\nsex: f - student identifies as female, m - student identifies as male\n\n. . .\nGoal: The goal is explore factors related to drinking behavior on a dry campus."
  },
  {
    "objectID": "slides/04_poisson_ch4_o.html#eda-response-variable",
    "href": "slides/04_poisson_ch4_o.html#eda-response-variable",
    "title": "Poisson Regression",
    "section": "EDA: Response variable",
    "text": "EDA: Response variable"
  },
  {
    "objectID": "slides/04_poisson_ch4_o.html#observed-vs.-expected-response",
    "href": "slides/04_poisson_ch4_o.html#observed-vs.-expected-response",
    "title": "Poisson Regression",
    "section": "Observed vs. expected response",
    "text": "Observed vs. expected response\n\n\n\n\n\n\n\n\n\n. . .\nWhat does it mean to be a “zero” in this data?"
  },
  {
    "objectID": "slides/04_poisson_ch4_o.html#two-types-of-zeros",
    "href": "slides/04_poisson_ch4_o.html#two-types-of-zeros",
    "title": "Poisson Regression",
    "section": "Two types of zeros",
    "text": "Two types of zeros\nThere are two types of zeros\n\nThose who happen to have a zero in the data set (people who drink but happened to not drink last weekend)\nThose who will always report a value of zero (non-drinkers)\n\nThese are called true zeros\n\n\n. . .\nWe introduce a new parameter \\(\\alpha\\) for the proportion of true zeros, then fit a model that has two components:\n. . .\n1️⃣ The association between mean number of drinks and various characteristics among those who drink\n2️⃣ The estimated proportion of non-drinkers"
  },
  {
    "objectID": "slides/04_poisson_ch4_o.html#zero-inflated-poisson-model-1",
    "href": "slides/04_poisson_ch4_o.html#zero-inflated-poisson-model-1",
    "title": "Poisson Regression",
    "section": "Zero-inflated Poisson model",
    "text": "Zero-inflated Poisson model\nZero-inflated Poisson (ZIP) model has two parts\n. . .\n1️⃣ Association, among those who drink, between the mean number of drinks and predictors sex and off campus residence\n. . .\n\\[\\log(\\lambda) = \\beta_0 + \\beta_1 ~ off\\_campus + \\beta_2 ~ sex\\] where \\(\\lambda\\) is the mean number of drinks among those who drink\n. . .\n2️⃣ Probability that a student does not drink\n\\[\\text{logit}(\\alpha) = \\log\\Big(\\frac{\\alpha}{1- \\alpha}\\Big) = \\beta_0 + \\beta_1 ~ first\\_year\\]\nwhere \\(\\alpha\\) is the proportion of non-drinkers\n. . .\nNote: The same variables can be used in each component"
  },
  {
    "objectID": "slides/04_poisson_ch4_o.html#details-of-the-zip-model",
    "href": "slides/04_poisson_ch4_o.html#details-of-the-zip-model",
    "title": "Poisson Regression",
    "section": "Details of the ZIP model",
    "text": "Details of the ZIP model\n\nThe ZIP model is a special case of a latent variable model\n\nA type of mixture model where observations for one or more groups occur together but the group membership unknown\n\nZero-inflated models are a common type of mixture model; they apply beyond Poisson regression"
  },
  {
    "objectID": "slides/04_poisson_ch4_o.html#zip-model-in-r",
    "href": "slides/04_poisson_ch4_o.html#zip-model-in-r",
    "title": "Poisson Regression",
    "section": "ZIP model in R",
    "text": "ZIP model in R\nFit ZIP models using the zeroinfl function from the pscl R package.\n\nlibrary(pscl)\n\ndrinks_zip &lt;- zeroinfl(drinks ~ off_campus + sex | first_year,\n                data = drinks)\ndrinks_zip\n\n\nCall:\nzeroinfl(formula = drinks ~ off_campus + sex | first_year, data = drinks)\n\nCount model coefficients (poisson with log link):\n(Intercept)   off_campus         sexm  \n     0.7543       0.4159       1.0209  \n\nZero-inflation model coefficients (binomial with logit link):\n(Intercept)   first_year  \n    -0.6036       1.1364"
  },
  {
    "objectID": "slides/04_poisson_ch4_o.html#tidy-output",
    "href": "slides/04_poisson_ch4_o.html#tidy-output",
    "title": "Poisson Regression",
    "section": "Tidy output",
    "text": "Tidy output\nUse the tidy function from the poissonreg package for tidy model output.\n\nlibrary(poissonreg)\n\n. . .\nMean number of drinks among those who drink\n\ntidy(drinks_zip, type = \"count\") %&gt;% kable(digits = 3)\n\n\n\n\nterm\ntype\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\ncount\n0.754\n0.144\n5.238\n0.000\n\n\noff_campus\ncount\n0.416\n0.206\n2.021\n0.043\n\n\nsexm\ncount\n1.021\n0.175\n5.827\n0.000"
  },
  {
    "objectID": "slides/04_poisson_ch4_o.html#tidy-output-1",
    "href": "slides/04_poisson_ch4_o.html#tidy-output-1",
    "title": "Poisson Regression",
    "section": "Tidy output",
    "text": "Tidy output\nProportion of non-drinkers\n\ntidy(drinks_zip, type = \"zero\") %&gt;% kable(digits = 3)\n\n\n\n\nterm\ntype\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\nzero\n-0.604\n0.311\n-1.938\n0.053\n\n\nfirst_year\nzero\n1.136\n0.610\n1.864\n0.062"
  },
  {
    "objectID": "slides/04_poisson_ch4_o.html#interpreting-the-model-coefficients",
    "href": "slides/04_poisson_ch4_o.html#interpreting-the-model-coefficients",
    "title": "Poisson Regression",
    "section": "Interpreting the model coefficients",
    "text": "Interpreting the model coefficients\n\n\n\n\n\nterm\ntype\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\ncount\n0.754\n0.144\n5.238\n0.000\n\n\noff_campus\ncount\n0.416\n0.206\n2.021\n0.043\n\n\nsexm\ncount\n1.021\n0.175\n5.827\n0.000\n\n\n\n\n\n\nQuestions\n\nInterpret the intercept.\nInterpret the coefficients of off_campus and sexm."
  },
  {
    "objectID": "slides/04_poisson_ch4_o.html#estimated-proportion-zeros",
    "href": "slides/04_poisson_ch4_o.html#estimated-proportion-zeros",
    "title": "Poisson Regression",
    "section": "Estimated proportion zeros",
    "text": "Estimated proportion zeros\n\n\n\n\n\nterm\ntype\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\nzero\n-0.604\n0.311\n-1.938\n0.053\n\n\nfirst_year\nzero\n1.136\n0.610\n1.864\n0.062\n\n\n\n\n\nQuestions:\nBased on the model…\n\nWhat is the probability a first-year student is a non-drinker?\nWhat is the probability a upperclass student (sophomore, junior, senior) is a non-drinker?"
  },
  {
    "objectID": "slides/04_poisson_ch4_o.html#these-are-just-a-few-of-the-many-models",
    "href": "slides/04_poisson_ch4_o.html#these-are-just-a-few-of-the-many-models",
    "title": "Poisson Regression",
    "section": "These are just a few of the many models…",
    "text": "These are just a few of the many models…\n\nUse the Vuong Test to compare the fit of the ZIP model to a regular Poisson model\n\nWhy can’t we use a drop-in-deviance test?\n\nWe’ve just discussed the ZIP model here, but there are other model applications beyond the standard Poisson regression model (e.g., hurdle models, Zero-inflated Negative Binomial models, etc. )"
  },
  {
    "objectID": "slides/04_poisson_ch4_o.html#estimating-coefficients-in-poisson-model",
    "href": "slides/04_poisson_ch4_o.html#estimating-coefficients-in-poisson-model",
    "title": "Poisson Regression",
    "section": "Estimating coefficients in Poisson model",
    "text": "Estimating coefficients in Poisson model\n\nLeast squares estimation would not work because the normality and equal variance assumptions don’t hold for Poisson regression\nMaximum likelihood estimation is used to estimate the coefficients of Poisson regression.\nThe likelihood is the product of the probabilities for the \\(n\\) independent observations in the data"
  },
  {
    "objectID": "slides/04_poisson_ch4_o.html#likelihood-for-regular-poisson-model",
    "href": "slides/04_poisson_ch4_o.html#likelihood-for-regular-poisson-model",
    "title": "Poisson Regression",
    "section": "Likelihood for regular Poisson model",
    "text": "Likelihood for regular Poisson model\nLet’s go back to example about household size in the Philippines. We will focus on the model using the main effect of age to understand variability in mean household size.\n. . .\nSuppose the first five observations have household sizes of 4, 2, 8, 6, and 1. Then the likelihood is\n. . .\n\\(L = \\frac{e^{-\\lambda_1}\\lambda_1^4}{4!} * \\frac{e^{-\\lambda_2}\\lambda_2^2}{2!} * \\frac{e^{-\\lambda_3}\\lambda_3^8}{8!} *\n\\frac{e^{-\\lambda_4}\\lambda_4^6}{6!} * \\frac{e^{-\\lambda_5}\\lambda_5^1}{1!}\\)"
  },
  {
    "objectID": "slides/04_poisson_ch4_o.html#likelihood-for-regular-poisson-model-1",
    "href": "slides/04_poisson_ch4_o.html#likelihood-for-regular-poisson-model-1",
    "title": "Poisson Regression",
    "section": "Likelihood for regular Poisson model",
    "text": "Likelihood for regular Poisson model\nWe will use the log likelihood to make finding the MLE easier\n. . .\n\\(\\begin{aligned}\\log(L) &= -\\lambda_1 + 4\\log(\\lambda_1) - \\lambda_2 + 2\\log(\\lambda_2) - \\lambda_3 + 8\\log(\\lambda_3)\\\\ & -\\lambda_4 + 6 \\log(\\lambda_4) - \\lambda_5 + \\log(\\lambda_5) + C \\end{aligned}\\)\nwhere - \\(\\lambda\\) is the mean number in household depending on \\(x_i\\) - \\(C = -[\\log(4!) + \\log(2!) + \\log(8!) + \\log(6!)+ \\log(1!)]\\)"
  },
  {
    "objectID": "slides/04_poisson_ch4_o.html#likelihood-for-regular-poisson-model-2",
    "href": "slides/04_poisson_ch4_o.html#likelihood-for-regular-poisson-model-2",
    "title": "Poisson Regression",
    "section": "Likelihood for regular Poisson model",
    "text": "Likelihood for regular Poisson model\nGiven the age of the head of the household, we fit the model\n\\[\\log(\\lambda_i) = \\beta_0 + \\beta_1~age_i\\]\nThen we replace each \\(\\lambda_i\\) in \\(\\log(L)\\) with \\(e^{\\beta_0 + \\beta_1~age_i}\\).\n. . .\nSuppose the first five observations have ages \\(X = (32, 21, 55, 44, 28)\\). Then\n. . .\n\\[\\begin{aligned} \\log(L) &= [-e^{\\beta_0 + \\beta_132}+ 4(\\beta_0 + \\beta_1 32)] + [ - e^{\\beta_0 + \\beta_121} + 2(\\beta_0 + \\beta_121)] \\\\ &+  [- e^{\\beta_0 + \\beta_155} + 8(\\beta_0 + \\beta_155)] +  [-e^{\\beta_0 + \\beta_144} + 6(\\beta_0 + \\beta_144)] \\\\ &+ [-e^{\\beta_0 + \\beta_128}(\\beta_0 + \\beta_128)] + C \\end{aligned}\\]\n. . .\nUse search algorithm to find the values of \\(\\beta_0\\) and \\(\\beta_1\\) that maximize the above equation."
  },
  {
    "objectID": "slides/04_poisson_ch4_o.html#probabilities-under-zip-model",
    "href": "slides/04_poisson_ch4_o.html#probabilities-under-zip-model",
    "title": "Poisson Regression",
    "section": "Probabilities under ZIP model",
    "text": "Probabilities under ZIP model\nThere are three different types of observations in the data:\n\nObserved zero and will always be 0 (true zeros)\nObserved 0 but will not always be 0\nObserved non-zero count and will not always be 0"
  },
  {
    "objectID": "slides/04_poisson_ch4_o.html#probabilities-under-zip-model-1",
    "href": "slides/04_poisson_ch4_o.html#probabilities-under-zip-model-1",
    "title": "Poisson Regression",
    "section": "Probabilities under ZIP model",
    "text": "Probabilities under ZIP model\nTrue zeros\n\\[P(0 | \\text{true zero})= \\alpha\\]\n. . .\nObserved 0 but will not always be 0\n\\[P(0 | \\text{not always zero}) = (1 - \\alpha)\\frac{e^{-\\lambda}\\lambda^0}{0!}\\]\n. . .\nDid not observe 0 and will not always be 0\n\\[P(z_i | \\text{not always zero}) = (1 - \\alpha)\\frac{e^{-\\lambda}\\lambda^{z_i}}{z_i!}\\]"
  },
  {
    "objectID": "slides/04_poisson_ch4_o.html#probabilities-under-zip-model-2",
    "href": "slides/04_poisson_ch4_o.html#probabilities-under-zip-model-2",
    "title": "Poisson Regression",
    "section": "Probabilities under ZIP model",
    "text": "Probabilities under ZIP model\nPutting this all together. Let \\(y_i\\) be an observed response then\n\\[P(Y_i = y_i | x_i) = \\begin{cases}\n\\alpha + (1 - \\alpha)e^{-\\lambda_i} && \\text{ if } y_i = 0 \\\\\n(1 - \\alpha)\\frac{e^{-\\lambda_i}\\lambda_i^{y_i}}{y_i!} && \\text{ if } y_i &gt; 0\n\\end{cases}\\]\n. . .\nRecall from our example,\n\\[\\lambda_i = e^{\\beta_0 + \\beta_1~off\\_campus_i + \\beta_2 ~ sex_i}\\]\n\\[\\alpha_i = \\frac{e^{\\beta_{0\\alpha} + \\beta_{1\\alpha} ~ first\\_year_i}}{1 + e^{\\beta_{0\\alpha} + \\beta_{1\\alpha} ~ first\\_year_i}}\\]\n\nPlug in \\(\\lambda_i\\) and \\(\\alpha_i\\) into the above equation obtain the likelihood function"
  },
  {
    "objectID": "slides/04_poisson_ch4_o.html#acknowledgements",
    "href": "slides/04_poisson_ch4_o.html#acknowledgements",
    "title": "Poisson Regression",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThese slides are based on content in BMLR: Chapter 4\nInitial versions of the slides are by Dr. Maria Tackett, Duke University"
  },
  {
    "objectID": "slides/04_poisson_ch4.html#setup",
    "href": "slides/04_poisson_ch4.html#setup",
    "title": "Poisson Regression",
    "section": "Setup",
    "text": "Setup\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(GGally)\nlibrary(knitr)\nlibrary(patchwork)\nlibrary(viridis)\nlibrary(ggfortify)\nlibrary(gridExtra)"
  },
  {
    "objectID": "slides/04_poisson_ch4.html#learning-goals-12",
    "href": "slides/04_poisson_ch4.html#learning-goals-12",
    "title": "Poisson Regression",
    "section": "Learning goals (1/2)",
    "text": "Learning goals (1/2)\n\nDescribe properties of the Poisson random variable\nWrite the Poisson regression model\nDescribe how the Poisson regression differs from least-squares regression\nInterpret the coefficients for the Poisson regression model\nCompare two Poisson regression models\nDefine and calculate residuals for the Poisson regression model\nUse Goodness-of-fit to assess model fit"
  },
  {
    "objectID": "slides/04_poisson_ch4.html#learning-goals-22",
    "href": "slides/04_poisson_ch4.html#learning-goals-22",
    "title": "Poisson Regression",
    "section": "Learning goals (2/2)",
    "text": "Learning goals (2/2)\n\nIdentify overdispersion\nApply modeling approaches to deal with overdispersion\nExplore properties of negative binomial versus Poisson response\nFit and interpret models with offset to adjust for differences in sampling effort\nFit and interpret Zero-inflated Poisson models\nWrite likelihood for Poisson and Zero-inflated Poisson model"
  },
  {
    "objectID": "slides/04_poisson_ch4.html#scenarios-to-use-poisson-regression",
    "href": "slides/04_poisson_ch4.html#scenarios-to-use-poisson-regression",
    "title": "Poisson Regression",
    "section": "Scenarios to use Poisson regression",
    "text": "Scenarios to use Poisson regression\n\nDoes the number of employers conducting on-campus interviews during a year differ for public and private colleges?\nDoes the daily number of asthma-related visits to an Emergency Room differ depending on air pollution indices?\nDoes the number of paint defects per square foot of wall differ based on the years of experience of the painter?\n\n\nEach response variable is a count per a unit of time or space."
  },
  {
    "objectID": "slides/04_poisson_ch4.html#poisson-distribution",
    "href": "slides/04_poisson_ch4.html#poisson-distribution",
    "title": "Poisson Regression",
    "section": "Poisson distribution",
    "text": "Poisson distribution\nLet \\(Y\\) be the number of events in a given unit of time or space. Then \\(Y\\) can be modeled using a Poisson distribution\n\n\\[P(Y=y) = \\frac{e^{-\\lambda}\\lambda^y}{y!} \\hspace{10mm} y=0,1,2,\\ldots, \\infty\\]"
  },
  {
    "objectID": "slides/04_poisson_ch4.html#poisson-features",
    "href": "slides/04_poisson_ch4.html#poisson-features",
    "title": "Poisson Regression",
    "section": "Poisson Features",
    "text": "Poisson Features\n\n\\(E(Y) = Var(Y) = \\lambda\\)\nThe distribution is typically skewed right, particularly if \\(\\lambda\\) is small\nThe distribution becomes more symmetric as \\(\\lambda\\) increases\n\nIf \\(\\lambda\\) is sufficiently large, it can be approximated using a normal distribution (Click here for an example.)"
  },
  {
    "objectID": "slides/04_poisson_ch4.html#poisson-graphs",
    "href": "slides/04_poisson_ch4.html#poisson-graphs",
    "title": "Poisson Regression",
    "section": "Poisson Graphs",
    "text": "Poisson Graphs\n\nGraphsTable\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMean\nVariance\n\n\n\n\nlambda = 1\n0.99351\n0.9902178\n\n\nlambda = 5\n4.99367\n4.9865798\n\n\nlambda = 50\n49.99288\n49.8962683"
  },
  {
    "objectID": "slides/04_poisson_ch4.html#example",
    "href": "slides/04_poisson_ch4.html#example",
    "title": "Poisson Regression",
    "section": "Example",
    "text": "Example\nThe annual number of earthquakes registering at least 2.5 on the Richter Scale and having an epicenter within 40 miles of downtown Memphis follows a Poisson distribution with mean 6.5. What is the probability there will be at 3 or fewer such earthquakes next year?\n\n\\[P(Y \\leq 3) = P(Y = 0) + P(Y = 1) + P(Y = 2) + P(Y = 3)\\]\n\\[ = \\frac{e^{-6.5}6.5^0}{0!} + \\frac{e^{-6.5}6.5^1}{1!} + \\frac{e^{-6.5}6.5^2}{2!} + \\frac{e^{-6.5}6.5^3}{3!}\\]\n\\[ = 0.112\\]\n\n\n\nppois(3, 6.5)\n\n[1] 0.1118496\n\n\n\n\n\nExample adapted from Introduction to Probability Theory Example 28-2"
  },
  {
    "objectID": "slides/04_poisson_ch4.html#poisson-regression-1",
    "href": "slides/04_poisson_ch4.html#poisson-regression-1",
    "title": "Poisson Regression",
    "section": "Poisson regression",
    "text": "Poisson regression\nThe data: Household size in the Philippines\n\nThe data fHH1.csv come from the 2015 Family Income and Expenditure Survey conducted by the Philippine Statistics Authority.\nGoal: Understand the association between household size and various characteristics of the household\nResponse: - total: Number of people in the household other than the head\n\n\nPredictors: - location: Where the house is located - age: Age of the head of household - roof: Type of roof on the residence (proxy for wealth)\n\nOther variables: - numLT5: Number in the household under 5 years old"
  },
  {
    "objectID": "slides/04_poisson_ch4.html#the-data",
    "href": "slides/04_poisson_ch4.html#the-data",
    "title": "Poisson Regression",
    "section": "The data",
    "text": "The data\n\nhh_data &lt;- read_csv(\"data/fHH1.csv\")\nhh_data |&gt; slice(1:5) |&gt; kable()\n\n\n\n\nlocation\nage\ntotal\nnumLT5\nroof\n\n\n\n\nCentralLuzon\n65\n0\n0\nPredominantly Strong Material\n\n\nMetroManila\n75\n3\n0\nPredominantly Strong Material\n\n\nDavaoRegion\n54\n4\n0\nPredominantly Strong Material\n\n\nVisayas\n49\n3\n0\nPredominantly Strong Material\n\n\nMetroManila\n74\n3\n0\nPredominantly Strong Material"
  },
  {
    "objectID": "slides/04_poisson_ch4.html#response-variable",
    "href": "slides/04_poisson_ch4.html#response-variable",
    "title": "Poisson Regression",
    "section": "Response variable",
    "text": "Response variable\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmean\nvar\n\n\n\n\n3.685\n5.534"
  },
  {
    "objectID": "slides/04_poisson_ch4.html#why-the-least-squares-model-doesnt-work",
    "href": "slides/04_poisson_ch4.html#why-the-least-squares-model-doesnt-work",
    "title": "Poisson Regression",
    "section": "Why the least-squares model doesn’t work",
    "text": "Why the least-squares model doesn’t work\nThe goal is to model \\(\\lambda\\), the expected number of people in the household (other than the head), as a function of the predictors (covariates)\n\nWe might be tempted to try a linear model \\[\\lambda_i = \\beta_0 + \\beta_1x_{i1} + \\beta_2x_{i2} + \\dots + \\beta_px_{ip}\\]\n\n\nThis model won’t work because…\n\nIt could produce negative values of \\(\\lambda\\) for certain values of the predictors\nThe equal variance assumption required to conduct inference for linear regression is violated."
  },
  {
    "objectID": "slides/04_poisson_ch4.html#poisson-regression-model",
    "href": "slides/04_poisson_ch4.html#poisson-regression-model",
    "title": "Poisson Regression",
    "section": "Poisson regression model",
    "text": "Poisson regression model\nIf \\(Y_i \\sim Poisson\\) with \\(\\lambda = \\lambda_i\\) for the given values \\(x_{i1}, \\ldots, x_{ip}\\), then\n\\[\\log(\\lambda_i) = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\dots + \\beta_p x_{ip}\\]\n\nEach observation can have a different value of \\(\\lambda\\) based on its value of the predictors \\(x_1, \\ldots, x_p\\)\n\\(\\lambda\\) determines the mean and variance, so we don’t need to estimate a separate error term"
  },
  {
    "objectID": "slides/04_poisson_ch4.html#poisson-vs.-multiple-linear-regression",
    "href": "slides/04_poisson_ch4.html#poisson-vs.-multiple-linear-regression",
    "title": "Poisson Regression",
    "section": "Poisson vs. multiple linear regression",
    "text": "Poisson vs. multiple linear regression\n\n\n\n\n\nRegression models: Linear regression (left) and Poisson regression (right).\n\n\n\n\n\n\nFrom BMLR Figure 4.1"
  },
  {
    "objectID": "slides/04_poisson_ch4.html#assumptions-for-poisson-regression",
    "href": "slides/04_poisson_ch4.html#assumptions-for-poisson-regression",
    "title": "Poisson Regression",
    "section": "Assumptions for Poisson regression",
    "text": "Assumptions for Poisson regression\n\n\nPoisson response: The response variable is a count per unit of time or space, described by a Poisson distribution, at each level of the predictor(s)\nIndependence: The observations must be independent of one another\nMean = Variance: The mean must equal the variance\nLinearity: The log of the mean rate, \\(\\log(\\lambda)\\), must be a linear function of the predictor(s)"
  },
  {
    "objectID": "slides/04_poisson_ch4.html#model-1-number-in-household-vs.-age",
    "href": "slides/04_poisson_ch4.html#model-1-number-in-household-vs.-age",
    "title": "Poisson Regression",
    "section": "Model 1: Number in household vs. age",
    "text": "Model 1: Number in household vs. age\n\nmodel1 &lt;- glm(total ~ age, data = hh_data, family = poisson)\n\ntidy(model1) |&gt; \n  kable(digits = 4)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n1.5499\n0.0503\n30.8290\n0\n\n\nage\n-0.0047\n0.0009\n-5.0258\n0\n\n\n\n\n\n\\[\\log(\\hat{\\lambda}) = 1.5499  - 0.0047 ~ age\\]\n\nQuestion: The coefficient for age is -0.0047. Interpret this coefficient in context."
  },
  {
    "objectID": "slides/04_poisson_ch4.html#answers",
    "href": "slides/04_poisson_ch4.html#answers",
    "title": "Poisson Regression",
    "section": "Answers",
    "text": "Answers\n\nEach additional year older the head of household is, the estimated average log of the number of people in the household is .0047 lower.\nEach additional year older the head of household is, the estimated average number of people in the household reduces by 0.5%.\nEach additional year older the head of household is the estimated average number of people in the household changes by a factor of .995.\nFor every 10 years older the head of household is, the estimated average number of people in the household reduces by 5%."
  },
  {
    "objectID": "slides/04_poisson_ch4.html#understanding-the-interpretation",
    "href": "slides/04_poisson_ch4.html#understanding-the-interpretation",
    "title": "Poisson Regression",
    "section": "Understanding the interpretation",
    "text": "Understanding the interpretation\nLet’s derive the change in predicted mean when we go from \\(x\\) to \\(x+1\\)\n(see boardwork)"
  },
  {
    "objectID": "slides/04_poisson_ch4.html#is-the-coefficient-of-age-statistically-significant",
    "href": "slides/04_poisson_ch4.html#is-the-coefficient-of-age-statistically-significant",
    "title": "Poisson Regression",
    "section": "Is the coefficient of age statistically significant?",
    "text": "Is the coefficient of age statistically significant?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n1.5499\n0.0503\n30.8290\n0\n1.4512\n1.6482\n\n\nage\n-0.0047\n0.0009\n-5.0258\n0\n-0.0065\n-0.0029\n\n\n\n\n\n\\[H_0: \\beta_1 = 0 \\hspace{2mm} \\text{ vs. } \\hspace{2mm} H_a: \\beta_1 \\neq 0\\]\n\nTest statistic\n\\[Z = \\frac{\\hat{\\beta}_1 - 0}{SE(\\hat{\\beta}_1)} = \\frac{-0.0047 - 0}{0.0009} = -5.026 \\text{ (using exact values)}\\]\n\n\nP-value\n\\[P(|Z| &gt; |-5.026|) = 5.01 \\times 10^{-7} \\approx 0\\]"
  },
  {
    "objectID": "slides/04_poisson_ch4.html#what-are-plausible-values-for-the-coefficient-of-age",
    "href": "slides/04_poisson_ch4.html#what-are-plausible-values-for-the-coefficient-of-age",
    "title": "Poisson Regression",
    "section": "What are plausible values for the coefficient of age?",
    "text": "What are plausible values for the coefficient of age?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n1.5499\n0.0503\n30.8290\n0\n1.4512\n1.6482\n\n\nage\n-0.0047\n0.0009\n-5.0258\n0\n-0.0065\n-0.0029\n\n\n\n\n\n95% confidence interval for the coefficient of age\n\\[\\hat{\\beta}_1 \\pm Z^{*}\\times SE(\\hat{\\beta}_1)\\] \\[-0.0047 \\pm 1.96 \\times 0.0009 = \\mathbf{(-.0065, -0.0029)}\\]\n\nQuestion: Interpret the interval in terms of the change in mean household size."
  },
  {
    "objectID": "slides/04_poisson_ch4.html#which-can-best-help-us-determine-whether-model-1-is-a-good-fit",
    "href": "slides/04_poisson_ch4.html#which-can-best-help-us-determine-whether-model-1-is-a-good-fit",
    "title": "Poisson Regression",
    "section": "Which can best help us determine whether Model 1 is a good fit?",
    "text": "Which can best help us determine whether Model 1 is a good fit?\n\nPlotsCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\np1 &lt;- ggplot(data = hh_data, aes(x = age, y = total)) + \n  geom_point() + \n  labs(y = \"Total household size\", \n       title = \"Plot A\")\n\np2 &lt;- hh_data |&gt;\n  group_by(age) |&gt; \n  summarise(mean = mean(total)) |&gt;\n  ggplot(aes(x = age, y = mean))+ \n  geom_point() + \n  labs(y = \"Empirical mean household size\", \n       title = \"Plot B\")\n\np3 &lt;- hh_data |&gt;\n  group_by(age) |&gt; \n  summarise(log_mean = log(mean(total))) |&gt;\n  ggplot(aes(x = age, y = log_mean)) + \n  geom_point() + \n  labs(y = \"Log empirical mean household size\", \n       title = \"Plot C\")\n\np1 + p2 + p3 + plot_annotation(tag_levels = 'A')"
  },
  {
    "objectID": "slides/04_poisson_ch4.html#model-2-add-a-quadratic-effect-for-age",
    "href": "slides/04_poisson_ch4.html#model-2-add-a-quadratic-effect-for-age",
    "title": "Poisson Regression",
    "section": "Model 2: Add a quadratic effect for age",
    "text": "Model 2: Add a quadratic effect for age\n\nhh_data &lt;- hh_data |&gt; \n  mutate(age2 = age*age)\n\nmodel2 &lt;- glm(total ~ age + age2, data = hh_data, family = poisson)\ntidy(model2, conf.int = T) |&gt; \n  kable(digits = 4)\n\n\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n-0.3325\n0.1788\n-1.8594\n0.063\n-0.6863\n0.0148\n\n\nage\n0.0709\n0.0069\n10.2877\n0.000\n0.0575\n0.0845\n\n\nage2\n-0.0007\n0.0001\n-11.0578\n0.000\n-0.0008\n-0.0006"
  },
  {
    "objectID": "slides/04_poisson_ch4.html#model-2-add-a-quadratic-effect-for-age-1",
    "href": "slides/04_poisson_ch4.html#model-2-add-a-quadratic-effect-for-age-1",
    "title": "Poisson Regression",
    "section": "Model 2: Add a quadratic effect for age",
    "text": "Model 2: Add a quadratic effect for age\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n-0.3325\n0.1788\n-1.8594\n0.063\n-0.6863\n0.0148\n\n\nage\n0.0709\n0.0069\n10.2877\n0.000\n0.0575\n0.0845\n\n\nage2\n-0.0007\n0.0001\n-11.0578\n0.000\n-0.0008\n-0.0006\n\n\n\n\n\nWe can determine whether to keep \\(age^2\\) in the model in two ways:\n1️⃣ Use the p-value (or confidence interval) for the coefficient (since we are adding a single term to the model)\n2️⃣ Conduct a drop-in-deviance test"
  },
  {
    "objectID": "slides/04_poisson_ch4.html#deviance",
    "href": "slides/04_poisson_ch4.html#deviance",
    "title": "Poisson Regression",
    "section": "Deviance",
    "text": "Deviance\nA deviance is a way to measure how the observed data deviates from the model predictions.\n\nIt’s a measure unexplained variability in the response variable (similar to SSE in linear regression )\nLower deviance means the model is a better fit to the data\n\n\nWe can calculate the “deviance residual” for each observation in the data (more on the formula later). Let \\((\\text{deviance residual}_i\\) be the deviance residual for the \\(i^{th}\\) observation, then\n\\[\\text{deviance} = \\sum(\\text{deviance residual})_i^2\\]\n\n\nNote: Deviance is also known as the “residual deviance”"
  },
  {
    "objectID": "slides/04_poisson_ch4.html#drop-in-deviance-test",
    "href": "slides/04_poisson_ch4.html#drop-in-deviance-test",
    "title": "Poisson Regression",
    "section": "Drop-in-Deviance Test",
    "text": "Drop-in-Deviance Test\nWe can use a drop-in-deviance test to compare two models. To conduct the test\n1️⃣ Compute the deviance for each model\n2️⃣ Calculate the drop in deviance\n\\[\\text{drop-in-deviance =  Deviance(reduced model) - Deviance(larger model)}\\]\n\n3️⃣ Given the reduced model is the true model \\((H_0 \\text{ true})\\), then \\[\\text{drop-in-deviance} \\sim \\chi_d^2\\]\nwhere \\(d\\) is the difference in degrees of freedom between the two models (i.e., the difference in number of terms)"
  },
  {
    "objectID": "slides/04_poisson_ch4.html#drop-in-deviance---model1-and-model2",
    "href": "slides/04_poisson_ch4.html#drop-in-deviance---model1-and-model2",
    "title": "Poisson Regression",
    "section": "Drop-in-deviance - Model1 and Model2",
    "text": "Drop-in-deviance - Model1 and Model2\n\nanova(model1, model2, test = \"Chisq\") |&gt;\n  kable(digits = 3)\n\n\n\n\nResid. Df\nResid. Dev\nDf\nDeviance\nPr(&gt;Chi)\n\n\n\n\n1498\n2337.089\nNA\nNA\nNA\n\n\n1497\n2200.944\n1\n136.145\n0\n\n\n\n\n\n\nQuestions:\n\nWrite the null and alternative hypotheses.\nWhat does the value 2337.089 tell you?\nWhat does the value 1 tell you?\nWhat is your conclusion?"
  },
  {
    "objectID": "slides/04_poisson_ch4.html#add-location-to-the-model",
    "href": "slides/04_poisson_ch4.html#add-location-to-the-model",
    "title": "Poisson Regression",
    "section": "Add location to the model?",
    "text": "Add location to the model?\nSuppose we want to add location to the model, so we compare the following models:\nModel A: \\(\\lambda_i = \\beta_0 + \\beta_1 ~ age_i + \\beta_2 ~ age_i^2\\)\nModel B: \\(\\lambda_i =  \\beta_0 + \\beta_1 ~ age_i + \\beta_2 ~ age_i^2 + \\beta_3 ~ Loc1_i + \\beta_4 ~ Loc2_i + \\beta_5 ~ Loc3_i + \\beta_6 ~ Loc4_i\\)"
  },
  {
    "objectID": "slides/04_poisson_ch4.html#question",
    "href": "slides/04_poisson_ch4.html#question",
    "title": "Poisson Regression",
    "section": "Question",
    "text": "Question\nWhich of the following are reliable ways to determine if location should be added to the model?\n\n\nDrop-in-deviance test\nUse the p-value for each coefficient\nLikelihood ratio test\nNested F Test\nBIC"
  },
  {
    "objectID": "slides/04_poisson_ch4.html#add-location-to-the-model-1",
    "href": "slides/04_poisson_ch4.html#add-location-to-the-model-1",
    "title": "Poisson Regression",
    "section": "Add location to the model?",
    "text": "Add location to the model?\n\nmodel3 &lt;- glm(total ~ age + age2 + location, data = hh_data, family = poisson)\n\n\nUse a drop-in-deviance test to determine if Model 2 or Model 3 (with location) is a better fit for the data.\n\n\n\nanova(model2, model3, test = \"Chisq\") |&gt;\n  kable(digits = 3)\n\n\n\n\nResid. Df\nResid. Dev\nDf\nDeviance\nPr(&gt;Chi)\n\n\n\n\n1497\n2200.944\nNA\nNA\nNA\n\n\n1493\n2187.800\n4\n13.144\n0.011\n\n\n\n\n\nThe p-value is small (0.01 &lt; 0.05), so we conclude that Model 3 is a better fit for the data."
  },
  {
    "objectID": "slides/04_poisson_ch4.html#model-3",
    "href": "slides/04_poisson_ch4.html#model-3",
    "title": "Poisson Regression",
    "section": "Model 3",
    "text": "Model 3\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n-0.3843\n0.1821\n-2.1107\n0.0348\n-0.7444\n-0.0306\n\n\nage\n0.0704\n0.0069\n10.1900\n0.0000\n0.0569\n0.0840\n\n\nage2\n-0.0007\n0.0001\n-10.9437\n0.0000\n-0.0008\n-0.0006\n\n\nlocationDavaoRegion\n-0.0194\n0.0538\n-0.3605\n0.7185\n-0.1250\n0.0859\n\n\nlocationIlocosRegion\n0.0610\n0.0527\n1.1580\n0.2468\n-0.0423\n0.1641\n\n\nlocationMetroManila\n0.0545\n0.0472\n1.1542\n0.2484\n-0.0378\n0.1473\n\n\nlocationVisayas\n0.1121\n0.0417\n2.6853\n0.0072\n0.0308\n0.1945\n\n\n\n\n\n\nDoes this model sufficiently explain the variability in the mean household size?"
  },
  {
    "objectID": "slides/04_poisson_ch4.html#pearson-residuals",
    "href": "slides/04_poisson_ch4.html#pearson-residuals",
    "title": "Poisson Regression",
    "section": "Pearson residuals",
    "text": "Pearson residuals\nWe can calculate two types of residuals for Poisson regression: Pearson residuals and deviance residuals\n\n\\[\\text{Pearson residual}_i = \\frac{\\text{observed} - \\text{predicted}}{\\text{std. error}} = \\frac{y_i - \\hat{\\lambda}_i}{\\sqrt{\\hat{\\lambda}_i}}\\]\n\n\n\nSimilar interpretation as standardized residuals from linear regression\nExpect most to fall between -2 and 2\nUsed to calculate overdispersion parameter"
  },
  {
    "objectID": "slides/04_poisson_ch4.html#deviance-residuals",
    "href": "slides/04_poisson_ch4.html#deviance-residuals",
    "title": "Poisson Regression",
    "section": "Deviance residuals",
    "text": "Deviance residuals\nThe deviance residual indicates how much the observed data deviates from the fitted model\n\\[\\text{deviance residual}_i = \\text{sign}(y_i - \\hat{\\lambda}_i)\\sqrt{2\\Bigg[y_i\\log\\bigg(\\frac{y_i}{\\hat{\\lambda}_i}\\bigg) - (y_i - \\hat{\\lambda}_i)\\Bigg]}\\]\nwhere\n\\[\\text{sign}(y_i - \\hat{\\lambda}_i)  =  \\begin{cases}\n1 & \\text{ if }(y_i - \\hat{\\lambda}_i) &gt; 0 \\\\\n-1 & \\text{ if }(y_i - \\hat{\\lambda}_i) &lt; 0 \\\\\n0 & \\text{ if }(y_i - \\hat{\\lambda}_i) = 0\n\\end{cases}\\]"
  },
  {
    "objectID": "slides/04_poisson_ch4.html#model-3-residual-plots",
    "href": "slides/04_poisson_ch4.html#model-3-residual-plots",
    "title": "Poisson Regression",
    "section": "Model 3: Residual plots",
    "text": "Model 3: Residual plots\n\nmodel3_aug_pearson &lt;- augment(model3, type.residuals = \"pearson\") \nmodel3_aug_deviance &lt;- augment(model3, type.residuals = \"deviance\")\n\n\nPlotsCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\np1 &lt;- ggplot(data = model3_aug_pearson, aes(x = .fitted, y = .resid)) + \n  geom_point()  + \n  geom_smooth() + \n  labs(x = \"Fitted values\", \n       y = \"Pearson residuals\", \n       title = \"Pearson residuals vs. fitted\")\n\np2 &lt;-  ggplot(data = model3_aug_deviance, aes(x = .fitted, y = .resid)) + \n  geom_point()  + \n  geom_smooth() + \n  labs(x = \"Fitted values\", \n       y = \"Deviance residuals\", \n       title = \"Deviance residuals vs. fitted\")\n\np1 + p2"
  },
  {
    "objectID": "slides/04_poisson_ch4.html#goodness-of-fit-1",
    "href": "slides/04_poisson_ch4.html#goodness-of-fit-1",
    "title": "Poisson Regression",
    "section": "Goodness-of-fit",
    "text": "Goodness-of-fit\n\nGoal: Use the (residual) deviance to assess how much the predicted values differ from the observed values. Recall \\((\\text{deviance}) = \\sum_{i=1}^{n}(\\text{deviance residual})_i^2\\)\nIf the model sufficiently fits the data, then\n\n\\[\\text{deviance} \\sim \\chi^2_{df}\\]\nwhere \\(df\\) is the model’s residual degrees of freedom\n\n\nQuestion: What is the probability of observing a deviance larger than the one we’ve observed, given this model sufficiently fits the data?\n\n\n\n\\[P(\\chi^2_{df} &gt; \\text{ deviance})\\]"
  },
  {
    "objectID": "slides/04_poisson_ch4.html#model-3-goodness-of-fit-calculations",
    "href": "slides/04_poisson_ch4.html#model-3-goodness-of-fit-calculations",
    "title": "Poisson Regression",
    "section": "Model 3: Goodness-of-fit calculations",
    "text": "Model 3: Goodness-of-fit calculations\n\nmodel3$deviance\n\n[1] 2187.8\n\nmodel3$df.residual\n\n[1] 1493\n\n\n\npchisq(model3$deviance, model3$df.residual, lower.tail = FALSE)\n\n[1] 3.153732e-29\n\n\nThe probability of observing a deviance greater than 2187.8 is \\(\\approx 0\\), so there is significant evidence of lack-of-fit."
  },
  {
    "objectID": "slides/04_poisson_ch4.html#lack-of-fit",
    "href": "slides/04_poisson_ch4.html#lack-of-fit",
    "title": "Poisson Regression",
    "section": "Lack-of-fit",
    "text": "Lack-of-fit\nThere are a few potential reasons for lack-of-fit:\n\nMissing important interactions or higher-order terms\nMissing important variables (perhaps this means a more comprehensive data set is required)\nThere could be extreme observations causing the deviance to be larger than expected (assess based on the residual plots)\nThere could be a problem with the Poisson model\n\nMay need more flexibility in the model to handle overdispersion"
  },
  {
    "objectID": "slides/04_poisson_ch4.html#overdispersion",
    "href": "slides/04_poisson_ch4.html#overdispersion",
    "title": "Poisson Regression",
    "section": "Overdispersion",
    "text": "Overdispersion\nOverdispersion: There is more variability in the response than what is implied by the Poisson model\n\nTablesCode\n\n\n\n\nOverall\n\n\n\n\n\nmean\nvar\n\n\n\n\n3.685\n5.534\n\n\n\n\n\n\nby Location\n\n\n\n\n\nlocation\nmean\nvar\n\n\n\n\nCentralLuzon\n3.402\n4.152\n\n\nDavaoRegion\n3.390\n4.723\n\n\nIlocosRegion\n3.586\n5.402\n\n\nMetroManila\n3.707\n4.863\n\n\nVisayas\n3.902\n6.602\n\n\n\n\n\n\n\n\n\nhh_data |&gt;\n  summarise(mean = mean(total), var = var(total)) |&gt;\n  kable(digits = 3)\n\n\nhh_data |&gt;\n  group_by(location) |&gt;\n  summarise(mean = mean(total), var = var(total)) |&gt;\n  kable(digits = 3)"
  },
  {
    "objectID": "slides/04_poisson_ch4.html#why-overdispersion-matters",
    "href": "slides/04_poisson_ch4.html#why-overdispersion-matters",
    "title": "Poisson Regression",
    "section": "Why overdispersion matters",
    "text": "Why overdispersion matters\nIf there is overdispersion, then there is more variation in the response than what’s implied by a Poisson model. This means\n❌ The standard errors of the model coefficients are artificially small\n❌ The p-values are artificially small\n❌ This could lead to models that are more complex than what is needed\n\nWe can take overdispersion into account by\n\ninflating standard errors by multiplying them by a dispersion factor\nusing a negative-binomial regression model"
  },
  {
    "objectID": "slides/04_poisson_ch4.html#dispersion-parameter",
    "href": "slides/04_poisson_ch4.html#dispersion-parameter",
    "title": "Poisson Regression",
    "section": "Dispersion parameter",
    "text": "Dispersion parameter\nThe dispersion parameter is represented by \\(\\phi\\)\n\\[\\hat{\\phi} =\\frac{\\text{deviance}}{\\text{residual df}} = \\frac{\\sum_{i=1}^{n}(\\text{Pearson residuals})^2}{n - p}\\]\nwhere \\(p\\) is the number of terms in the model (including the intercept)\n\nIf there is no overdispersion \\(\\hat{\\phi} = 1\\)\nIf there is overdispersion \\(\\hat{\\phi} &gt;  1\\)"
  },
  {
    "objectID": "slides/04_poisson_ch4.html#accounting-for-dispersion-in-the-model",
    "href": "slides/04_poisson_ch4.html#accounting-for-dispersion-in-the-model",
    "title": "Poisson Regression",
    "section": "Accounting for dispersion in the model",
    "text": "Accounting for dispersion in the model\nWe inflate the standard errors of the coefficient by multiplying the variance by \\(\\hat{\\phi}\\)\n\n\\[SE_{Q}(\\hat{\\beta}) = \\sqrt{\\hat{\\phi}}  * SE(\\hat{\\beta})\\] - “Q” stands for quasi-Poisson, since this is an ad-hoc solution - The process for model building and model comparison is called quasilikelihood (similar to likelihood without exact underlying distributions)"
  },
  {
    "objectID": "slides/04_poisson_ch4.html#model-3-quasi-poisson-model",
    "href": "slides/04_poisson_ch4.html#model-3-quasi-poisson-model",
    "title": "Poisson Regression",
    "section": "Model 3: Quasi-Poisson model",
    "text": "Model 3: Quasi-Poisson model\n\nmodel3_q &lt;- glm(total ~ age + age2 + location, data = hh_data, \n                family = quasipoisson) #&lt;&lt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n-0.3843\n0.2166\n-1.7744\n0.0762\n-0.8134\n0.0358\n\n\nage\n0.0704\n0.0082\n8.5665\n0.0000\n0.0544\n0.0866\n\n\nage2\n-0.0007\n0.0001\n-9.2000\n0.0000\n-0.0009\n-0.0006\n\n\nlocationDavaoRegion\n-0.0194\n0.0640\n-0.3030\n0.7619\n-0.1451\n0.1058\n\n\nlocationIlocosRegion\n0.0610\n0.0626\n0.9735\n0.3304\n-0.0620\n0.1837\n\n\nlocationMetroManila\n0.0545\n0.0561\n0.9703\n0.3320\n-0.0552\n0.1649\n\n\nlocationVisayas\n0.1121\n0.0497\n2.2574\n0.0241\n0.0156\n0.2103"
  },
  {
    "objectID": "slides/04_poisson_ch4.html#poisson-vs.-q-poisson",
    "href": "slides/04_poisson_ch4.html#poisson-vs.-q-poisson",
    "title": "Poisson Regression",
    "section": "Poisson vs. Q-Poisson",
    "text": "Poisson vs. Q-Poisson\n\n\nPoisson\n\n\n\n\n\nterm\nestimate\nstd.error\n\n\n\n\n(Intercept)\n-0.3843\n0.1821\n\n\nage\n0.0704\n0.0069\n\n\nage2\n-0.0007\n0.0001\n\n\nlocationDavaoRegion\n-0.0194\n0.0538\n\n\nlocationIlocosRegion\n0.0610\n0.0527\n\n\nlocationMetroManila\n0.0545\n0.0472\n\n\nlocationVisayas\n0.1121\n0.0417\n\n\n\n\n\n\nQuasi-Poisson\n\n\n\n\n\nestimate\nstd.error\n\n\n\n\n-0.3843\n0.2166\n\n\n0.0704\n0.0082\n\n\n-0.0007\n0.0001\n\n\n-0.0194\n0.0640\n\n\n0.0610\n0.0626\n\n\n0.0545\n0.0561\n\n\n0.1121\n0.0497"
  },
  {
    "objectID": "slides/04_poisson_ch4.html#q-poisson-inference-for-coefficients",
    "href": "slides/04_poisson_ch4.html#q-poisson-inference-for-coefficients",
    "title": "Poisson Regression",
    "section": "Q-Poisson: Inference for coefficients",
    "text": "Q-Poisson: Inference for coefficients\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\n\n\n\n\n(Intercept)\n-0.3843\n0.2166\n\n\nage\n0.0704\n0.0082\n\n\nage2\n-0.0007\n0.0001\n\n\nlocationDavaoRegion\n-0.0194\n0.0640\n\n\nlocationIlocosRegion\n0.0610\n0.0626\n\n\nlocationMetroManila\n0.0545\n0.0561\n\n\nlocationVisayas\n0.1121\n0.0497\n\n\n\n\n\n\nTest statistic \\[t = \\frac{\\hat{\\beta} - 0}{SE_{Q}(\\hat{\\beta})} \\sim t_{n-p}\\]"
  },
  {
    "objectID": "slides/04_poisson_ch4.html#negative-binomial-regression-model-1",
    "href": "slides/04_poisson_ch4.html#negative-binomial-regression-model-1",
    "title": "Poisson Regression",
    "section": "Negative binomial regression model",
    "text": "Negative binomial regression model\nAnother approach to handle overdispersion is to use a negative binomial regression model\n\nThis has more flexibility than the quasi-Poisson model, because there is a new parameter in addition to \\(\\lambda\\)\n\n\n\nLet \\(Y\\) be a negative binomial random variable, \\(Y\\sim NegBinom(r, p)\\), then\n\\[P(Y = y_i) = {y_i + r - 1 \\choose r - 1}(1-p)^{y_i}p^r \\hspace{5mm} y_i = 0, 1, 2, \\ldots, \\infty\\]"
  },
  {
    "objectID": "slides/04_poisson_ch4.html#negative-binomial-regression-model-2",
    "href": "slides/04_poisson_ch4.html#negative-binomial-regression-model-2",
    "title": "Poisson Regression",
    "section": "Negative binomial regression model",
    "text": "Negative binomial regression model\n\nMain idea: Generate a \\(\\lambda\\) for each observation (household) and generate a count using the Poisson random variable with parameter \\(\\lambda\\)\n\nMakes the counts more dispersed than with a single parameter\n\nThink of it as a Poisson model such that \\(\\lambda\\) is also random\n\n\n\\(\\begin{aligned} &\\text{If }Y|\\lambda \\sim Poisson(\\lambda)\\\\\n&\\text{ and } \\lambda \\sim Gamma\\bigg(r, \\frac{1-p}{p}\\bigg)\\\\\n&\\text{ then } Y \\sim NegBinom(r, p)\\end{aligned}\\)"
  },
  {
    "objectID": "slides/04_poisson_ch4.html#negative-binomial-regression-in-r",
    "href": "slides/04_poisson_ch4.html#negative-binomial-regression-in-r",
    "title": "Poisson Regression",
    "section": "Negative binomial regression in R",
    "text": "Negative binomial regression in R\n\nlibrary(MASS)\nmodel3_nb &lt;- glm.nb(total ~ age + age2 + location, data = hh_data)\ntidy(model3_nb) |&gt; \n  kable(digits = 4)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-0.3753\n0.2076\n-1.8081\n0.0706\n\n\nage\n0.0699\n0.0079\n8.8981\n0.0000\n\n\nage2\n-0.0007\n0.0001\n-9.5756\n0.0000\n\n\nlocationDavaoRegion\n-0.0219\n0.0625\n-0.3501\n0.7262\n\n\nlocationIlocosRegion\n0.0577\n0.0615\n0.9391\n0.3477\n\n\nlocationMetroManila\n0.0562\n0.0551\n1.0213\n0.3071\n\n\nlocationVisayas\n0.1104\n0.0487\n2.2654\n0.0235"
  },
  {
    "objectID": "slides/04_poisson_ch4.html#data-airbnbs-in-nyc",
    "href": "slides/04_poisson_ch4.html#data-airbnbs-in-nyc",
    "title": "Poisson Regression",
    "section": "Data: Airbnbs in NYC",
    "text": "Data: Airbnbs in NYC\nThe data set NYCairbnb-sample.csv contains information about a random sample of 1000 Airbnbs in New York City. It is a subset of the data on 40628 Airbnbs scraped by Awad et al. (2017).\nVariables\n\nnumber_of_reviews: Number of reviews for the unit on Airbnb (proxy for number of rentals)\nprice: price per night in US dollars\nroom_type: Entire home/apartment, private room, or shared room\ndays: Number of days the unit has been listed (date when info scraped - date when unit first listed on Airbnb)\n\n\n\nData set pulled from BMLR Section 4.11.3."
  },
  {
    "objectID": "slides/04_poisson_ch4.html#data-airbnbs-in-nyc-1",
    "href": "slides/04_poisson_ch4.html#data-airbnbs-in-nyc-1",
    "title": "Poisson Regression",
    "section": "Data: Airbnbs in NYC",
    "text": "Data: Airbnbs in NYC\n\nairbnb &lt;- read_csv(\"data/NYCairbnb-sample.csv\") |&gt;\n  dplyr::select(id, number_of_reviews, days, room_type, price)\n\n\n\n\n\n\nid\nnumber_of_reviews\ndays\nroom_type\nprice\n\n\n\n\n15756544\n16\n1144\nPrivate room\n120\n\n\n14218251\n15\n471\nPrivate room\n89\n\n\n21644\n0\n2600\nPrivate room\n89\n\n\n13667835\n1\n283\nEntire home/apt\n150\n\n\n265912\n0\n1970\nEntire home/apt\n89\n\n\n\n\n\nGoal: Use the price and room type of Airbnbs to describe variation in the number of reviews (a proxy for number of rentals)."
  },
  {
    "objectID": "slides/04_poisson_ch4.html#eda",
    "href": "slides/04_poisson_ch4.html#eda",
    "title": "Poisson Regression",
    "section": "EDA",
    "text": "EDA\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\np1 &lt;- ggplot(data = airbnb, aes(x = number_of_reviews)) + \n  geom_histogram() + \n   labs(x = \"Number of reviews\",\n    title = \"Distribution of number of reviews\")\n\np2 &lt;- airbnb |&gt;\n  filter(price &lt;= 2000) |&gt;\n  group_by(price) |&gt;\n  summarise(log_mean = log(mean(number_of_reviews))) |&gt;\n  ggplot(aes(x = price, y = log_mean)) + \n  geom_point(alpha= 0.7) + \n  geom_smooth() + \n  labs(x = \"Price in  US dollars\",\n    y = \"Log(mean # reviews)\", \n    title = \"Log mean #  of reviews vs. price\", \n    subtitle = \"Airbnbs $2000 or less\")\n\np3 &lt;- airbnb |&gt;\n  filter(price &lt;= 500) |&gt;\n  group_by(price) |&gt;\n  summarise(log_mean = log(mean(number_of_reviews))) |&gt;\n  ggplot(aes(x = price, y = log_mean)) + \n  geom_point(alpha= 0.7) + \n  geom_smooth() + \n  labs(x = \"Price in  US dollars\",\n    y = \"Log(mean # reviews)\", \n    title = \"Log mean # of reviews vs. price\", \n    subtitle = \"Airbnbs $500 or less\")\n\np1  / (p2 + p3)"
  },
  {
    "objectID": "slides/04_poisson_ch4.html#eda-1",
    "href": "slides/04_poisson_ch4.html#eda-1",
    "title": "Poisson Regression",
    "section": "EDA",
    "text": "EDA\n\n\nOverall\n\n\n\n\n\nmean\nvar\n\n\n\n\n15.916\n765.969\n\n\n\n\n\n\nby Room type\n\n\n\n\n\nroom_type\nmean\nvar\n\n\n\n\nEntire home/apt\n16.283\n760.348\n\n\nPrivate room\n15.608\n786.399\n\n\nShared room\n15.028\n605.971"
  },
  {
    "objectID": "slides/04_poisson_ch4.html#considerations-for-modeling",
    "href": "slides/04_poisson_ch4.html#considerations-for-modeling",
    "title": "Poisson Regression",
    "section": "Considerations for modeling",
    "text": "Considerations for modeling\nWe would like to fit the Poisson regression model\n\\[\\log(\\lambda) = \\beta_0 + \\beta_1 ~ price + \\beta_2 ~ room\\_type1 + \\beta_3 ~ room\\_type2\\]\n\nQuestion: - Based on the EDA, what are some potential issues we may want to address in the model building?\n\nSuppose any model fit issues are addressed. What are some potential limitations to the conclusions and interpretations from the model?"
  },
  {
    "objectID": "slides/04_poisson_ch4.html#offset",
    "href": "slides/04_poisson_ch4.html#offset",
    "title": "Poisson Regression",
    "section": "Offset",
    "text": "Offset\n\nSometimes counts are not directly comparable because the observations differ based on some characteristic directly related to the counts, i.e. the sampling effort.\nAn offset can be used to adjust for differences in sampling effort.\n\n\n\nLet \\(x_{offset}\\) be the variable that accounts for differences in sampling effort, then \\(\\log(x_{offset})\\) will be added to the model.\n\n\n\n\\(\\log(\\lambda_i) = \\beta_0 + \\beta_1 ~ x_{i1} + \\beta_2 ~ x_{i2} + ... + \\beta_p ~ x_{ip} + \\log(x_{offset_i})\\)\n\nThe offset is a term in the model with coefficient always equal to 1."
  },
  {
    "objectID": "slides/04_poisson_ch4.html#adding-an-offset-to-the-airbnb-model",
    "href": "slides/04_poisson_ch4.html#adding-an-offset-to-the-airbnb-model",
    "title": "Poisson Regression",
    "section": "Adding an offset to the Airbnb model",
    "text": "Adding an offset to the Airbnb model\nWe will add the offset \\(\\log(days)\\) to the model. This accounts for the fact that we would expect Airbnbs that have been listed longer to have more reviews.\n\\[\\log(\\lambda_i) = \\beta_0 + \\beta_1 ~ price_i + \\beta_2 ~ room\\_type1_i + \\beta_3 ~ room\\_type2_i + \\log(days_i)\\] \nNote: The response variable for the model is still \\(\\log(\\lambda_i)\\), the log mean number of reviews"
  },
  {
    "objectID": "slides/04_poisson_ch4.html#detail-on-the-offset",
    "href": "slides/04_poisson_ch4.html#detail-on-the-offset",
    "title": "Poisson Regression",
    "section": "Detail on the offset",
    "text": "Detail on the offset\nWe want to adjust for the number of days, so we are interested in \\(\\frac{reviews}{days}\\).\n\nGiven \\(\\lambda\\) is the mean number of reviews\n\\[\\log\\Big(\\frac{\\lambda}{days}\\Big) = \\beta_0 + \\beta_1 ~ price + \\beta_2 ~ room\\_type1 + \\beta_3 ~ room\\_type2\\]\n\n\n\\[\\Rightarrow \\log({\\lambda}) - \\log(days) = \\beta_0 + \\beta_1 ~ price + \\beta_2 ~ room\\_type1 + \\beta_3 ~ room\\_type2\\]\n\n\n\\[\\Rightarrow \\log({\\lambda}) = \\beta_0 + \\beta_1 ~ price + \\beta_2 ~ room\\_type1 + \\beta_3 ~ room\\_type2 + \\log(days)\\]"
  },
  {
    "objectID": "slides/04_poisson_ch4.html#airbnb-model-in-r",
    "href": "slides/04_poisson_ch4.html#airbnb-model-in-r",
    "title": "Poisson Regression",
    "section": "Airbnb model in R",
    "text": "Airbnb model in R\n\nairbnb_model &lt;- glm(number_of_reviews ~ price + room_type, \n                    data = airbnb, family = poisson, \n                    offset = log(days)) #&lt;&lt;\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-4.1351\n0.0170\n-243.1397\n0\n\n\nprice\n-0.0005\n0.0001\n-7.0952\n0\n\n\nroom_typePrivate room\n-0.0994\n0.0174\n-5.6986\n0\n\n\nroom_typeShared room\n0.2436\n0.0452\n5.3841\n0\n\n\n\n\n\n\nThe coefficient for \\(\\log(days)\\) is fixed at 1, so it is not in the model output."
  },
  {
    "objectID": "slides/04_poisson_ch4.html#interpretations",
    "href": "slides/04_poisson_ch4.html#interpretations",
    "title": "Poisson Regression",
    "section": "Interpretations",
    "text": "Interpretations\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-4.1351\n0.0170\n-243.1397\n0\n\n\nprice\n-0.0005\n0.0001\n-7.0952\n0\n\n\nroom_typePrivate room\n-0.0994\n0.0174\n-5.6986\n0\n\n\nroom_typeShared room\n0.2436\n0.0452\n5.3841\n0\n\n\n\n\n\n\nQuestion:\n\nInterpret the coefficient of price.\nInterpret the coefficient of room_typePrivate room"
  },
  {
    "objectID": "slides/04_poisson_ch4.html#goodness-of-fit-2",
    "href": "slides/04_poisson_ch4.html#goodness-of-fit-2",
    "title": "Poisson Regression",
    "section": "Goodness-of-fit",
    "text": "Goodness-of-fit\n\\[\\begin{aligned}&H_0: \\text{ The model is a good fit for the data}\\\\\n&H_a: \\text{ There is significant lack of fit}\\end{aligned}\\]\n\npchisq(airbnb_model$deviance, airbnb_model$df.residual, lower.tail = F)\n\n[1] 0\n\n\n\nThere is evidence of significant lack of fit in the model. Therefore, more models would need to be explored that address the issues mentioned earlier.\n\n\nIn practice we would assess goodness-of-fit and finalize the model before any interpretations and conclusions."
  },
  {
    "objectID": "slides/04_poisson_ch4.html#data-weekend-drinking",
    "href": "slides/04_poisson_ch4.html#data-weekend-drinking",
    "title": "Poisson Regression",
    "section": "Data: Weekend drinking",
    "text": "Data: Weekend drinking\nThe data weekend-drinks.csv contains information from a survey of 77 students in a introductory statistics course on a dry campus.\nVariables\n\ndrinks: Number of drinks they had in the past weekend\noff_campus: 1 - lives off campus, 0 otherwise\nfirst_year: 1 - student is a first-year, 0 otherwise\nsex: f - student identifies as female, m - student identifies as male\n\n\nGoal: The goal is explore factors related to drinking behavior on a dry campus.\n\n\n\nCase study in BMLR - Section 4.10"
  },
  {
    "objectID": "slides/04_poisson_ch4.html#eda-response-variable",
    "href": "slides/04_poisson_ch4.html#eda-response-variable",
    "title": "Poisson Regression",
    "section": "EDA: Response variable",
    "text": "EDA: Response variable"
  },
  {
    "objectID": "slides/04_poisson_ch4.html#observed-vs.-expected-response",
    "href": "slides/04_poisson_ch4.html#observed-vs.-expected-response",
    "title": "Poisson Regression",
    "section": "Observed vs. expected response",
    "text": "Observed vs. expected response\n\n\nWhat does it mean to be a “zero” in this data?"
  },
  {
    "objectID": "slides/04_poisson_ch4.html#two-types-of-zeros",
    "href": "slides/04_poisson_ch4.html#two-types-of-zeros",
    "title": "Poisson Regression",
    "section": "Two types of zeros",
    "text": "Two types of zeros\nThere are two types of zeros\n\nThose who happen to have a zero in the data set (people who drink but happened to not drink last weekend)\nThose who will always report a value of zero (non-drinkers)\n\nThese are called true zeros\n\n\n\nWe introduce a new parameter \\(\\alpha\\) for the proportion of true zeros, then fit a model that has two components:\n\n\n1️⃣ The association between mean number of drinks and various characteristics among those who drink\n2️⃣ The estimated proportion of non-drinkers"
  },
  {
    "objectID": "slides/04_poisson_ch4.html#zero-inflated-poisson-model-1",
    "href": "slides/04_poisson_ch4.html#zero-inflated-poisson-model-1",
    "title": "Poisson Regression",
    "section": "Zero-inflated Poisson model",
    "text": "Zero-inflated Poisson model\nZero-inflated Poisson (ZIP) model has two parts\n\n1️⃣ Association, among those who drink, between the mean number of drinks and predictors sex and off campus residence\n\n\n\\[\\log(\\lambda) = \\beta_0 + \\beta_1 ~ off\\_campus + \\beta_2 ~ sex\\] where \\(\\lambda\\) is the mean number of drinks among those who drink\n\n\n2️⃣ Probability that a student does not drink\n\\[\\text{logit}(\\alpha) = \\log\\Big(\\frac{\\alpha}{1- \\alpha}\\Big) = \\beta_0 + \\beta_1 ~ first\\_year\\]\nwhere \\(\\alpha\\) is the proportion of non-drinkers\n\n\nNote: The same variables can be used in each component"
  },
  {
    "objectID": "slides/04_poisson_ch4.html#details-of-the-zip-model",
    "href": "slides/04_poisson_ch4.html#details-of-the-zip-model",
    "title": "Poisson Regression",
    "section": "Details of the ZIP model",
    "text": "Details of the ZIP model\n\nThe ZIP model is a special case of a latent variable model\n\nA type of mixture model where observations for one or more groups occur together but the group membership unknown\n\nZero-inflated models are a common type of mixture model; they apply beyond Poisson regression"
  },
  {
    "objectID": "slides/04_poisson_ch4.html#zip-model-in-r",
    "href": "slides/04_poisson_ch4.html#zip-model-in-r",
    "title": "Poisson Regression",
    "section": "ZIP model in R",
    "text": "ZIP model in R\nFit ZIP models using the zeroinfl function from the pscl R package.\n\nlibrary(pscl)\n\ndrinks_zip &lt;- zeroinfl(drinks ~ off_campus + sex | first_year,\n                data = drinks)\ndrinks_zip\n\n\nCall:\nzeroinfl(formula = drinks ~ off_campus + sex | first_year, data = drinks)\n\nCount model coefficients (poisson with log link):\n(Intercept)   off_campus         sexm  \n     0.7543       0.4159       1.0209  \n\nZero-inflation model coefficients (binomial with logit link):\n(Intercept)   first_year  \n    -0.6036       1.1364"
  },
  {
    "objectID": "slides/04_poisson_ch4.html#tidy-output",
    "href": "slides/04_poisson_ch4.html#tidy-output",
    "title": "Poisson Regression",
    "section": "Tidy output",
    "text": "Tidy output\nUse the tidy function from the poissonreg package for tidy model output.\n\nlibrary(poissonreg)\n\n\nMean number of drinks among those who drink\n\ntidy(drinks_zip, type = \"count\") %&gt;% kable(digits = 3)\n\n\n\n\nterm\ntype\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\ncount\n0.754\n0.144\n5.238\n0.000\n\n\noff_campus\ncount\n0.416\n0.206\n2.021\n0.043\n\n\nsexm\ncount\n1.021\n0.175\n5.827\n0.000"
  },
  {
    "objectID": "slides/04_poisson_ch4.html#tidy-output-1",
    "href": "slides/04_poisson_ch4.html#tidy-output-1",
    "title": "Poisson Regression",
    "section": "Tidy output",
    "text": "Tidy output\nProportion of non-drinkers\n\ntidy(drinks_zip, type = \"zero\") %&gt;% kable(digits = 3)\n\n\n\n\nterm\ntype\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\nzero\n-0.604\n0.311\n-1.938\n0.053\n\n\nfirst_year\nzero\n1.136\n0.610\n1.864\n0.062"
  },
  {
    "objectID": "slides/04_poisson_ch4.html#interpreting-the-model-coefficients",
    "href": "slides/04_poisson_ch4.html#interpreting-the-model-coefficients",
    "title": "Poisson Regression",
    "section": "Interpreting the model coefficients",
    "text": "Interpreting the model coefficients\n\n\n\n\n\nterm\ntype\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\ncount\n0.754\n0.144\n5.238\n0.000\n\n\noff_campus\ncount\n0.416\n0.206\n2.021\n0.043\n\n\nsexm\ncount\n1.021\n0.175\n5.827\n0.000\n\n\n\n\n\n\nQuestions\n\nInterpret the intercept.\nInterpret the coefficients of off_campus and sexm."
  },
  {
    "objectID": "slides/04_poisson_ch4.html#estimated-proportion-zeros",
    "href": "slides/04_poisson_ch4.html#estimated-proportion-zeros",
    "title": "Poisson Regression",
    "section": "Estimated proportion zeros",
    "text": "Estimated proportion zeros\n\n\n\n\n\nterm\ntype\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\nzero\n-0.604\n0.311\n-1.938\n0.053\n\n\nfirst_year\nzero\n1.136\n0.610\n1.864\n0.062\n\n\n\n\n\nQuestions:\nBased on the model…\n\nWhat is the probability a first-year student is a non-drinker?\nWhat is the probability a upperclass student (sophomore, junior, senior) is a non-drinker?"
  },
  {
    "objectID": "slides/04_poisson_ch4.html#these-are-just-a-few-of-the-many-models",
    "href": "slides/04_poisson_ch4.html#these-are-just-a-few-of-the-many-models",
    "title": "Poisson Regression",
    "section": "These are just a few of the many models…",
    "text": "These are just a few of the many models…\n\nUse the Vuong Test to compare the fit of the ZIP model to a regular Poisson model\n\nWhy can’t we use a drop-in-deviance test?\n\nWe’ve just discussed the ZIP model here, but there are other model applications beyond the standard Poisson regression model (e.g., hurdle models, Zero-inflated Negative Binomial models, etc. )"
  },
  {
    "objectID": "slides/04_poisson_ch4.html#estimating-coefficients-in-poisson-model",
    "href": "slides/04_poisson_ch4.html#estimating-coefficients-in-poisson-model",
    "title": "Poisson Regression",
    "section": "Estimating coefficients in Poisson model",
    "text": "Estimating coefficients in Poisson model\n\nLeast squares estimation would not work because the normality and equal variance assumptions don’t hold for Poisson regression\nMaximum likelihood estimation is used to estimate the coefficients of Poisson regression.\nThe likelihood is the product of the probabilities for the \\(n\\) independent observations in the data"
  },
  {
    "objectID": "slides/04_poisson_ch4.html#likelihood-for-regular-poisson-model",
    "href": "slides/04_poisson_ch4.html#likelihood-for-regular-poisson-model",
    "title": "Poisson Regression",
    "section": "Likelihood for regular Poisson model",
    "text": "Likelihood for regular Poisson model\nLet’s go back to example about household size in the Philippines. We will focus on the model using the main effect of age to understand variability in mean household size.\n\nSuppose the first five observations have household sizes of 4, 2, 8, 6, and 1. Then the likelihood is\n\n\n\\(L = \\frac{e^{-\\lambda_1}\\lambda_1^4}{4!} * \\frac{e^{-\\lambda_2}\\lambda_2^2}{2!} * \\frac{e^{-\\lambda_3}\\lambda_3^8}{8!} *\n\\frac{e^{-\\lambda_4}\\lambda_4^6}{6!} * \\frac{e^{-\\lambda_5}\\lambda_5^1}{1!}\\)"
  },
  {
    "objectID": "slides/04_poisson_ch4.html#likelihood-for-regular-poisson-model-1",
    "href": "slides/04_poisson_ch4.html#likelihood-for-regular-poisson-model-1",
    "title": "Poisson Regression",
    "section": "Likelihood for regular Poisson model",
    "text": "Likelihood for regular Poisson model\nWe will use the log likelihood to make finding the MLE easier\n\n\\(\\begin{aligned}\\log(L) &= -\\lambda_1 + 4\\log(\\lambda_1) - \\lambda_2 + 2\\log(\\lambda_2) - \\lambda_3 + 8\\log(\\lambda_3)\\\\ & -\\lambda_4 + 6 \\log(\\lambda_4) - \\lambda_5 + \\log(\\lambda_5) + C \\end{aligned}\\)\nwhere - \\(\\lambda\\) is the mean number in household depending on \\(x_i\\) - \\(C = -[\\log(4!) + \\log(2!) + \\log(8!) + \\log(6!)+ \\log(1!)]\\)"
  },
  {
    "objectID": "slides/04_poisson_ch4.html#likelihood-for-regular-poisson-model-2",
    "href": "slides/04_poisson_ch4.html#likelihood-for-regular-poisson-model-2",
    "title": "Poisson Regression",
    "section": "Likelihood for regular Poisson model",
    "text": "Likelihood for regular Poisson model\nGiven the age of the head of the household, we fit the model\n\\[\\log(\\lambda_i) = \\beta_0 + \\beta_1~age_i\\]\nThen we replace each \\(\\lambda_i\\) in \\(\\log(L)\\) with \\(e^{\\beta_0 + \\beta_1~age_i}\\).\n\nSuppose the first five observations have ages \\(X = (32, 21, 55, 44, 28)\\). Then\n\n\n\\[\\begin{aligned} \\log(L) &= [-e^{\\beta_0 + \\beta_132}+ 4(\\beta_0 + \\beta_1 32)] + [ - e^{\\beta_0 + \\beta_121} + 2(\\beta_0 + \\beta_121)] \\\\ &+  [- e^{\\beta_0 + \\beta_155} + 8(\\beta_0 + \\beta_155)] +  [-e^{\\beta_0 + \\beta_144} + 6(\\beta_0 + \\beta_144)] \\\\ &+ [-e^{\\beta_0 + \\beta_128}(\\beta_0 + \\beta_128)] + C \\end{aligned}\\]\n\n\nUse search algorithm to find the values of \\(\\beta_0\\) and \\(\\beta_1\\) that maximize the above equation."
  },
  {
    "objectID": "slides/04_poisson_ch4.html#probabilities-under-zip-model",
    "href": "slides/04_poisson_ch4.html#probabilities-under-zip-model",
    "title": "Poisson Regression",
    "section": "Probabilities under ZIP model",
    "text": "Probabilities under ZIP model\nThere are three different types of observations in the data:\n\nObserved zero and will always be 0 (true zeros)\nObserved 0 but will not always be 0\nObserved non-zero count and will not always be 0"
  },
  {
    "objectID": "slides/04_poisson_ch4.html#probabilities-under-zip-model-1",
    "href": "slides/04_poisson_ch4.html#probabilities-under-zip-model-1",
    "title": "Poisson Regression",
    "section": "Probabilities under ZIP model",
    "text": "Probabilities under ZIP model\nTrue zeros\n\\[P(0 | \\text{true zero})= \\alpha\\]\n\nObserved 0 but will not always be 0\n\\[P(0 | \\text{not always zero}) = (1 - \\alpha)\\frac{e^{-\\lambda}\\lambda^0}{0!}\\]\n\n\nDid not observe 0 and will not always be 0\n\\[P(z_i | \\text{not always zero}) = (1 - \\alpha)\\frac{e^{-\\lambda}\\lambda^{z_i}}{z_i!}\\]"
  },
  {
    "objectID": "slides/04_poisson_ch4.html#probabilities-under-zip-model-2",
    "href": "slides/04_poisson_ch4.html#probabilities-under-zip-model-2",
    "title": "Poisson Regression",
    "section": "Probabilities under ZIP model",
    "text": "Probabilities under ZIP model\nPutting this all together. Let \\(y_i\\) be an observed response then\n\\[P(Y_i = y_i | x_i) = \\begin{cases}\n\\alpha + (1 - \\alpha)e^{-\\lambda_i} && \\text{ if } y_i = 0 \\\\\n(1 - \\alpha)\\frac{e^{-\\lambda_i}\\lambda_i^{y_i}}{y_i!} && \\text{ if } y_i &gt; 0\n\\end{cases}\\]\n\nRecall from our example,\n\\[\\lambda_i = e^{\\beta_0 + \\beta_1~off\\_campus_i + \\beta_2 ~ sex_i}\\]\n\\[\\alpha_i = \\frac{e^{\\beta_{0\\alpha} + \\beta_{1\\alpha} ~ first\\_year_i}}{1 + e^{\\beta_{0\\alpha} + \\beta_{1\\alpha} ~ first\\_year_i}}\\]\n\nPlug in \\(\\lambda_i\\) and \\(\\alpha_i\\) into the above equation obtain the likelihood function"
  },
  {
    "objectID": "slides/04_poisson_ch4.html#acknowledgements",
    "href": "slides/04_poisson_ch4.html#acknowledgements",
    "title": "Poisson Regression",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThese slides are based on content in BMLR: Chapter 4\nInitial versions of the slides are by Dr. Maria Tackett, Duke University"
  },
  {
    "objectID": "slides/06_logistic_ch6_o.html",
    "href": "slides/06_logistic_ch6_o.html",
    "title": "Logistic Regression",
    "section": "",
    "text": "library(tidyverse)\nlibrary(tidymodels)\nlibrary(GGally)\nlibrary(knitr)\nlibrary(patchwork)\nlibrary(viridis)\nlibrary(ggfortify)\nlibrary(gridExtra)"
  },
  {
    "objectID": "slides/06_logistic_ch6_o.html#setup",
    "href": "slides/06_logistic_ch6_o.html#setup",
    "title": "Logistic Regression",
    "section": "",
    "text": "library(tidyverse)\nlibrary(tidymodels)\nlibrary(GGally)\nlibrary(knitr)\nlibrary(patchwork)\nlibrary(viridis)\nlibrary(ggfortify)\nlibrary(gridExtra)"
  },
  {
    "objectID": "slides/06_logistic_ch6_o.html#learning-goals",
    "href": "slides/06_logistic_ch6_o.html#learning-goals",
    "title": "Logistic Regression",
    "section": "Learning goals",
    "text": "Learning goals\n\nIdentify Bernoulli and binomial random variables\nWrite GLM for binomial response variable\nInterpret the coefficients for a logistic regression model\nVisualizations for logistic regression\nInterpret coefficients and results from an ordinal logistic regression model\nSummarize GLMs for independent observations"
  },
  {
    "objectID": "slides/06_logistic_ch6_o.html#bernoulli-binomial-random-variables-12",
    "href": "slides/06_logistic_ch6_o.html#bernoulli-binomial-random-variables-12",
    "title": "Logistic Regression",
    "section": "Bernoulli + Binomial random variables (1/2)",
    "text": "Bernoulli + Binomial random variables (1/2)\nLogistic regression is used to analyze data with two types of responses:\n\nBinary: These responses take on two values success \\((Y = 1)\\) or failure \\((Y = 0)\\), yes \\((Y = 1)\\) or no \\((Y = 0)\\), etc.\n\n. . .\n\\[P(Y = y) = p^y(1-p)^{1-y} \\hspace{10mm} y = 0, 1\\]"
  },
  {
    "objectID": "slides/06_logistic_ch6_o.html#bernoulli-binomial-random-variables-22",
    "href": "slides/06_logistic_ch6_o.html#bernoulli-binomial-random-variables-22",
    "title": "Logistic Regression",
    "section": "Bernoulli + Binomial random variables (2/2)",
    "text": "Bernoulli + Binomial random variables (2/2)\nLogistic regression is used to analyze data with two types of responses:\n\nBinomial: Number of successes in a Bernoulli process, \\(n\\) independent trials with a constant probability of success \\(p\\).\n\n. . .\n\\[P(Y = y) = {n \\choose y}p^{y}(1-p)^{n - y} \\hspace{10mm} y = 0, 1, \\ldots, n\\]\n. . .\nIn both instances, the goal is to model \\(p\\) the probability of success."
  },
  {
    "objectID": "slides/06_logistic_ch6_o.html#binary-vs.-binomial-data",
    "href": "slides/06_logistic_ch6_o.html#binary-vs.-binomial-data",
    "title": "Logistic Regression",
    "section": "Binary vs. Binomial data",
    "text": "Binary vs. Binomial data\nFor each example, identify if the response is a Bernoulli or Binomial response\n\nUse median age and unemployment rate in a county to predict the percent of Obama votes in the county in the 2008 presidential election.\nUse GPA and MCAT scores to estimate the probability a student is accepted into medical school.\nUse sex, age, and smoking history to estimate the probability an individual has lung cancer.\nUse offensive and defensive statistics from the 2017-2018 NBA season to predict a team’s winning percentage."
  },
  {
    "objectID": "slides/06_logistic_ch6_o.html#logistic-regression-model",
    "href": "slides/06_logistic_ch6_o.html#logistic-regression-model",
    "title": "Logistic Regression",
    "section": "Logistic regression model",
    "text": "Logistic regression model\n\\(\\log\\Big(\\frac{p}{1-p}\\Big) = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\dots + \\beta_px_p\\)\n\nThe response variable, \\(\\log\\Big(\\frac{p}{1-p}\\Big)\\), is the log(odds) of success, i.e. the logit\nUse the model to calculate the probability of success \\(\\hat{p} = \\frac{e^{\\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\dots + \\beta_px_p}}{1 + e^{\\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\dots + \\beta_px_p}}\\)\nWhen the response is a Bernoulli random variable, the probabilities can be used to classify each observation as a success or failure"
  },
  {
    "objectID": "slides/06_logistic_ch6_o.html#logistic-vs-linear-regression-model",
    "href": "slides/06_logistic_ch6_o.html#logistic-vs-linear-regression-model",
    "title": "Logistic Regression",
    "section": "Logistic vs linear regression model",
    "text": "Logistic vs linear regression model\n\nPlotCode\n\n\n\n\n\n\n\nGraph from BMLR Chapter 6\n\n\n\n\n\n\n\nset.seed(0)\ndat &lt;- tibble(x=runif(200, -5, 10),\n                  p=exp(-2+1*x)/(1+exp(-2+1*x)),\n                  y=rbinom(200, 1, p),\n                  y2=.3408+.0901*x,\n                  logit=log(p/(1-p)))\ndat2 &lt;- tibble(x = c(dat$x, dat$x),\n               y = c(dat$y2, dat$p),\n               `Regression model` = c(rep(\"linear\", 200),\n                                      rep(\"logistic\", 200)))\n\nggplot() + \n  geom_point(data = dat, aes(x, y)) +\n  geom_line(data = dat2, aes(x, y, linetype = `Regression model`, \n                             color = `Regression model`)) + \n  labs(title = \"Linear vs. logistic regression models for binary response data\") + \n  scale_colour_manual(name = 'Regression model',\n                      values = c('blue', 'red'), \n                      labels = c('linear', 'logistic'), guide ='legend')"
  },
  {
    "objectID": "slides/06_logistic_ch6_o.html#logit-link",
    "href": "slides/06_logistic_ch6_o.html#logit-link",
    "title": "Logistic Regression",
    "section": "Logit link",
    "text": "Logit link\nBernoulli and Binomial random variables can be written in one-parameter exponential family form, \\(f(y;\\theta) = e^{[a(y)b(\\theta) + c(\\theta) + d(y)]}\\)\n. . .\nBernoulli\n\\[f(y;p) = e^{y\\log(\\frac{p}{1-p}) + \\log(1-p)}\\]\n. . .\nBinomial\n\\[f(y;n,p) = e^{y\\log(\\frac{p}{1-p}) + n\\log(1-p) + \\log{n \\choose y}}\\]\n. . .\nThey have the same canonical link \\(b(p) = \\log\\big(\\frac{p}{1-p}\\big)\\)"
  },
  {
    "objectID": "slides/06_logistic_ch6_o.html#assumptions-for-logistic-regression",
    "href": "slides/06_logistic_ch6_o.html#assumptions-for-logistic-regression",
    "title": "Logistic Regression",
    "section": "Assumptions for logistic regression",
    "text": "Assumptions for logistic regression\nThe following assumptions need to be satisfied to use logistic regression to make inferences\n. . .\n1️⃣ \\(\\hspace{0.5mm}\\) Binary response: The response is dichotomous (has two possible outcomes) or is the sum of dichotomous responses\n2️⃣ \\(\\hspace{0.5mm}\\) Independence: The observations must be independent of one another\n3️⃣ \\(\\hspace{0.5mm}\\) Variance structure: Variance of a binomial random variable is \\(np(1-p)\\) \\((n = 1 \\text{ for Bernoulli})\\) , so the variability is highest when \\(p = 0.5\\)\n4️⃣ \\(\\hspace{0.5mm}\\) Linearity: The log of the odds ratio, \\(\\log\\big(\\frac{p}{1-p}\\big)\\), must be a linear function of the predictors \\(x_1, \\ldots, x_p\\)"
  },
  {
    "objectID": "slides/06_logistic_ch6_o.html#covid-19-infection-prevention-practices-at-food-establishments",
    "href": "slides/06_logistic_ch6_o.html#covid-19-infection-prevention-practices-at-food-establishments",
    "title": "Logistic Regression",
    "section": "COVID-19 infection prevention practices at food establishments",
    "text": "COVID-19 infection prevention practices at food establishments\nResearchers at Wollo Univeristy in Ethiopia conducted a study in July and August 2020 to understand factors associated with good COVID-19 infection prevention practices at food establishments. Their study is published in Andualem et al. (2022)\n\nThey were particularly interested in the understanding implementation of prevention practices at food establishments, given the workers’ increased risk due to daily contact with customers.\n\n\nAndualem, A., Tegegne, B., Ademe, S., Natnael, T., Berihun, G., Abebe, M., … & Adane, M. (2022). COVID-19 infection prevention practices among a sample of food handlers of food and drink establishments in Ethiopia. PloS one, 17(1), e0259851."
  },
  {
    "objectID": "slides/06_logistic_ch6_o.html#the-data",
    "href": "slides/06_logistic_ch6_o.html#the-data",
    "title": "Logistic Regression",
    "section": "The data",
    "text": "The data\n“An institution-based cross-sectional study was conducted among 422 food handlers in Dessie City and Kombolcha Town food and drink establishments in July and August 2020. The study participants were selected using a simple random sampling technique. Data were collected by trained data collectors using a pretested structured questionnaire and an on-the-spot observational checklist.”\n\n\nAndualem, A., Tegegne, B., Ademe, S., Natnael, T., Berihun, G., Abebe, M., … & Adane, M. (2022). COVID-19 infection prevention practices among a sample of food handlers of food and drink establishments in Ethiopia. PloS one, 17(1), e0259851."
  },
  {
    "objectID": "slides/06_logistic_ch6_o.html#response-variable",
    "href": "slides/06_logistic_ch6_o.html#response-variable",
    "title": "Logistic Regression",
    "section": "Response variable",
    "text": "Response variable\n“The outcome variable of this study was the good or poor practices of COVID-19 infection prevention among food handlers. Nine yes/no questions, one observational checklist and five multiple choice infection prevention practices questions were asked with a minimum score of 1 and maximum score of 25. Good infection prevention practice (the variable of interest) was determined for food handlers who scored 75% or above, whereas poor infection prevention practices refers to those food handlers who scored below 75% on the practice questions.”\n\n\nAndualem, A., Tegegne, B., Ademe, S., Natnael, T., Berihun, G., Abebe, M., … & Adane, M. (2022). COVID-19 infection prevention practices among a sample of food handlers of food and drink establishments in Ethiopia. PloS one, 17(1), e0259851."
  },
  {
    "objectID": "slides/06_logistic_ch6_o.html#results",
    "href": "slides/06_logistic_ch6_o.html#results",
    "title": "Logistic Regression",
    "section": "Results",
    "text": "Results\n\n\n\nAndualem, A., Tegegne, B., Ademe, S., Natnael, T., Berihun, G., Abebe, M., … & Adane, M. (2022). COVID-19 infection prevention practices among a sample of food handlers of food and drink establishments in Ethiopia. PloS one, 17(1), e0259851."
  },
  {
    "objectID": "slides/06_logistic_ch6_o.html#interpreting-the-results",
    "href": "slides/06_logistic_ch6_o.html#interpreting-the-results",
    "title": "Logistic Regression",
    "section": "Interpreting the results",
    "text": "Interpreting the results\n\nIs the response a Bernoulli or Binomial?\nWhat is the strongest predictor of having good COVID-19 infection prevention practices?\n\nIt’s often unreliable to look answer this question just based on the model output. Why are we able to answer this question based on the model output in this case?\n\nDescribe the effect (coefficient interpretation and inference) of having COVID-19 infection prevention policies available at the food establishment.\nThe intercept describes what group of food handlers?"
  },
  {
    "objectID": "slides/06_logistic_ch6_o.html#access-to-personal-protective-equipment",
    "href": "slides/06_logistic_ch6_o.html#access-to-personal-protective-equipment",
    "title": "Logistic Regression",
    "section": "Access to personal protective equipment",
    "text": "Access to personal protective equipment\nWe will use the data from Andualem et al. (2022) to explore the association between age, sex, years of service, and whether someone works at a food establishment with access to personal protective equipment (PPE) as of August 2020. We will use access to PPE as a proxy for wearing PPE.\n\nPlotCode\n\n\n\n\n\n\n\nage\nsex\nyears\nppe_access\n\n\n\n\n34\nMale\n2\n1\n\n\n32\nFemale\n3\n1\n\n\n32\nFemale\n1\n1\n\n\n40\nMale\n4\n1\n\n\n32\nMale\n10\n1\n\n\n\n\n\n\n\n\ncovid_df &lt;- read_csv(\"data/covid-prevention-study.csv\") |&gt;\n  rename(age = \"Age of food handlers\", \n         years = \"Years of service\", \n         ppe_access = \"Availability of PPEs\") |&gt;\n  mutate(sex = factor(if_else(Sex == 2, \"Female\", \"Male\"))) |&gt;\n  select(age, sex, years, ppe_access) \n\ncovid_df |&gt; slice(1:5) |&gt; kable()"
  },
  {
    "objectID": "slides/06_logistic_ch6_o.html#eda-for-binary-response-12",
    "href": "slides/06_logistic_ch6_o.html#eda-for-binary-response-12",
    "title": "Logistic Regression",
    "section": "EDA for binary response (1/2)",
    "text": "EDA for binary response (1/2)\n\nlibrary(Stat2Data)\npar(mfrow = c(1, 2))\nemplogitplot1(ppe_access ~ age, data = covid_df, ngroups = 10)\nemplogitplot1(ppe_access ~ years, data = covid_df, ngroups = 5)"
  },
  {
    "objectID": "slides/06_logistic_ch6_o.html#eda-for-binary-response-22",
    "href": "slides/06_logistic_ch6_o.html#eda-for-binary-response-22",
    "title": "Logistic Regression",
    "section": "EDA for binary response (2/2)",
    "text": "EDA for binary response (2/2)\n\nplotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlibrary(viridis)\nggplot(data = covid_df, aes(x = sex, fill = factor(ppe_access))) + \n  geom_bar(position = \"fill\")  +\n  labs(x = \"Sex\", \n       fill = \"PPE Access\", \n       title = \"PPE Access by Sex\") + \n  scale_fill_viridis_d()"
  },
  {
    "objectID": "slides/06_logistic_ch6_o.html#model-results",
    "href": "slides/06_logistic_ch6_o.html#model-results",
    "title": "Logistic Regression",
    "section": "Model results",
    "text": "Model results\n\nppe_model &lt;- glm(factor(ppe_access) ~ age + sex + years, data = covid_df, \n                 family = binomial)\ntidy(ppe_model, conf.int = TRUE) |&gt;\n  kable(digits = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n-2.127\n0.458\n-4.641\n0.000\n-3.058\n-1.257\n\n\nage\n0.056\n0.017\n3.210\n0.001\n0.023\n0.091\n\n\nsexMale\n0.341\n0.224\n1.524\n0.128\n-0.098\n0.780\n\n\nyears\n0.264\n0.066\n4.010\n0.000\n0.143\n0.401"
  },
  {
    "objectID": "slides/06_logistic_ch6_o.html#visualizing-coefficient-estimates",
    "href": "slides/06_logistic_ch6_o.html#visualizing-coefficient-estimates",
    "title": "Logistic Regression",
    "section": "Visualizing coefficient estimates",
    "text": "Visualizing coefficient estimates\n\nmodel_coef &lt;- tidy(ppe_model, exponentiate = TRUE, conf.int = TRUE)\n\n\nggplot(data = model_coef, aes(x = term, y = estimate)) +\n  geom_point() +\n  geom_hline(yintercept = 1, lty = 2) + \n  geom_pointrange(aes(ymin = conf.low, ymax = conf.high))+\n  labs(title = \"Exponentiated model coefficients\") + \n  coord_flip()"
  },
  {
    "objectID": "slides/06_logistic_ch6_o.html#data-supporting-railroads-in-the-1870s",
    "href": "slides/06_logistic_ch6_o.html#data-supporting-railroads-in-the-1870s",
    "title": "Logistic Regression",
    "section": "Data: Supporting railroads in the 1870s",
    "text": "Data: Supporting railroads in the 1870s\nThe data set RR_Data_Hale.csv contains information on support for referendums related to railroad subsidies for 11 communities in Alabama in the 1870s. The data were originally analyzed as part of a thesis project by a student at St. Olaf College. The variables in the data are\n\npctBlack: percentage of Black residents in the county\ndistance: distance the proposed railroad is from the community (in miles)\nYesVotes: number of “yes” votes in favor of the proposed railroad line\nNumVotes: number of votes cast in the election\n\n. . .\nPrimary question: Was voting on the railroad referendum related to the distance from the proposed railroad line, after adjusting for the racial composition of a community?"
  },
  {
    "objectID": "slides/06_logistic_ch6_o.html#the-data-1",
    "href": "slides/06_logistic_ch6_o.html#the-data-1",
    "title": "Logistic Regression",
    "section": "The data",
    "text": "The data\n\nrr &lt;- read_csv(\"data/RR_Data_Hale.csv\")\nrr |&gt; slice(1:5) |&gt; kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCounty\npopBlack\npopWhite\npopTotal\npctBlack\ndistance\nYesVotes\nNumVotes\n\n\n\n\nCarthage\n841\n599\n1440\n58.40\n17\n61\n110\n\n\nCederville\n1774\n146\n1920\n92.40\n7\n0\n15\n\n\nFive Mile Creek\n140\n626\n766\n18.28\n15\n4\n42\n\n\nGreensboro\n1425\n975\n2400\n59.38\n0\n1790\n1804\n\n\nHarrison\n443\n355\n798\n55.51\n7\n0\n15"
  },
  {
    "objectID": "slides/06_logistic_ch6_o.html#eda-13",
    "href": "slides/06_logistic_ch6_o.html#eda-13",
    "title": "Logistic Regression",
    "section": "EDA (1/3)",
    "text": "EDA (1/3)\n\nrr &lt;- rr |&gt;\n  mutate(pctYes = YesVotes/NumVotes, \n         emp_logit = log(pctYes / (1 - pctYes)))\n\nrr |&gt; head(5)\n\n# A tibble: 5 × 10\n  County   popBlack popWhite popTotal pctBlack distance YesVotes NumVotes pctYes emp_logit\n  &lt;chr&gt;       &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;\n1 Carthage      841      599     1440     58.4       17       61      110 0.555      0.219\n2 Cedervi…     1774      146     1920     92.4        7        0       15 0       -Inf    \n3 Five Mi…      140      626      766     18.3       15        4       42 0.0952    -2.25 \n4 Greensb…     1425      975     2400     59.4        0     1790     1804 0.992      4.85 \n5 Harrison      443      355      798     55.5        7        0       15 0       -Inf"
  },
  {
    "objectID": "slides/06_logistic_ch6_o.html#eda-23",
    "href": "slides/06_logistic_ch6_o.html#eda-23",
    "title": "Logistic Regression",
    "section": "EDA (2/3)",
    "text": "EDA (2/3)\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\np1 &lt;- ggplot(data = rr, aes(x = distance, y = emp_logit)) + \n  geom_point() + \n  geom_smooth(method  = \"lm\", se = FALSE) + \n  labs(x = \"Distance to proposed railroad\", \n       y = \" \")\n  \np2 &lt;- ggplot(data = rr, aes(x = pctBlack, y = emp_logit)) + \n  geom_point() + \n  geom_smooth(method  = \"lm\", se = FALSE) + \n  labs(x = \"% Black residents\", \n       y = \"\")\np1 + p2 + plot_annotation(title = \"Log(odds yes vote) vs. predictor variables\")"
  },
  {
    "objectID": "slides/06_logistic_ch6_o.html#eda-33",
    "href": "slides/06_logistic_ch6_o.html#eda-33",
    "title": "Logistic Regression",
    "section": "EDA (3/3)",
    "text": "EDA (3/3)\n\nrr &lt;- rr |&gt;\n  mutate(inFavor = if_else(pctYes &gt; 0.5, \"Yes\", \"No\"))\n\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(data = rr, aes(x = distance, y = pctBlack, color = inFavor)) + \n  geom_point() + \n  geom_smooth(method  = \"lm\", se = FALSE, aes(lty = inFavor)) + \n  labs(x = \"Distance to proposed railroad\", \n       y = \"% Black residents\",\n       title = \"% Black residents vs. distance\", \n       subtitle = \"Based on vote outcome\") + \n  scale_color_viridis_d(end = 0.85)\n\n\n\n\n. . .\nCheck for potential multicollinearity and interaction effect."
  },
  {
    "objectID": "slides/06_logistic_ch6_o.html#model",
    "href": "slides/06_logistic_ch6_o.html#model",
    "title": "Logistic Regression",
    "section": "Model",
    "text": "Model\nLet \\(p\\) be the percent of yes votes in a county. We’ll start by fitting the following model:\n\\[\\log\\Big(\\frac{p}{1-p}\\Big)  = \\beta_0 + \\beta_1 ~ dist + \\beta_2 ~ pctBlack\\]\n. . .\nLikelihood\n\\[\\begin{aligned}L(p) &= \\prod_{i=1}^{n} {m_i \\choose y_i}p_i^{y_i}(1 - p_i)^{m_i - y_i} \\\\\n&= \\prod_{i=1}^{n} {m_i \\choose y_i}\\Big[\\frac{e^{\\beta_0 + \\beta_1 ~ dist_i + \\beta_2 ~ pctBlack_i}}{1 + e^{\\beta_0 + \\beta_1 ~ dist_i + \\beta_2 ~ pctBlack_i}}\\Big]^{y_i}\\Big[\\frac{1}{e^{\\beta_0 + \\beta_1 ~ dist_i + \\beta_2 ~ pctBlack_i}}\\Big]^{m_i - y_i} \\\\\\end{aligned}\\]\nUse IWLS to find \\(\\hat{\\beta}_0, \\hat{\\beta}_1, \\hat{\\beta}_2\\)."
  },
  {
    "objectID": "slides/06_logistic_ch6_o.html#model-in-r",
    "href": "slides/06_logistic_ch6_o.html#model-in-r",
    "title": "Logistic Regression",
    "section": "Model in R",
    "text": "Model in R\n\nrr_model &lt;- glm(cbind(YesVotes, NumVotes - YesVotes) ~ distance + pctBlack, \n                data = rr, family = binomial)\ntidy(rr_model, conf.int = TRUE) |&gt;\n  kable(digits = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n4.222\n0.297\n14.217\n0.000\n3.644\n4.809\n\n\ndistance\n-0.292\n0.013\n-22.270\n0.000\n-0.318\n-0.267\n\n\npctBlack\n-0.013\n0.004\n-3.394\n0.001\n-0.021\n-0.006\n\n\n\n\n\n. . .\n\\[\\log\\Big(\\frac{\\hat{p}}{1-\\hat{p}}\\Big)  = 4.22 - 0.292 ~ dist - 0.013 ~ pctBlack\\]\n\n\nSee Section 6.5 of Generalized Linear Models with Examples in R by Dunn and Smyth (available through Duke library) for details on estimating the standard errors."
  },
  {
    "objectID": "slides/06_logistic_ch6_o.html#residuals",
    "href": "slides/06_logistic_ch6_o.html#residuals",
    "title": "Logistic Regression",
    "section": "Residuals",
    "text": "Residuals\nSimilar to Poisson regression, there are two types of residuals: Pearson and deviance residuals\n. . .\nPearson residuals\n\\[\\text{Pearson residual}_i = \\frac{\\text{actual count} - \\text{predicted count}}{\\text{SD count}} = \\frac{Y_i - m_i\\hat{p}_i}{\\sqrt{m_i\\hat{p}_i(1 - \\hat{p}_i)}}\\]\n. . .\nDeviance residuals\n\\[d_i = \\text{sign}(Y_i - m_i\\hat{p}_i)\\sqrt{2\\Big[Y_i\\log\\Big(\\frac{Y_i}{m_i\\hat{p}_i}\\Big) + (m_i - Y_i)\\log\\Big(\\frac{m_i - Y_i}{m_i - m_i\\hat{p}_i}\\Big)\\Big]}\\]"
  },
  {
    "objectID": "slides/06_logistic_ch6_o.html#plot-of-deviance-residuals",
    "href": "slides/06_logistic_ch6_o.html#plot-of-deviance-residuals",
    "title": "Logistic Regression",
    "section": "Plot of deviance residuals",
    "text": "Plot of deviance residuals\n\nModelResidual PlotPlot Code\n\n\n\nrr_int_model &lt;- glm(cbind(YesVotes, NumVotes - YesVotes) ~ distance + pctBlack +\n                      distance*pctBlack, \n                data = rr, family = binomial)\n\n\nrr_int_aug &lt;- augment(rr_int_model, type.predict = \"response\", \n                        type.residuals = \"deviance\")\n\nrr_int_aug |&gt; slice(1:5) |&gt; kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n.rownames\ncbind(YesVotes, NumVotes - YesVotes)\ndistance\npctBlack\n.fitted\n.resid\n.hat\n.sigma\n.cooksd\n.std.resid\n\n\n\n\n1\n61\n49\n17\n58.40\n0.2075801\n7.964510\n0.4663943\n5.0884957\n32.9667672\n\n\n2\n0\n15\n7\n92.40\n0.6776101\n-5.827504\n0.0492925\n6.3049338\n0.4298496\n\n\n3\n4\n38\n15\n18.28\n0.2024659\n-1.885115\n0.6433983\n6.6366201\n3.7828247\n\n\n4\n1790\n14\n0\n59.38\n0.9760416\n5.230746\n0.8996698\n0.5044966\n452.2582760\n\n\n5\n0\n15\n7\n55.51\n0.8513123\n-7.561561\n0.0240118\n5.9951340\n0.5412285\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(data = rr_int_aug, aes(x = .fitted, y = .resid)) + \n  geom_point() + \n  geom_hline(yintercept = 0, color = \"red\") + \n  labs(x = \"Fitted values\", \n       y = \"Deviance residuals\", \n       title = \"Deviance residuals vs. fitted\", \n       subtitle = \"for model with interaction term\")"
  },
  {
    "objectID": "slides/06_logistic_ch6_o.html#goodness-of-fit",
    "href": "slides/06_logistic_ch6_o.html#goodness-of-fit",
    "title": "Logistic Regression",
    "section": "Goodness of fit",
    "text": "Goodness of fit\nSimilar to Poisson regression, the sum of the squared deviance residuals is used to assess goodness of fit.\n\\[\\begin{aligned} &H_0: \\text{ Model is a good fit} \\\\\n&H_a: \\text{ Model is not a good fit}\\end{aligned}\\]\n\nWhen \\(m_i\\) is large and the model is a good fit \\((H_0 \\text{ true})\\) the residual deviance follows a \\(\\chi^2\\) distribution with \\(n - p\\) degrees of freedom.\n\nRecall \\(n - p\\) is the residual degrees of freedom.\n\nIf the model fits, we expect the residual deviance to be approximately what value?"
  },
  {
    "objectID": "slides/06_logistic_ch6_o.html#adjusting-for-overdispersion-12",
    "href": "slides/06_logistic_ch6_o.html#adjusting-for-overdispersion-12",
    "title": "Logistic Regression",
    "section": "Adjusting for overdispersion (1/2)",
    "text": "Adjusting for overdispersion (1/2)\n\nOverdispersion occurs when there is extra-binomial variation, i.e. the variance is greater than what we would expect, \\(np(1-p)\\).\nSimilar to Poisson regression, we can adjust for overdispersion in the binomial regression model by using a dispersion parameter \\[\\hat{\\phi} = \\sum \\frac{(\\text{Pearson residuals})^2}{n-p}\\]\n\nBy multiplying by \\(\\hat{\\phi}\\), we are accounting for the reduction in information we would expect from independent observations."
  },
  {
    "objectID": "slides/06_logistic_ch6_o.html#adjusting-for-overdispersion-22",
    "href": "slides/06_logistic_ch6_o.html#adjusting-for-overdispersion-22",
    "title": "Logistic Regression",
    "section": "Adjusting for overdispersion (2/2)",
    "text": "Adjusting for overdispersion (2/2)\n\nWe adjust for overdispersion using a quasibinomial model.\n\n“Quasi” reflects the fact we are no longer using a binomial model with true likelihood.\n\nThe standard errors of the coefficients are \\(SE_{Q}(\\hat{\\beta}_j) = \\sqrt{\\hat{\\phi}} SE(\\hat{\\beta})\\)\n\nInference is done using the \\(t\\) distribution to account for extra variability"
  },
  {
    "objectID": "slides/06_logistic_ch6_o.html#predicting-ed-wait-and-treatment-times",
    "href": "slides/06_logistic_ch6_o.html#predicting-ed-wait-and-treatment-times",
    "title": "Logistic Regression",
    "section": "Predicting ED wait and treatment times",
    "text": "Predicting ED wait and treatment times\nAtaman and Sariyer (2021) use ordinal logistic regression to predict patient wait and treatment times in an emergency department (ED). The goal is to identify relevant factors that can be used to inform recommendations for reducing wait and treatment times, thus improving the quality of care in the ED.\nData: Daily records for ED arrivals in August 2018 at a public hospital in Izmir, Turkey.\nResponse variable: Wait time, a categorical variable with three levels: - Patients who wait less than 10 minutes - Patients whose waiting time is in the range of 10-60 minutes - Patients who wait more than 60 minutes\n\n\nAtaman, M. G., & Sarıyer, G. (2021). Predicting waiting and treatment times in emergency departments using ordinal logistic regression models. The American Journal of Emergency Medicine, 46, 45-50."
  },
  {
    "objectID": "slides/06_logistic_ch6_o.html#ordinal-logistic-regression",
    "href": "slides/06_logistic_ch6_o.html#ordinal-logistic-regression",
    "title": "Logistic Regression",
    "section": "Ordinal logistic regression",
    "text": "Ordinal logistic regression\nLet \\(Y\\) be an ordinal response variable that takes levels \\(1, 2, \\ldots, J\\) with associated probabilities \\(p_1, p_2, \\ldots, p_J\\).\nThe proportional odds model can be written as the following:\n\\[\\begin{aligned}&\\log\\Big(\\frac{P(Y\\leq 1)}{P(Y &gt; 1)}\\Big) = \\beta_{01} + \\beta_1x_1 + \\dots +  \\beta_px_p \\\\\n& \\log\\Big(\\frac{P(Y\\leq 2)}{P(Y &gt; 2)}\\Big) = \\beta_{02} + \\beta_1x_1 + \\dots +  \\beta_px_p \\\\\n& \\dots \\\\\n& \\log\\Big(\\frac{P(Y\\leq J-1)}{P(Y &gt; J-1)}\\Big) = \\beta_{0{J-1}} + \\beta_1x_1 + \\dots +  \\beta_px_p\\end{aligned}\\]"
  },
  {
    "objectID": "slides/06_logistic_ch6_o.html#questions",
    "href": "slides/06_logistic_ch6_o.html#questions",
    "title": "Logistic Regression",
    "section": "Questions",
    "text": "Questions\n\nHow is the proportional odds model similar to the multinomial logistic model?\nHow is it different?"
  },
  {
    "objectID": "slides/06_logistic_ch6_o.html#effect-of-arrival-mode",
    "href": "slides/06_logistic_ch6_o.html#effect-of-arrival-mode",
    "title": "Logistic Regression",
    "section": "Effect of arrival mode",
    "text": "Effect of arrival mode\n\n\n\n\n\n\n\n\n\n. . .\nQuestion\nThe variable arrival mode takes two categories: ambulance and walk-in. Describe the effect of arrival mode in this model. Note that the baseline level is “walk-in”."
  },
  {
    "objectID": "slides/06_logistic_ch6_o.html#effect-of-triage-level",
    "href": "slides/06_logistic_ch6_o.html#effect-of-triage-level",
    "title": "Logistic Regression",
    "section": "Effect of triage level",
    "text": "Effect of triage level\n. . .\nConsider the full output with the ordinal logistic models for wait and treatment times.\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n. . .\nUse the results from both models to describe the effect of triage level (red = urgent, green = non-urgent) on the wait and treatment times in the ED. Note that “red” is the baseline level. Is this what you expected?"
  },
  {
    "objectID": "slides/06_logistic_ch6_o.html#wrap-up",
    "href": "slides/06_logistic_ch6_o.html#wrap-up",
    "title": "Logistic Regression",
    "section": "Wrap up",
    "text": "Wrap up\n\nCovered fitting, interpreting, and drawing conclusions from GLMs\n\nLooked at Poisson, Negative Binomial, and Logistic (binary, binomial, ordinal) in detail\n\nUsed Pearson and deviance residuals to assess model fit and determine if new variables should be added to the model\nAddressed issues of overdispersion and zero-inflation\nUsed the properties of the one-parameter exponential family to identify the best link function for any GLM\n\n. . .\nEverything we’ve done thus far as been under the assumption that the observations are independent. Looking ahead we will consider models for data with dependent (correlated) observations."
  },
  {
    "objectID": "slides/06_logistic_ch6_o.html#acknowledgements",
    "href": "slides/06_logistic_ch6_o.html#acknowledgements",
    "title": "Logistic Regression",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThese slides are based on content in\n\nBMLR: Chapter 6 - Logistic Regression"
  },
  {
    "objectID": "slides/06_logistic_ch6_o.html#acknowledgements-1",
    "href": "slides/06_logistic_ch6_o.html#acknowledgements-1",
    "title": "Logistic Regression",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThese slides are based on content in\n\nBMLR: Chapter 6 - Logistic Regression\nInitial versions of the slides are by Dr. Maria Tackett, Duke University"
  },
  {
    "objectID": "slides/06_logistic_ch6.html#setup",
    "href": "slides/06_logistic_ch6.html#setup",
    "title": "Logistic Regression",
    "section": "Setup",
    "text": "Setup\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(GGally)\nlibrary(knitr)\nlibrary(patchwork)\nlibrary(viridis)\nlibrary(ggfortify)\nlibrary(gridExtra)"
  },
  {
    "objectID": "slides/06_logistic_ch6.html#learning-goals",
    "href": "slides/06_logistic_ch6.html#learning-goals",
    "title": "Logistic Regression",
    "section": "Learning goals",
    "text": "Learning goals\n\nIdentify Bernoulli and binomial random variables\nWrite GLM for binomial response variable\nInterpret the coefficients for a logistic regression model\nVisualizations for logistic regression\nInterpret coefficients and results from an ordinal logistic regression model\nSummarize GLMs for independent observations"
  },
  {
    "objectID": "slides/06_logistic_ch6.html#bernoulli-binomial-random-variables-12",
    "href": "slides/06_logistic_ch6.html#bernoulli-binomial-random-variables-12",
    "title": "Logistic Regression",
    "section": "Bernoulli + Binomial random variables (1/2)",
    "text": "Bernoulli + Binomial random variables (1/2)\nLogistic regression is used to analyze data with two types of responses:\n\nBinary: These responses take on two values success \\((Y = 1)\\) or failure \\((Y = 0)\\), yes \\((Y = 1)\\) or no \\((Y = 0)\\), etc.\n\n\n\\[P(Y = y) = p^y(1-p)^{1-y} \\hspace{10mm} y = 0, 1\\]"
  },
  {
    "objectID": "slides/06_logistic_ch6.html#bernoulli-binomial-random-variables-22",
    "href": "slides/06_logistic_ch6.html#bernoulli-binomial-random-variables-22",
    "title": "Logistic Regression",
    "section": "Bernoulli + Binomial random variables (2/2)",
    "text": "Bernoulli + Binomial random variables (2/2)\nLogistic regression is used to analyze data with two types of responses:\n\nBinomial: Number of successes in a Bernoulli process, \\(n\\) independent trials with a constant probability of success \\(p\\).\n\n\n\\[P(Y = y) = {n \\choose y}p^{y}(1-p)^{n - y} \\hspace{10mm} y = 0, 1, \\ldots, n\\]\n\n\nIn both instances, the goal is to model \\(p\\) the probability of success."
  },
  {
    "objectID": "slides/06_logistic_ch6.html#binary-vs.-binomial-data",
    "href": "slides/06_logistic_ch6.html#binary-vs.-binomial-data",
    "title": "Logistic Regression",
    "section": "Binary vs. Binomial data",
    "text": "Binary vs. Binomial data\nFor each example, identify if the response is a Bernoulli or Binomial response\n\nUse median age and unemployment rate in a county to predict the percent of Obama votes in the county in the 2008 presidential election.\nUse GPA and MCAT scores to estimate the probability a student is accepted into medical school.\nUse sex, age, and smoking history to estimate the probability an individual has lung cancer.\nUse offensive and defensive statistics from the 2017-2018 NBA season to predict a team’s winning percentage."
  },
  {
    "objectID": "slides/06_logistic_ch6.html#logistic-regression-model",
    "href": "slides/06_logistic_ch6.html#logistic-regression-model",
    "title": "Logistic Regression",
    "section": "Logistic regression model",
    "text": "Logistic regression model\n\\(\\log\\Big(\\frac{p}{1-p}\\Big) = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\dots + \\beta_px_p\\)\n\nThe response variable, \\(\\log\\Big(\\frac{p}{1-p}\\Big)\\), is the log(odds) of success, i.e. the logit\nUse the model to calculate the probability of success \\(\\hat{p} = \\frac{e^{\\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\dots + \\beta_px_p}}{1 + e^{\\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\dots + \\beta_px_p}}\\)\nWhen the response is a Bernoulli random variable, the probabilities can be used to classify each observation as a success or failure"
  },
  {
    "objectID": "slides/06_logistic_ch6.html#logistic-vs-linear-regression-model",
    "href": "slides/06_logistic_ch6.html#logistic-vs-linear-regression-model",
    "title": "Logistic Regression",
    "section": "Logistic vs linear regression model",
    "text": "Logistic vs linear regression model\n\nPlotCode\n\n\n\n\n\n\n\nGraph from BMLR Chapter 6\n\n\n\n\n\n\n\nset.seed(0)\ndat &lt;- tibble(x=runif(200, -5, 10),\n                  p=exp(-2+1*x)/(1+exp(-2+1*x)),\n                  y=rbinom(200, 1, p),\n                  y2=.3408+.0901*x,\n                  logit=log(p/(1-p)))\ndat2 &lt;- tibble(x = c(dat$x, dat$x),\n               y = c(dat$y2, dat$p),\n               `Regression model` = c(rep(\"linear\", 200),\n                                      rep(\"logistic\", 200)))\n\nggplot() + \n  geom_point(data = dat, aes(x, y)) +\n  geom_line(data = dat2, aes(x, y, linetype = `Regression model`, \n                             color = `Regression model`)) + \n  labs(title = \"Linear vs. logistic regression models for binary response data\") + \n  scale_colour_manual(name = 'Regression model',\n                      values = c('blue', 'red'), \n                      labels = c('linear', 'logistic'), guide ='legend')"
  },
  {
    "objectID": "slides/06_logistic_ch6.html#logit-link",
    "href": "slides/06_logistic_ch6.html#logit-link",
    "title": "Logistic Regression",
    "section": "Logit link",
    "text": "Logit link\nBernoulli and Binomial random variables can be written in one-parameter exponential family form, \\(f(y;\\theta) = e^{[a(y)b(\\theta) + c(\\theta) + d(y)]}\\)\n\nBernoulli\n\\[f(y;p) = e^{y\\log(\\frac{p}{1-p}) + \\log(1-p)}\\]\n\n\nBinomial\n\\[f(y;n,p) = e^{y\\log(\\frac{p}{1-p}) + n\\log(1-p) + \\log{n \\choose y}}\\]\n\n\nThey have the same canonical link \\(b(p) = \\log\\big(\\frac{p}{1-p}\\big)\\)"
  },
  {
    "objectID": "slides/06_logistic_ch6.html#assumptions-for-logistic-regression",
    "href": "slides/06_logistic_ch6.html#assumptions-for-logistic-regression",
    "title": "Logistic Regression",
    "section": "Assumptions for logistic regression",
    "text": "Assumptions for logistic regression\nThe following assumptions need to be satisfied to use logistic regression to make inferences\n\n1️⃣ \\(\\hspace{0.5mm}\\) Binary response: The response is dichotomous (has two possible outcomes) or is the sum of dichotomous responses\n2️⃣ \\(\\hspace{0.5mm}\\) Independence: The observations must be independent of one another\n3️⃣ \\(\\hspace{0.5mm}\\) Variance structure: Variance of a binomial random variable is \\(np(1-p)\\) \\((n = 1 \\text{ for Bernoulli})\\) , so the variability is highest when \\(p = 0.5\\)\n4️⃣ \\(\\hspace{0.5mm}\\) Linearity: The log of the odds ratio, \\(\\log\\big(\\frac{p}{1-p}\\big)\\), must be a linear function of the predictors \\(x_1, \\ldots, x_p\\)"
  },
  {
    "objectID": "slides/06_logistic_ch6.html#covid-19-infection-prevention-practices-at-food-establishments",
    "href": "slides/06_logistic_ch6.html#covid-19-infection-prevention-practices-at-food-establishments",
    "title": "Logistic Regression",
    "section": "COVID-19 infection prevention practices at food establishments",
    "text": "COVID-19 infection prevention practices at food establishments\nResearchers at Wollo Univeristy in Ethiopia conducted a study in July and August 2020 to understand factors associated with good COVID-19 infection prevention practices at food establishments. Their study is published in Andualem et al. (2022)\n\nThey were particularly interested in the understanding implementation of prevention practices at food establishments, given the workers’ increased risk due to daily contact with customers.\n\n\nAndualem, A., Tegegne, B., Ademe, S., Natnael, T., Berihun, G., Abebe, M., … & Adane, M. (2022). COVID-19 infection prevention practices among a sample of food handlers of food and drink establishments in Ethiopia. PloS one, 17(1), e0259851."
  },
  {
    "objectID": "slides/06_logistic_ch6.html#the-data",
    "href": "slides/06_logistic_ch6.html#the-data",
    "title": "Logistic Regression",
    "section": "The data",
    "text": "The data\n“An institution-based cross-sectional study was conducted among 422 food handlers in Dessie City and Kombolcha Town food and drink establishments in July and August 2020. The study participants were selected using a simple random sampling technique. Data were collected by trained data collectors using a pretested structured questionnaire and an on-the-spot observational checklist.”\n\n\nAndualem, A., Tegegne, B., Ademe, S., Natnael, T., Berihun, G., Abebe, M., … & Adane, M. (2022). COVID-19 infection prevention practices among a sample of food handlers of food and drink establishments in Ethiopia. PloS one, 17(1), e0259851."
  },
  {
    "objectID": "slides/06_logistic_ch6.html#response-variable",
    "href": "slides/06_logistic_ch6.html#response-variable",
    "title": "Logistic Regression",
    "section": "Response variable",
    "text": "Response variable\n“The outcome variable of this study was the good or poor practices of COVID-19 infection prevention among food handlers. Nine yes/no questions, one observational checklist and five multiple choice infection prevention practices questions were asked with a minimum score of 1 and maximum score of 25. Good infection prevention practice (the variable of interest) was determined for food handlers who scored 75% or above, whereas poor infection prevention practices refers to those food handlers who scored below 75% on the practice questions.”\n\n\nAndualem, A., Tegegne, B., Ademe, S., Natnael, T., Berihun, G., Abebe, M., … & Adane, M. (2022). COVID-19 infection prevention practices among a sample of food handlers of food and drink establishments in Ethiopia. PloS one, 17(1), e0259851."
  },
  {
    "objectID": "slides/06_logistic_ch6.html#results",
    "href": "slides/06_logistic_ch6.html#results",
    "title": "Logistic Regression",
    "section": "Results",
    "text": "Results\n\n\n\nAndualem, A., Tegegne, B., Ademe, S., Natnael, T., Berihun, G., Abebe, M., … & Adane, M. (2022). COVID-19 infection prevention practices among a sample of food handlers of food and drink establishments in Ethiopia. PloS one, 17(1), e0259851."
  },
  {
    "objectID": "slides/06_logistic_ch6.html#interpreting-the-results",
    "href": "slides/06_logistic_ch6.html#interpreting-the-results",
    "title": "Logistic Regression",
    "section": "Interpreting the results",
    "text": "Interpreting the results\n\nIs the response a Bernoulli or Binomial?\nWhat is the strongest predictor of having good COVID-19 infection prevention practices?\n\nIt’s often unreliable to look answer this question just based on the model output. Why are we able to answer this question based on the model output in this case?\n\nDescribe the effect (coefficient interpretation and inference) of having COVID-19 infection prevention policies available at the food establishment.\nThe intercept describes what group of food handlers?"
  },
  {
    "objectID": "slides/06_logistic_ch6.html#access-to-personal-protective-equipment",
    "href": "slides/06_logistic_ch6.html#access-to-personal-protective-equipment",
    "title": "Logistic Regression",
    "section": "Access to personal protective equipment",
    "text": "Access to personal protective equipment\nWe will use the data from Andualem et al. (2022) to explore the association between age, sex, years of service, and whether someone works at a food establishment with access to personal protective equipment (PPE) as of August 2020. We will use access to PPE as a proxy for wearing PPE.\n\nPlotCode\n\n\n\n\n\n\n\nage\nsex\nyears\nppe_access\n\n\n\n\n34\nMale\n2\n1\n\n\n32\nFemale\n3\n1\n\n\n32\nFemale\n1\n1\n\n\n40\nMale\n4\n1\n\n\n32\nMale\n10\n1\n\n\n\n\n\n\n\n\ncovid_df &lt;- read_csv(\"data/covid-prevention-study.csv\") |&gt;\n  rename(age = \"Age of food handlers\", \n         years = \"Years of service\", \n         ppe_access = \"Availability of PPEs\") |&gt;\n  mutate(sex = factor(if_else(Sex == 2, \"Female\", \"Male\"))) |&gt;\n  select(age, sex, years, ppe_access) \n\ncovid_df |&gt; slice(1:5) |&gt; kable()"
  },
  {
    "objectID": "slides/06_logistic_ch6.html#eda-for-binary-response-12",
    "href": "slides/06_logistic_ch6.html#eda-for-binary-response-12",
    "title": "Logistic Regression",
    "section": "EDA for binary response (1/2)",
    "text": "EDA for binary response (1/2)\n\nlibrary(Stat2Data)\npar(mfrow = c(1, 2))\nemplogitplot1(ppe_access ~ age, data = covid_df, ngroups = 10)\nemplogitplot1(ppe_access ~ years, data = covid_df, ngroups = 5)"
  },
  {
    "objectID": "slides/06_logistic_ch6.html#eda-for-binary-response-22",
    "href": "slides/06_logistic_ch6.html#eda-for-binary-response-22",
    "title": "Logistic Regression",
    "section": "EDA for binary response (2/2)",
    "text": "EDA for binary response (2/2)\n\nplotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlibrary(viridis)\nggplot(data = covid_df, aes(x = sex, fill = factor(ppe_access))) + \n  geom_bar(position = \"fill\")  +\n  labs(x = \"Sex\", \n       fill = \"PPE Access\", \n       title = \"PPE Access by Sex\") + \n  scale_fill_viridis_d()"
  },
  {
    "objectID": "slides/06_logistic_ch6.html#model-results",
    "href": "slides/06_logistic_ch6.html#model-results",
    "title": "Logistic Regression",
    "section": "Model results",
    "text": "Model results\n\nppe_model &lt;- glm(factor(ppe_access) ~ age + sex + years, data = covid_df, \n                 family = binomial)\ntidy(ppe_model, conf.int = TRUE) |&gt;\n  kable(digits = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n-2.127\n0.458\n-4.641\n0.000\n-3.058\n-1.257\n\n\nage\n0.056\n0.017\n3.210\n0.001\n0.023\n0.091\n\n\nsexMale\n0.341\n0.224\n1.524\n0.128\n-0.098\n0.780\n\n\nyears\n0.264\n0.066\n4.010\n0.000\n0.143\n0.401"
  },
  {
    "objectID": "slides/06_logistic_ch6.html#visualizing-coefficient-estimates",
    "href": "slides/06_logistic_ch6.html#visualizing-coefficient-estimates",
    "title": "Logistic Regression",
    "section": "Visualizing coefficient estimates",
    "text": "Visualizing coefficient estimates\n\nmodel_coef &lt;- tidy(ppe_model, exponentiate = TRUE, conf.int = TRUE)\n\n\nggplot(data = model_coef, aes(x = term, y = estimate)) +\n  geom_point() +\n  geom_hline(yintercept = 1, lty = 2) + \n  geom_pointrange(aes(ymin = conf.low, ymax = conf.high))+\n  labs(title = \"Exponentiated model coefficients\") + \n  coord_flip()"
  },
  {
    "objectID": "slides/06_logistic_ch6.html#data-supporting-railroads-in-the-1870s",
    "href": "slides/06_logistic_ch6.html#data-supporting-railroads-in-the-1870s",
    "title": "Logistic Regression",
    "section": "Data: Supporting railroads in the 1870s",
    "text": "Data: Supporting railroads in the 1870s\nThe data set RR_Data_Hale.csv contains information on support for referendums related to railroad subsidies for 11 communities in Alabama in the 1870s. The data were originally analyzed as part of a thesis project by a student at St. Olaf College. The variables in the data are\n\npctBlack: percentage of Black residents in the county\ndistance: distance the proposed railroad is from the community (in miles)\nYesVotes: number of “yes” votes in favor of the proposed railroad line\nNumVotes: number of votes cast in the election\n\n\nPrimary question: Was voting on the railroad referendum related to the distance from the proposed railroad line, after adjusting for the racial composition of a community?"
  },
  {
    "objectID": "slides/06_logistic_ch6.html#the-data-1",
    "href": "slides/06_logistic_ch6.html#the-data-1",
    "title": "Logistic Regression",
    "section": "The data",
    "text": "The data\n\nrr &lt;- read_csv(\"data/RR_Data_Hale.csv\")\nrr |&gt; slice(1:5) |&gt; kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCounty\npopBlack\npopWhite\npopTotal\npctBlack\ndistance\nYesVotes\nNumVotes\n\n\n\n\nCarthage\n841\n599\n1440\n58.40\n17\n61\n110\n\n\nCederville\n1774\n146\n1920\n92.40\n7\n0\n15\n\n\nFive Mile Creek\n140\n626\n766\n18.28\n15\n4\n42\n\n\nGreensboro\n1425\n975\n2400\n59.38\n0\n1790\n1804\n\n\nHarrison\n443\n355\n798\n55.51\n7\n0\n15"
  },
  {
    "objectID": "slides/06_logistic_ch6.html#eda-13",
    "href": "slides/06_logistic_ch6.html#eda-13",
    "title": "Logistic Regression",
    "section": "EDA (1/3)",
    "text": "EDA (1/3)\n\nrr &lt;- rr |&gt;\n  mutate(pctYes = YesVotes/NumVotes, \n         emp_logit = log(pctYes / (1 - pctYes)))\n\nrr |&gt; head(5)\n\n# A tibble: 5 × 10\n  County   popBlack popWhite popTotal pctBlack distance YesVotes NumVotes pctYes emp_logit\n  &lt;chr&gt;       &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;\n1 Carthage      841      599     1440     58.4       17       61      110 0.555      0.219\n2 Cedervi…     1774      146     1920     92.4        7        0       15 0       -Inf    \n3 Five Mi…      140      626      766     18.3       15        4       42 0.0952    -2.25 \n4 Greensb…     1425      975     2400     59.4        0     1790     1804 0.992      4.85 \n5 Harrison      443      355      798     55.5        7        0       15 0       -Inf"
  },
  {
    "objectID": "slides/06_logistic_ch6.html#eda-23",
    "href": "slides/06_logistic_ch6.html#eda-23",
    "title": "Logistic Regression",
    "section": "EDA (2/3)",
    "text": "EDA (2/3)\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\np1 &lt;- ggplot(data = rr, aes(x = distance, y = emp_logit)) + \n  geom_point() + \n  geom_smooth(method  = \"lm\", se = FALSE) + \n  labs(x = \"Distance to proposed railroad\", \n       y = \" \")\n  \np2 &lt;- ggplot(data = rr, aes(x = pctBlack, y = emp_logit)) + \n  geom_point() + \n  geom_smooth(method  = \"lm\", se = FALSE) + \n  labs(x = \"% Black residents\", \n       y = \"\")\np1 + p2 + plot_annotation(title = \"Log(odds yes vote) vs. predictor variables\")"
  },
  {
    "objectID": "slides/06_logistic_ch6.html#eda-33",
    "href": "slides/06_logistic_ch6.html#eda-33",
    "title": "Logistic Regression",
    "section": "EDA (3/3)",
    "text": "EDA (3/3)\n\nrr &lt;- rr |&gt;\n  mutate(inFavor = if_else(pctYes &gt; 0.5, \"Yes\", \"No\"))\n\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(data = rr, aes(x = distance, y = pctBlack, color = inFavor)) + \n  geom_point() + \n  geom_smooth(method  = \"lm\", se = FALSE, aes(lty = inFavor)) + \n  labs(x = \"Distance to proposed railroad\", \n       y = \"% Black residents\",\n       title = \"% Black residents vs. distance\", \n       subtitle = \"Based on vote outcome\") + \n  scale_color_viridis_d(end = 0.85)\n\n\n\n\n\nCheck for potential multicollinearity and interaction effect."
  },
  {
    "objectID": "slides/06_logistic_ch6.html#model",
    "href": "slides/06_logistic_ch6.html#model",
    "title": "Logistic Regression",
    "section": "Model",
    "text": "Model\nLet \\(p\\) be the percent of yes votes in a county. We’ll start by fitting the following model:\n\\[\\log\\Big(\\frac{p}{1-p}\\Big)  = \\beta_0 + \\beta_1 ~ dist + \\beta_2 ~ pctBlack\\]\n\nLikelihood\n\\[\\begin{aligned}L(p) &= \\prod_{i=1}^{n} {m_i \\choose y_i}p_i^{y_i}(1 - p_i)^{m_i - y_i} \\\\\n&= \\prod_{i=1}^{n} {m_i \\choose y_i}\\Big[\\frac{e^{\\beta_0 + \\beta_1 ~ dist_i + \\beta_2 ~ pctBlack_i}}{1 + e^{\\beta_0 + \\beta_1 ~ dist_i + \\beta_2 ~ pctBlack_i}}\\Big]^{y_i}\\Big[\\frac{1}{e^{\\beta_0 + \\beta_1 ~ dist_i + \\beta_2 ~ pctBlack_i}}\\Big]^{m_i - y_i} \\\\\\end{aligned}\\]\nUse IWLS to find \\(\\hat{\\beta}_0, \\hat{\\beta}_1, \\hat{\\beta}_2\\)."
  },
  {
    "objectID": "slides/06_logistic_ch6.html#model-in-r",
    "href": "slides/06_logistic_ch6.html#model-in-r",
    "title": "Logistic Regression",
    "section": "Model in R",
    "text": "Model in R\n\nrr_model &lt;- glm(cbind(YesVotes, NumVotes - YesVotes) ~ distance + pctBlack, \n                data = rr, family = binomial)\ntidy(rr_model, conf.int = TRUE) |&gt;\n  kable(digits = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n4.222\n0.297\n14.217\n0.000\n3.644\n4.809\n\n\ndistance\n-0.292\n0.013\n-22.270\n0.000\n-0.318\n-0.267\n\n\npctBlack\n-0.013\n0.004\n-3.394\n0.001\n-0.021\n-0.006\n\n\n\n\n\n\n\\[\\log\\Big(\\frac{\\hat{p}}{1-\\hat{p}}\\Big)  = 4.22 - 0.292 ~ dist - 0.013 ~ pctBlack\\]\n\n\n\nSee Section 6.5 of Generalized Linear Models with Examples in R by Dunn and Smyth (available through Duke library) for details on estimating the standard errors."
  },
  {
    "objectID": "slides/06_logistic_ch6.html#residuals",
    "href": "slides/06_logistic_ch6.html#residuals",
    "title": "Logistic Regression",
    "section": "Residuals",
    "text": "Residuals\nSimilar to Poisson regression, there are two types of residuals: Pearson and deviance residuals\n\nPearson residuals\n\\[\\text{Pearson residual}_i = \\frac{\\text{actual count} - \\text{predicted count}}{\\text{SD count}} = \\frac{Y_i - m_i\\hat{p}_i}{\\sqrt{m_i\\hat{p}_i(1 - \\hat{p}_i)}}\\]\n\n\nDeviance residuals\n\\[d_i = \\text{sign}(Y_i - m_i\\hat{p}_i)\\sqrt{2\\Big[Y_i\\log\\Big(\\frac{Y_i}{m_i\\hat{p}_i}\\Big) + (m_i - Y_i)\\log\\Big(\\frac{m_i - Y_i}{m_i - m_i\\hat{p}_i}\\Big)\\Big]}\\]"
  },
  {
    "objectID": "slides/06_logistic_ch6.html#plot-of-deviance-residuals",
    "href": "slides/06_logistic_ch6.html#plot-of-deviance-residuals",
    "title": "Logistic Regression",
    "section": "Plot of deviance residuals",
    "text": "Plot of deviance residuals\n\nModelResidual PlotPlot Code\n\n\n\nrr_int_model &lt;- glm(cbind(YesVotes, NumVotes - YesVotes) ~ distance + pctBlack +\n                      distance*pctBlack, \n                data = rr, family = binomial)\n\n\nrr_int_aug &lt;- augment(rr_int_model, type.predict = \"response\", \n                        type.residuals = \"deviance\")\n\nrr_int_aug |&gt; slice(1:5) |&gt; kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n.rownames\ncbind(YesVotes, NumVotes - YesVotes)\ndistance\npctBlack\n.fitted\n.resid\n.hat\n.sigma\n.cooksd\n.std.resid\n\n\n\n\n1\n61\n49\n17\n58.40\n0.2075801\n7.964510\n0.4663943\n5.0884957\n32.9667672\n\n\n2\n0\n15\n7\n92.40\n0.6776101\n-5.827504\n0.0492925\n6.3049338\n0.4298496\n\n\n3\n4\n38\n15\n18.28\n0.2024659\n-1.885115\n0.6433983\n6.6366201\n3.7828247\n\n\n4\n1790\n14\n0\n59.38\n0.9760416\n5.230746\n0.8996698\n0.5044966\n452.2582760\n\n\n5\n0\n15\n7\n55.51\n0.8513123\n-7.561561\n0.0240118\n5.9951340\n0.5412285\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(data = rr_int_aug, aes(x = .fitted, y = .resid)) + \n  geom_point() + \n  geom_hline(yintercept = 0, color = \"red\") + \n  labs(x = \"Fitted values\", \n       y = \"Deviance residuals\", \n       title = \"Deviance residuals vs. fitted\", \n       subtitle = \"for model with interaction term\")"
  },
  {
    "objectID": "slides/06_logistic_ch6.html#goodness-of-fit",
    "href": "slides/06_logistic_ch6.html#goodness-of-fit",
    "title": "Logistic Regression",
    "section": "Goodness of fit",
    "text": "Goodness of fit\nSimilar to Poisson regression, the sum of the squared deviance residuals is used to assess goodness of fit.\n\\[\\begin{aligned} &H_0: \\text{ Model is a good fit} \\\\\n&H_a: \\text{ Model is not a good fit}\\end{aligned}\\]\n\nWhen \\(m_i\\) is large and the model is a good fit \\((H_0 \\text{ true})\\) the residual deviance follows a \\(\\chi^2\\) distribution with \\(n - p\\) degrees of freedom.\n\nRecall \\(n - p\\) is the residual degrees of freedom.\n\nIf the model fits, we expect the residual deviance to be approximately what value?"
  },
  {
    "objectID": "slides/06_logistic_ch6.html#adjusting-for-overdispersion-12",
    "href": "slides/06_logistic_ch6.html#adjusting-for-overdispersion-12",
    "title": "Logistic Regression",
    "section": "Adjusting for overdispersion (1/2)",
    "text": "Adjusting for overdispersion (1/2)\n\nOverdispersion occurs when there is extra-binomial variation, i.e. the variance is greater than what we would expect, \\(np(1-p)\\).\nSimilar to Poisson regression, we can adjust for overdispersion in the binomial regression model by using a dispersion parameter \\[\\hat{\\phi} = \\sum \\frac{(\\text{Pearson residuals})^2}{n-p}\\]\n\nBy multiplying by \\(\\hat{\\phi}\\), we are accounting for the reduction in information we would expect from independent observations."
  },
  {
    "objectID": "slides/06_logistic_ch6.html#adjusting-for-overdispersion-22",
    "href": "slides/06_logistic_ch6.html#adjusting-for-overdispersion-22",
    "title": "Logistic Regression",
    "section": "Adjusting for overdispersion (2/2)",
    "text": "Adjusting for overdispersion (2/2)\n\nWe adjust for overdispersion using a quasibinomial model.\n\n“Quasi” reflects the fact we are no longer using a binomial model with true likelihood.\n\nThe standard errors of the coefficients are \\(SE_{Q}(\\hat{\\beta}_j) = \\sqrt{\\hat{\\phi}} SE(\\hat{\\beta})\\)\n\nInference is done using the \\(t\\) distribution to account for extra variability"
  },
  {
    "objectID": "slides/06_logistic_ch6.html#predicting-ed-wait-and-treatment-times",
    "href": "slides/06_logistic_ch6.html#predicting-ed-wait-and-treatment-times",
    "title": "Logistic Regression",
    "section": "Predicting ED wait and treatment times",
    "text": "Predicting ED wait and treatment times\nAtaman and Sariyer (2021) use ordinal logistic regression to predict patient wait and treatment times in an emergency department (ED). The goal is to identify relevant factors that can be used to inform recommendations for reducing wait and treatment times, thus improving the quality of care in the ED.\nData: Daily records for ED arrivals in August 2018 at a public hospital in Izmir, Turkey.\nResponse variable: Wait time, a categorical variable with three levels: - Patients who wait less than 10 minutes - Patients whose waiting time is in the range of 10-60 minutes - Patients who wait more than 60 minutes\n\n\nAtaman, M. G., & Sarıyer, G. (2021). Predicting waiting and treatment times in emergency departments using ordinal logistic regression models. The American Journal of Emergency Medicine, 46, 45-50."
  },
  {
    "objectID": "slides/06_logistic_ch6.html#ordinal-logistic-regression",
    "href": "slides/06_logistic_ch6.html#ordinal-logistic-regression",
    "title": "Logistic Regression",
    "section": "Ordinal logistic regression",
    "text": "Ordinal logistic regression\nLet \\(Y\\) be an ordinal response variable that takes levels \\(1, 2, \\ldots, J\\) with associated probabilities \\(p_1, p_2, \\ldots, p_J\\).\nThe proportional odds model can be written as the following:\n\\[\\begin{aligned}&\\log\\Big(\\frac{P(Y\\leq 1)}{P(Y &gt; 1)}\\Big) = \\beta_{01} + \\beta_1x_1 + \\dots +  \\beta_px_p \\\\\n& \\log\\Big(\\frac{P(Y\\leq 2)}{P(Y &gt; 2)}\\Big) = \\beta_{02} + \\beta_1x_1 + \\dots +  \\beta_px_p \\\\\n& \\dots \\\\\n& \\log\\Big(\\frac{P(Y\\leq J-1)}{P(Y &gt; J-1)}\\Big) = \\beta_{0{J-1}} + \\beta_1x_1 + \\dots +  \\beta_px_p\\end{aligned}\\]"
  },
  {
    "objectID": "slides/06_logistic_ch6.html#questions",
    "href": "slides/06_logistic_ch6.html#questions",
    "title": "Logistic Regression",
    "section": "Questions",
    "text": "Questions\n\nHow is the proportional odds model similar to the multinomial logistic model?\nHow is it different?"
  },
  {
    "objectID": "slides/06_logistic_ch6.html#effect-of-arrival-mode",
    "href": "slides/06_logistic_ch6.html#effect-of-arrival-mode",
    "title": "Logistic Regression",
    "section": "Effect of arrival mode",
    "text": "Effect of arrival mode\n\n\nQuestion\nThe variable arrival mode takes two categories: ambulance and walk-in. Describe the effect of arrival mode in this model. Note that the baseline level is “walk-in”."
  },
  {
    "objectID": "slides/06_logistic_ch6.html#effect-of-triage-level",
    "href": "slides/06_logistic_ch6.html#effect-of-triage-level",
    "title": "Logistic Regression",
    "section": "Effect of triage level",
    "text": "Effect of triage level\n\nConsider the full output with the ordinal logistic models for wait and treatment times.\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nUse the results from both models to describe the effect of triage level (red = urgent, green = non-urgent) on the wait and treatment times in the ED. Note that “red” is the baseline level. Is this what you expected?"
  },
  {
    "objectID": "slides/06_logistic_ch6.html#wrap-up",
    "href": "slides/06_logistic_ch6.html#wrap-up",
    "title": "Logistic Regression",
    "section": "Wrap up",
    "text": "Wrap up\n\nCovered fitting, interpreting, and drawing conclusions from GLMs\n\nLooked at Poisson, Negative Binomial, and Logistic (binary, binomial, ordinal) in detail\n\nUsed Pearson and deviance residuals to assess model fit and determine if new variables should be added to the model\nAddressed issues of overdispersion and zero-inflation\nUsed the properties of the one-parameter exponential family to identify the best link function for any GLM\n\n\nEverything we’ve done thus far as been under the assumption that the observations are independent. Looking ahead we will consider models for data with dependent (correlated) observations."
  },
  {
    "objectID": "slides/06_logistic_ch6.html#acknowledgements",
    "href": "slides/06_logistic_ch6.html#acknowledgements",
    "title": "Logistic Regression",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThese slides are based on content in\n\nBMLR: Chapter 6 - Logistic Regression"
  },
  {
    "objectID": "slides/06_logistic_ch6.html#acknowledgements-1",
    "href": "slides/06_logistic_ch6.html#acknowledgements-1",
    "title": "Logistic Regression",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThese slides are based on content in\n\nBMLR: Chapter 6 - Logistic Regression\nInitial versions of the slides are by Dr. Maria Tackett, Duke University"
  },
  {
    "objectID": "slides/08_multilevel_mod_ch8_o.html",
    "href": "slides/08_multilevel_mod_ch8_o.html",
    "title": "Multilevel Models",
    "section": "",
    "text": "library(tidyverse)\nlibrary(tidymodels)\nlibrary(GGally)\nlibrary(knitr)\nlibrary(patchwork)\nlibrary(viridis)\nlibrary(ggfortify)\nlibrary(kableExtra)\nlibrary(lme4)\nlibrary(broom.mixed)"
  },
  {
    "objectID": "slides/08_multilevel_mod_ch8_o.html#setup",
    "href": "slides/08_multilevel_mod_ch8_o.html#setup",
    "title": "Multilevel Models",
    "section": "",
    "text": "library(tidyverse)\nlibrary(tidymodels)\nlibrary(GGally)\nlibrary(knitr)\nlibrary(patchwork)\nlibrary(viridis)\nlibrary(ggfortify)\nlibrary(kableExtra)\nlibrary(lme4)\nlibrary(broom.mixed)"
  },
  {
    "objectID": "slides/08_multilevel_mod_ch8_o.html#learning-goals-12",
    "href": "slides/08_multilevel_mod_ch8_o.html#learning-goals-12",
    "title": "Multilevel Models",
    "section": "Learning goals (1/2)",
    "text": "Learning goals (1/2)\n\nRecognize a potential for correlation in a data set\nIdentify observational units at varying levels\nUnderstand issues correlated data may cause in modeling\nUnderstand how random effects models can be used to take correlation into account\nUse EDA for multilevel data\nConduct univariate and bivariate EDA for multilevel models"
  },
  {
    "objectID": "slides/08_multilevel_mod_ch8_o.html#learning-goals-22",
    "href": "slides/08_multilevel_mod_ch8_o.html#learning-goals-22",
    "title": "Multilevel Models",
    "section": "Learning goals (2/2)",
    "text": "Learning goals (2/2)\n\nWrite multilevel model , including assumptions about variance components, in by-level and composite forms\nInterpret the model parameters, fixed effects, and variance components\nFit and interpret multilevel models\nCompare maximum likelihood (ML) and restricted maximum likelihood (REML) estimation approaches\nUnderstand general process for fitting and comparing multilevel models"
  },
  {
    "objectID": "slides/08_multilevel_mod_ch8_o.html#multilevel-data",
    "href": "slides/08_multilevel_mod_ch8_o.html#multilevel-data",
    "title": "Multilevel Models",
    "section": "Multilevel data",
    "text": "Multilevel data\n\nWe can think of correlated data as a multilevel structure\n\nPopulation elements are aggregated into groups\nThere are observational units and measurements at each level\n\n\n. . .\n\nFor now we will focus on data with two levels:\n\nLevel one: Most basic level of observation\nLevel two: Groups formed from aggregated level-one observations"
  },
  {
    "objectID": "slides/08_multilevel_mod_ch8_o.html#two-types-of-effects",
    "href": "slides/08_multilevel_mod_ch8_o.html#two-types-of-effects",
    "title": "Multilevel Models",
    "section": "Two types of effects",
    "text": "Two types of effects\n\nFixed effects: Effects that are of interest in the study\n\nCan think of these as effects whose interpretations would be included in a write up of the study\n\n\n. . .\n\nRandom effects: Effects we’re not interested in studying but whose variability we want to understand\n\nCan think of these as effects whose interpretations would not necessarily be included in a write up of the study"
  },
  {
    "objectID": "slides/08_multilevel_mod_ch8_o.html#practice-questions",
    "href": "slides/08_multilevel_mod_ch8_o.html#practice-questions",
    "title": "Multilevel Models",
    "section": "Practice Questions",
    "text": "Practice Questions\nRadon is a carcinogen – a naturally occurring radioactive gas whose decay products are also radioactive – known to cause lung cancer in high concentrations. The EPA sampled more than 80,000 homes across the U.S. Each house came from a randomly selected county and measurements were made on each level of each home. Uranium measurements at the county level were included to improve the radon estimates.\n\nWhat is the most basic level of observation (Level One)?\nWhat are the group units (Level Two, Level Three, etc…)\nWhat is the response variable?\nDescribe the within-group variation.\nWhat are the fixed effects? What are the random effects?\n\n\n\nEx. 1 from Section 7.10.1 in BMLR"
  },
  {
    "objectID": "slides/08_multilevel_mod_ch8_o.html#data-music-performance-anxiety",
    "href": "slides/08_multilevel_mod_ch8_o.html#data-music-performance-anxiety",
    "title": "Multilevel Models",
    "section": "Data: Music performance anxiety",
    "text": "Data: Music performance anxiety\nThe data musicdata.csv come from the Sadler and Miller (2010) study of the emotional state of musicians before performances. The dataset contains information collected from 37 undergraduate music majors who completed the Positive Affect Negative Affect Schedule (PANAS), an instrument produces a measure of anxiety (negative affect) and a measure of happiness (positive affect). This analysis will focus on negative affect as a measure of performance anxiety.\nThe primary variables we’ll use are\n\nna: negative affect score on PANAS (the response variable)\nperform_type: type of performance (Solo, Large Ensemble, Small Ensemble)\ninstrument: type of intstrument (Voice, Orchestral, Piano)"
  },
  {
    "objectID": "slides/08_multilevel_mod_ch8_o.html#look-at-data",
    "href": "slides/08_multilevel_mod_ch8_o.html#look-at-data",
    "title": "Multilevel Models",
    "section": "Look at data",
    "text": "Look at data\n\nData ViewCode\n\n\n\n\n\n\n\nid\ndiary\nperform_type\nna\ngender\ninstrument\n\n\n\n\n1\n1\nSolo\n11\nFemale\nvoice\n\n\n1\n2\nLarge Ensemble\n19\nFemale\nvoice\n\n\n1\n3\nLarge Ensemble\n14\nFemale\nvoice\n\n\n43\n1\nSolo\n19\nFemale\nvoice\n\n\n43\n2\nSolo\n13\nFemale\nvoice\n\n\n43\n3\nSmall Ensemble\n19\nFemale\nvoice\n\n\n\n\n\n\n\n\nmusic &lt;- read_csv(\"data/musicdata.csv\")\nmusic %&gt;%\n  filter(id %in% c(1, 43)) %&gt;%\n  group_by(id) %&gt;%\n  slice(1:3) %&gt;%\n  select(id, diary, perform_type, na, gender, instrument) %&gt;%\n  kable()\n\n\n\n\n\nWhat are the Level One observations? Level Two observations?\nWhat are the Level One variables? Level Two variables?"
  },
  {
    "objectID": "slides/08_multilevel_mod_ch8_o.html#univariate-exploratory-data-analysis",
    "href": "slides/08_multilevel_mod_ch8_o.html#univariate-exploratory-data-analysis",
    "title": "Multilevel Models",
    "section": "Univariate exploratory data analysis",
    "text": "Univariate exploratory data analysis\nLevel One variables\nTwo ways to approach univariate EDA (visualizations and summary statistics) for Level One variables:\n\nUse individual observations (i.e., treat observations as independent)\nUse aggregated values for each Level Two observation\n\n. . .\nLevel Two variables\n\nUse a data set that contains one row per Level Two observation\n\n. . .\nLet’s do some Univariate EDA together"
  },
  {
    "objectID": "slides/08_multilevel_mod_ch8_o.html#unviariate-eda",
    "href": "slides/08_multilevel_mod_ch8_o.html#unviariate-eda",
    "title": "Multilevel Models",
    "section": "Unviariate EDA",
    "text": "Unviariate EDA\n\nPlotsCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\np1 &lt;- ggplot(data = music, aes(x = na)) + \n  geom_histogram(fill = \"steelblue\", color = \"black\", binwidth = 2) + \n  labs(x = \"Individual negative affect\", \n       title = \"Negative affect scores\")\n\np2 &lt;- music %&gt;%\n  group_by(id) %&gt;%\n  summarise(mean_na = mean(na)) %&gt;%\n  ggplot(aes(x = mean_na)) + \n  geom_histogram(fill = \"steelblue\", color = \"black\", binwidth = 2) + \n  labs(x = \"Mean negative affect\", \n       title = \"Mean negative affect scores\")\n\np1 + p2"
  },
  {
    "objectID": "slides/08_multilevel_mod_ch8_o.html#bivariate-exploratory-data-analysis",
    "href": "slides/08_multilevel_mod_ch8_o.html#bivariate-exploratory-data-analysis",
    "title": "Multilevel Models",
    "section": "Bivariate exploratory data analysis",
    "text": "Bivariate exploratory data analysis\nGoals\n\nExplore general association between the predictor and response variable\nExplore whether subjects at a given level of the predictor tend to have similar mean responses\nExplore whether variation in response differs at different levels of a predictor\n\n. . .\nThere are two ways to visualize these associations:\n\nOne plot of individual observations (i.e., treat observations as independent)\nSeparate plots of responses vs. predictor for each Level Two observation (lattice plots)\n\n. . .\nNow lets do some Bivariate EDA."
  },
  {
    "objectID": "slides/08_multilevel_mod_ch8_o.html#questions-we-want-to-answer",
    "href": "slides/08_multilevel_mod_ch8_o.html#questions-we-want-to-answer",
    "title": "Multilevel Models",
    "section": "Questions we want to answer",
    "text": "Questions we want to answer\nThe goal is to understand variability in performance anxiety (na) based on performance-level and musician-level characteristics. Specifically:\n\nWhat is the association between performance type (large ensemble or not) and performance anxiety? Does the association differ based on instrument type (orchestral or not)?\n\n. . .\nWhat is the problem with using the following model to draw conclusions?\n\nModel OutputCode\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n15.721\n0.359\n43.778\n0.000\n\n\norchestra\n1.789\n0.552\n3.243\n0.001\n\n\nlarge_ensemble\n-0.277\n0.791\n-0.350\n0.727\n\n\norchestra:large_ensemble\n-1.709\n1.062\n-1.609\n0.108\n\n\n\n\n\n\n\n\nmusic &lt;- music %&gt;%\n  mutate(orchestra = if_else(instrument == \"orchestral instrument\", 1, 0), \n         large_ensemble = if_else(perform_type == \"Large Ensemble\", 1,0))\n\nols &lt;- lm(na ~ orchestra + large_ensemble + orchestra * large_ensemble, \n          data = music)\ntidy(ols) %&gt;% kable(digits = 3)"
  },
  {
    "objectID": "slides/08_multilevel_mod_ch8_o.html#other-modeling-approaches",
    "href": "slides/08_multilevel_mod_ch8_o.html#other-modeling-approaches",
    "title": "Multilevel Models",
    "section": "Other modeling approaches",
    "text": "Other modeling approaches\n1️⃣ Condense each musician’s set of responses into a single outcome (e.g., mean max, last observation, etc.) and fit a linear model on these condensed observations\n\nLeaves few observations (37) to fit the model\nIgnoring a lot of information in the multiple observations for each musician\n\n. . .\n2️⃣ Fit a separate model for each musician understand the association between performance type (Level One models). Then fit a system of Level Two models to predict the fitted coefficients in the Level One model for each subject based on instrument type (Level Two model).\n. . .\nLet’s look at approach #2"
  },
  {
    "objectID": "slides/08_multilevel_mod_ch8_o.html#two-level-model",
    "href": "slides/08_multilevel_mod_ch8_o.html#two-level-model",
    "title": "Multilevel Models",
    "section": "Two-level model",
    "text": "Two-level model\nWe’ll start with the Level One model to understand the association between performance type and performance anxiety for the \\(i^{th}\\) musician.\n\\[na_{ij} = a_i + b_i ~ LargeEnsemble_{ij} + \\epsilon_i, \\hspace{5mm} \\epsilon_{ij} \\sim N(0,\\sigma^2)\\]\n. . .\nWhy is it more meaningful to use performance type for the Level One model than instrument?\n. . .\nFor now, estimate \\(a_i\\) and \\(b_i\\) using least-squares regression."
  },
  {
    "objectID": "slides/08_multilevel_mod_ch8_o.html#example-level-one-model",
    "href": "slides/08_multilevel_mod_ch8_o.html#example-level-one-model",
    "title": "Multilevel Models",
    "section": "Example Level One model",
    "text": "Example Level One model\nBelow is partial data for observation #22\n\nmusic %&gt;%\n  filter(id == 22) %&gt;%\n  select(id, diary, perform_type, instrument, na) %&gt;%\n  slice(1:3, 13:15) %&gt;%\n  kable()\n\n\n\n\nid\ndiary\nperform_type\ninstrument\nna\n\n\n\n\n22\n1\nSolo\norchestral instrument\n24\n\n\n22\n2\nLarge Ensemble\norchestral instrument\n21\n\n\n22\n3\nLarge Ensemble\norchestral instrument\n14\n\n\n22\n13\nLarge Ensemble\norchestral instrument\n12\n\n\n22\n14\nLarge Ensemble\norchestral instrument\n19\n\n\n22\n15\nSolo\norchestral instrument\n25"
  },
  {
    "objectID": "slides/08_multilevel_mod_ch8_o.html#level-one-model",
    "href": "slides/08_multilevel_mod_ch8_o.html#level-one-model",
    "title": "Multilevel Models",
    "section": "Level One model",
    "text": "Level One model\n\nModelCodeSlopes and Intercepts\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n24.500\n1.96\n12.503\n0.000\n\n\nlarge_ensemble\n-7.833\n2.53\n-3.097\n0.009\n\n\n\n\n\n\n\n\nmusic %&gt;%\n  filter(id == 22) %&gt;%\n  lm(na ~ large_ensemble, data = .) %&gt;%\n  tidy() %&gt;%\n  kable(digits = 3)\n\n. . .\nRepeat for all 37 musicians\n\n\n\n\n\n\n\nRecreated from BMLR Figure 8.9\n\n\n\n\n. . .\nNow let’s consider if there is an association between the estimated slopes, estimated intercepts, and the type of instrument"
  },
  {
    "objectID": "slides/08_multilevel_mod_ch8_o.html#level-two-model",
    "href": "slides/08_multilevel_mod_ch8_o.html#level-two-model",
    "title": "Multilevel Models",
    "section": "Level Two Model",
    "text": "Level Two Model\nThe slope and intercept for the \\(i^{th}\\) musician can be modeled as\n\\[\\begin{aligned}&a_i = \\alpha_0 + \\alpha_1 ~ Orchestra_i + u_i \\\\\n&b_i = \\beta_0 + \\beta_1 ~ Orchestra_i + v_i\\end{aligned}\\]\n. . .\nNote the response variable in the Level Two models are not observed outcomes but the (fitted) slope and intercept from each musician\n. . .\nLet’s fit a level 2 model"
  },
  {
    "objectID": "slides/08_multilevel_mod_ch8_o.html#estimated-coefficients-by-instrument",
    "href": "slides/08_multilevel_mod_ch8_o.html#estimated-coefficients-by-instrument",
    "title": "Multilevel Models",
    "section": "Estimated coefficients by instrument",
    "text": "Estimated coefficients by instrument\n\nOutputCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmusicians &lt;- music %&gt;%\n  distinct(id, orchestra) %&gt;%\n  bind_cols(model_stats)\n\np1 &lt;- ggplot(data = musicians, aes(x = intercepts, y = factor(orchestra))) + \n  geom_boxplot(fill = \"steelblue\", color = \"black\") + \n  labs(x = \"Fitted intercepts\", \n       y = \"Orchestra\")\n\np2 &lt;- ggplot(data = musicians, aes(x = slopes, y = factor(orchestra))) + \n  geom_boxplot(fill = \"steelblue\", color = \"black\") + \n  labs(x = \"Fitted slopes\", \n       y = \"Orchestra\")\n\np1 / p2"
  },
  {
    "objectID": "slides/08_multilevel_mod_ch8_o.html#level-two-model-1",
    "href": "slides/08_multilevel_mod_ch8_o.html#level-two-model-1",
    "title": "Multilevel Models",
    "section": "Level Two model",
    "text": "Level Two model\n\nModelsCode\n\n\nModel for intercepts\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n16.283\n0.671\n24.249\n0.000\n\n\norchestra\n1.411\n0.991\n1.424\n0.163\n\n\n\n\n\nModel for slopes\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-0.771\n0.851\n-0.906\n0.373\n\n\norchestra\n-1.406\n1.203\n-1.168\n0.253\n\n\n\n\n\n\n\nModel for intercepts\n\na &lt;- lm(intercepts ~ orchestra, data = musicians) \ntidy(a) %&gt;%\n  kable(digits = 3)\n\nModel for slopes\n\nb &lt;- lm(slopes ~ orchestra, data = musicians) \ntidy(b) %&gt;%\n  kable(digits = 3)"
  },
  {
    "objectID": "slides/08_multilevel_mod_ch8_o.html#writing-out-the-models",
    "href": "slides/08_multilevel_mod_ch8_o.html#writing-out-the-models",
    "title": "Multilevel Models",
    "section": "Writing out the models",
    "text": "Writing out the models\nLevel One\n\\[\\hat{na}_{ij}  = \\hat{a}_i + \\hat{b}_i ~ LargeEnsemble_{ij}\\]\nfor each musician.\nLevel Two\n\\[\\begin{aligned}&\\hat{a}_i = 16.283 + 1.441 ~ Orchestra_i \\\\\n&\\hat{b}_i = -0.771 - 1.406 ~ Orchestra_i\\end{aligned}\\]"
  },
  {
    "objectID": "slides/08_multilevel_mod_ch8_o.html#composite-model",
    "href": "slides/08_multilevel_mod_ch8_o.html#composite-model",
    "title": "Multilevel Models",
    "section": "Composite model",
    "text": "Composite model\n\\[\\begin{aligned}\\hat{na}_i &= 16.283 + 1.441 ~ Orchestra_i - 0.771 ~ LargeEnsemble_{ij} \\\\\n&- 1.406 ~ Orchestra:LargeEnsemble_{ij}\\end{aligned}\\]\n. . .\n(Note that we also have the error terms \\(\\epsilon_{ij}, u_i, v_i\\) that we will discuss soon)\n\nWhat is the predicted average performance anxiety before solos and small ensemble performances for vocalists and keyboardists? For those who place orchestral instruments?\nWhat is the predicted average performance anxiety before large ensemble performances for those who play orchestral instruments?"
  },
  {
    "objectID": "slides/08_multilevel_mod_ch8_o.html#disadvantages-to-this-approach",
    "href": "slides/08_multilevel_mod_ch8_o.html#disadvantages-to-this-approach",
    "title": "Multilevel Models",
    "section": "Disadvantages to this approach",
    "text": "Disadvantages to this approach\n⚠️ Weighs each musician the same regardless of number of diary entries\n⚠️ Drops subjects who have missing values for slope (7 individuals who didn’t play a large ensemble performance)\n⚠️ Does not share strength effectively across individuals\n. . .\nWe will use a unified approach that utilizes likelihood-based methods to address some of these drawbacks."
  },
  {
    "objectID": "slides/08_multilevel_mod_ch8_o.html#questions",
    "href": "slides/08_multilevel_mod_ch8_o.html#questions",
    "title": "Multilevel Models",
    "section": "Questions",
    "text": "Questions\n\nHow many Level One models are fit?\nHow many Level Two models are fit?"
  },
  {
    "objectID": "slides/08_multilevel_mod_ch8_o.html#framework",
    "href": "slides/08_multilevel_mod_ch8_o.html#framework",
    "title": "Multilevel Models",
    "section": "Framework",
    "text": "Framework\nLet \\(Y_{ij}\\) be the performance anxiety for the \\(i^{th}\\) musician before performance \\(j\\).\nLevel One\n\\[Y_{ij} = a_i + b_i ~ LargeEnsemble + \\epsilon_{ij}\\]\n. . .\nLevel Two\n\\[\\begin{aligned}&a_i = \\alpha_0 + \\alpha_1 ~ Orchestra_i+ u_i\\\\\n&b_i = \\beta_0 + \\beta_1~Orchestra_i + v_i\\end{aligned}\\]\n. . .\nThis approach uses likelihood-based methods (instead of least squares) to address the previously mentioned disadvantages"
  },
  {
    "objectID": "slides/08_multilevel_mod_ch8_o.html#creating-a-composite-model",
    "href": "slides/08_multilevel_mod_ch8_o.html#creating-a-composite-model",
    "title": "Multilevel Models",
    "section": "Creating a Composite Model",
    "text": "Creating a Composite Model\nPlug in the equations for \\(a_i\\) and \\(b_i\\) to get the composite model\n\n\n\\[Y_{ij} = a_i + b_i ~ LargeEnsemble + \\epsilon_{ij}\\]\n\n\\[\\begin{aligned}&a_i = \\alpha_0 + \\alpha_1 ~ Orchestra_i+ u_i\\\\\n&b_i = \\beta_0 + \\beta_1~Orchestra_i + v_i\\end{aligned}\\]\n\n\n. . .\n\\[\\begin{aligned}Y_{ij} &= a_i \\hspace{1.8in}+ \\hspace{1.62in} b_i ~ LargeEnsemble  \\hspace{2.2in}+ \\epsilon_{ij}\\\\\n&=\n(\\alpha_0 + \\alpha_1 ~ Orchestra_i+ u_i) + (\\beta_0 + \\beta_1~Orchestra_i + v_i)LargeEnsemble\\hspace{2.17in}+\\epsilon_{ij}\\\\\n&= \\space \\alpha_0 + \\alpha_1 ~ Orchestra_i+ u_i \\hspace{.2in} + \\beta_0LargeEnsemble+\\beta_1Orchestra_iLargeEnsemble+v_iLargeEnsemble+\\epsilon_{ij}\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/08_multilevel_mod_ch8_o.html#lmer-syntax",
    "href": "slides/08_multilevel_mod_ch8_o.html#lmer-syntax",
    "title": "Multilevel Models",
    "section": "lmer Syntax",
    "text": "lmer Syntax\nlmer(response~fixed effect variables + (random effect variables|group variable),data = your_data)\n\nwe determine fixed and random effects based on our composite model\nterms with Greek letters will be fixed effects\nterms with random components, usually \\(u\\), \\(v\\), or \\(w\\) will be random effects"
  },
  {
    "objectID": "slides/08_multilevel_mod_ch8_o.html#fitting-our-model-with-lmer",
    "href": "slides/08_multilevel_mod_ch8_o.html#fitting-our-model-with-lmer",
    "title": "Multilevel Models",
    "section": "Fitting our Model with lmer",
    "text": "Fitting our Model with lmer\n\\[\\begin{aligned} Y_{ij}&= \\space \\alpha_0 + \\alpha_1 ~ Orchestra_i+ u_i +\\space\\space\\space \\beta_0LargeEnsemble+\\beta_1Orchestra_iLargeEnsemble+v_iLargeEnsemble+\\epsilon_{ij}\\\\\n&=\\alpha_0 + \\alpha_1 ~ Orchestra_i + \\beta_0LargeEnsemble+\\beta_1Orchestra_iLargeEnsemble+u_i + v_iLargeEnsemble+\\epsilon_{ij}\n\\end{aligned}\\]\n\nAll varibles with greek letters are fixed effects and all variables with \\(u\\), \\(v\\) are random effects.\n\n. . .\nlmer(na ~ 1 + Orchastra + LargeEnsemble + Orchestra * LargeEnsemble + (1+LargeEnsemble|id),data = music)"
  },
  {
    "objectID": "slides/08_multilevel_mod_ch8_o.html#composite-model-1",
    "href": "slides/08_multilevel_mod_ch8_o.html#composite-model-1",
    "title": "Multilevel Models",
    "section": "Composite model",
    "text": "Composite model\nPlug in the equations for \\(a_i\\) and \\(b_i\\) to get the composite model\n\\[\\begin{aligned}Y_{ij} &= (\\alpha_0 + \\alpha_1 ~ Orchestra_i + \\beta_0 ~ LargeEnsemble_{ij} \\\\\n&+ \\beta_1 ~ Orchestra_i:LargeEnsemble_{ij})\\\\\n&+ (u_i + v_i ~ LargeEnsemble_{ij} + \\epsilon_{ij})\\end{aligned}\\]\n\nThe fixed effects to estimate are \\(\\alpha_0, \\alpha_1, \\beta_0, \\beta_1\\)\nThe error terms are \\(u_i, v_i, \\epsilon_{ij}\\)\n\n. . .\nNote that we no longer need to estimate \\(a_i\\) and \\(b_i\\) directly as we did earlier. They conceptually connect the Level One and Level Two models."
  },
  {
    "objectID": "slides/08_multilevel_mod_ch8_o.html#notation",
    "href": "slides/08_multilevel_mod_ch8_o.html#notation",
    "title": "Multilevel Models",
    "section": "Notation",
    "text": "Notation\n\nGreek letters denote the fixed effect model parameters to be estimated\n\ne.g., \\(\\alpha_0, \\alpha_1, \\beta_0, \\beta_1\\)\n\nRoman letters denote the preliminary fixed effects at lower levels that will not be estimated directly.\n\ne.g. \\(a_i, b_i\\)\n\n\\(\\sigma\\) and \\(\\rho\\) denote variance components that will be estimated\n\\(\\epsilon_{ij}, u_i, v_i\\) denote error terms"
  },
  {
    "objectID": "slides/08_multilevel_mod_ch8_o.html#error-terms",
    "href": "slides/08_multilevel_mod_ch8_o.html#error-terms",
    "title": "Multilevel Models",
    "section": "Error terms",
    "text": "Error terms\n\nWe generally assume that the error terms are normally distributed, e.g. error associated with each performance of a given musician is \\(\\epsilon_{ij} \\sim N(0, \\sigma^2)\\)\nFor the Level Two models, the errors are\n\n\\(u_i\\): deviation of musician \\(i\\) from the mean performance anxiety before solos and small ensembles after accounting for the instrument\n\\(v_i\\): deviance of musician \\(i\\) from the mean difference in performance anxiety between large ensembles and other performance types after accounting for instrument\n\nNeed to account for fact that \\(u_i\\) and \\(v_i\\) are correlated for the \\(i^{th}\\) musician"
  },
  {
    "objectID": "slides/08_multilevel_mod_ch8_o.html#intercept-and-slope",
    "href": "slides/08_multilevel_mod_ch8_o.html#intercept-and-slope",
    "title": "Multilevel Models",
    "section": "Intercept and Slope",
    "text": "Intercept and Slope\n\nPlotCode\n\n\n\n\n\n\n\nRecreated from Figure 8.11\n\n\n\n\nQuestion: Describe what we learn about the association between the slopes and intercepts based on this plot.\n\n\n\nmusicians %&gt;%\n  filter(!is.na(slopes)) %&gt;%\nggplot(aes(x = intercepts, y = slopes)) + \n  geom_point() + \n  geom_smooth(method = \"lm\") + \n  labs(x = \"Fitted intercepts\",\n       y = \"Fitted slopes\", \n       title = \"Fitted slopes and intercepts\", \n       subtitle = paste0(\"r = \", round(cor(musicians %&gt;% filter(!is.na(slopes)) %&gt;% pull(intercepts), musicians %&gt;% filter(!is.na(slopes)) %&gt;% pull(slopes)),3)))"
  },
  {
    "objectID": "slides/08_multilevel_mod_ch8_o.html#distribution-of-level-two-errors",
    "href": "slides/08_multilevel_mod_ch8_o.html#distribution-of-level-two-errors",
    "title": "Multilevel Models",
    "section": "Distribution of Level Two errors",
    "text": "Distribution of Level Two errors\nUse a multivariate normal distribution for the Level Two error terms\n\\[\\left[ \\begin{array}{c}\n            u_{i} \\\\ v_{i}\n          \\end{array}  \\right] \\sim N \\left( \\left[\n          \\begin{array}{c}\n            0 \\\\ 0\n          \\end{array} \\right], \\left[\n          \\begin{array}{cc}\n            \\sigma_{u}^{2} & \\rho_{uv}\\sigma_{u}\\sigma_v \\\\\n            \\rho_{uv}\\sigma_{u}\\sigma_v & \\sigma_{v}^{2}\n          \\end{array} \\right] \\right)\\]\nwhere \\(\\sigma^2_u\\) and \\(\\sigma^2_v\\) are the variance of \\(u_i\\)’s and \\(v_i\\)’s respectively, and \\(\\sigma_{uv}  = \\rho_{uv}\\sigma_u\\sigma_v\\) is covariance between \\(u_i\\) and \\(v_i\\)\n\nWhat does it mean for \\(\\rho_{uv} &gt; 0\\)?\nWhat does it mean for \\(\\rho_{uv} &lt; 0\\)?\nBeing able to write out these mammoth variance-covariance matrices is less important than recognizing the number of variance components that must be estimated by our intended model."
  },
  {
    "objectID": "slides/08_multilevel_mod_ch8_o.html#visualizing-multivariate-normal-distribution",
    "href": "slides/08_multilevel_mod_ch8_o.html#visualizing-multivariate-normal-distribution",
    "title": "Multilevel Models",
    "section": "Visualizing multivariate normal distribution",
    "text": "Visualizing multivariate normal distribution\n\n\n\n\n\nRecreated from Figure 8.12"
  },
  {
    "objectID": "slides/08_multilevel_mod_ch8_o.html#fit-the-model-12",
    "href": "slides/08_multilevel_mod_ch8_o.html#fit-the-model-12",
    "title": "Multilevel Models",
    "section": "Fit the model (1/2)",
    "text": "Fit the model (1/2)\nFit multilevel model using the lmer function from the lme4 package. Display results using hte tidy() function from the broom.mixed package.\n\nModel OutputCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\neffect\ngroup\nterm\nestimate\nstd.error\nstatistic\n\n\n\n\nfixed\nNA\n(Intercept)\n15.930\n0.641\n24.833\n\n\nfixed\nNA\norchestra\n1.693\n0.945\n1.791\n\n\nfixed\nNA\nlarge_ensemble\n-0.911\n0.845\n-1.077\n\n\nfixed\nNA\norchestra:large_ensemble\n-1.424\n1.099\n-1.295\n\n\nran_pars\nid\nsd__(Intercept)\n2.378\nNA\nNA\n\n\nran_pars\nid\ncor__(Intercept).large_ensemble\n-0.635\nNA\nNA\n\n\nran_pars\nid\nsd__large_ensemble\n0.672\nNA\nNA\n\n\nran_pars\nResidual\nsd__Observation\n4.670\nNA\nNA\n\n\n\n\n\n\n\n\nlibrary(lme4)\nlibrary(broom.mixed)\nmusic_model &lt;- lmer(na ~ orchestra + large_ensemble + \n                      orchestra:large_ensemble + (large_ensemble|id), \n                    REML = TRUE, data = music)\n\ntidy(music_model) %&gt;% kable(digits = 3)"
  },
  {
    "objectID": "slides/08_multilevel_mod_ch8_o.html#fit-the-model-in-r-22",
    "href": "slides/08_multilevel_mod_ch8_o.html#fit-the-model-in-r-22",
    "title": "Multilevel Models",
    "section": "Fit the model in R (2/2)",
    "text": "Fit the model in R (2/2)\nna ~ orchestra + large_ensemble + orchestra:large_ensemble: Represents the fixed effects + intercept\n. . .\n(large_ensemble|id): Represents the error terms and associated variance components - Specifies two error terms: \\(u_i\\) corresponding to the intercepts, \\(v_i\\) corresponding to effect of large ensemble"
  },
  {
    "objectID": "slides/08_multilevel_mod_ch8_o.html#tidy-output",
    "href": "slides/08_multilevel_mod_ch8_o.html#tidy-output",
    "title": "Multilevel Models",
    "section": "Tidy output",
    "text": "Tidy output\nDisplay results using the tidy function from the broom.mixed package.\n\nlibrary(broom.mixed)\ntidy(music_model) \n\n\nGet fixed effects only\n\n. . .\ntidy(music_model) %&gt;% filter(effect == “fixed”)\n\nGet errors and variance components only\n\n. . .\ntidy(music_model) %&gt;% filter(effect == “ran_pars”)"
  },
  {
    "objectID": "slides/08_multilevel_mod_ch8_o.html#model-output-fixed-effects",
    "href": "slides/08_multilevel_mod_ch8_o.html#model-output-fixed-effects",
    "title": "Multilevel Models",
    "section": "Model output: Fixed effects",
    "text": "Model output: Fixed effects\n\n\n\n\n\n\n\n\n\n\n\n\n\neffect\ngroup\nterm\nestimate\nstd.error\nstatistic\n\n\n\n\nfixed\nNA\n(Intercept)\n15.930\n0.641\n24.833\n\n\nfixed\nNA\norchestra\n1.693\n0.945\n1.791\n\n\nfixed\nNA\nlarge_ensemble\n-0.911\n0.845\n-1.077\n\n\nfixed\nNA\norchestra:large_ensemble\n-1.424\n1.099\n-1.295\n\n\n\n\n\n. . .\nLabel the fixed effects"
  },
  {
    "objectID": "slides/08_multilevel_mod_ch8_o.html#model-output-random-effects",
    "href": "slides/08_multilevel_mod_ch8_o.html#model-output-random-effects",
    "title": "Multilevel Models",
    "section": "Model output: Random effects",
    "text": "Model output: Random effects\n\n\n\n\n\n\n\n\n\n\n\n\n\neffect\ngroup\nterm\nestimate\nstd.error\nstatistic\n\n\n\n\nran_pars\nid\nsd__(Intercept)\n2.378\nNA\nNA\n\n\nran_pars\nid\ncor__(Intercept).large_ensemble\n-0.635\nNA\nNA\n\n\nran_pars\nid\nsd__large_ensemble\n0.672\nNA\nNA\n\n\nran_pars\nResidual\nsd__Observation\n4.670\nNA\nNA\n\n\n\n\n\n. . .\nLabel the error terms and variance components"
  },
  {
    "objectID": "slides/08_multilevel_mod_ch8_o.html#interpret-the-effects",
    "href": "slides/08_multilevel_mod_ch8_o.html#interpret-the-effects",
    "title": "Multilevel Models",
    "section": "Interpret the effects",
    "text": "Interpret the effects\n\nSplit into 3 groups.\nEach group will write the interpretation for one main effect and one variance component. The terms on each slide do not necessarily have a direct correspondence to each other.\nOne person write the group’s interpretations on the board."
  },
  {
    "objectID": "slides/08_multilevel_mod_ch8_o.html#comparing-fixed-effects-estimates",
    "href": "slides/08_multilevel_mod_ch8_o.html#comparing-fixed-effects-estimates",
    "title": "Multilevel Models",
    "section": "Comparing fixed effects estimates",
    "text": "Comparing fixed effects estimates\nBelow are the estimated coefficients and standard errors for four modeling approaches.\n\n\n\nfrom BMLR Table 8.3\n\n\nVariable\nIndependence\nTwo.Stage\nLVCF\nMultilevel\n\n\n\n\nIntercept\n15.72(0.36)\n16.28(0.67)\n15.20(1.25)\n15.93(0.64)\n\n\nOrch\n1.79(0.55)\n1.41(0.99)\n1.45(1.84)\n1.69(0.95)\n\n\nLarge\n-0.28(0.79)\n-0.77(0.85)\n-\n-0.91(0.85)\n\n\nOrch*Large\n-1.71(1.06)\n-1.41(1.20)\n-\n-1.42(1.10)"
  },
  {
    "objectID": "slides/08_multilevel_mod_ch8_o.html#questions-1",
    "href": "slides/08_multilevel_mod_ch8_o.html#questions-1",
    "title": "Multilevel Models",
    "section": "Questions",
    "text": "Questions\n\nHow do the coefficient estimates compare?\nHow do the standard errors compare?"
  },
  {
    "objectID": "slides/08_multilevel_mod_ch8_o.html#pseudo-r-squared-values",
    "href": "slides/08_multilevel_mod_ch8_o.html#pseudo-r-squared-values",
    "title": "Multilevel Models",
    "section": "Pseudo R-squared Values",
    "text": "Pseudo R-squared Values\n\\[\\textrm{Pseudo }R^2_{L1} = \\frac{\\hat{\\sigma}^{2}(\\textrm{Model A})-\\hat{\\sigma}^{2}(\\textrm{Model B})}{\\hat{\\sigma}^{2}(\\textrm{Model A})}\\] - Pseudo R-squared values are not universally reliable as measures of model performance. - Because of the complexity of estimating fixed effects and variance components at various levels of a multilevel model, it is not unusual to encounter situations in which covariates in a Level Two equation for, say, the intercept remain constant (while other aspects of the model change), yet the associated pseudo R-squared values differ or are negative. For this reason, pseudo R-squared values in multilevel models should be interpreted cautiously."
  },
  {
    "objectID": "slides/08_multilevel_mod_ch8_o.html#ml-and-reml-1",
    "href": "slides/08_multilevel_mod_ch8_o.html#ml-and-reml-1",
    "title": "Multilevel Models",
    "section": "ML and REML",
    "text": "ML and REML\nMaximum Likelihood (ML) and Restricted (Residual) Maximum Likelihood (REML) are the two most common methods for estimating the fixed effects and variance components\n. . .\nMaximum Likelihood (ML)\n\nJointly estimate the fixed effects and variance components using all the sample data\nCan be used to draw conclusions about fixed and random effects\nIssue: Fixed effects are treated as known values when estimating variance components\n\nResults in biased estimates of variance components (especially when sample size is small)"
  },
  {
    "objectID": "slides/08_multilevel_mod_ch8_o.html#ml-and-reml-2",
    "href": "slides/08_multilevel_mod_ch8_o.html#ml-and-reml-2",
    "title": "Multilevel Models",
    "section": "ML and REML",
    "text": "ML and REML\nRestricted Maximum Likelihood (REML)\n\nEstimate the variance components using the sample residuals not the sample data\nIt is conditional on the fixed effects, so it accounts for uncertainty in fixed effects estimates. This results in unbiased estimates of variance components.\n\n. . .\nExample using OLS\n\n\n[See the post Maximum Likelihood (ML) vs. REML for details and illustration of ML vs. REML."
  },
  {
    "objectID": "slides/08_multilevel_mod_ch8_o.html#ml-or-reml",
    "href": "slides/08_multilevel_mod_ch8_o.html#ml-or-reml",
    "title": "Multilevel Models",
    "section": "ML or REML?",
    "text": "ML or REML?\n\nResearch has not determined one method absolutely superior to the other\nREML (REML = TRUE; default in lmer) is preferable when\n\nthe number of parameters is large or primary, or\nprimary objective is to obtain estimates of the model parameters\n\nML (REML = FALSE) must be used if you want to compare nested fixed effects models using a likelihood ratio test (e.g., a drop-in-deviance test).\n\nFor REML, the goodness-of-fit and likelihood ratio tests can only be used to draw conclusions about variance components\n\n\n\n\n\nSource: Singer, J. D. & Willett, J. B. (2003). Applied longitudinal data analysis: Modeling change and event occurrence. Oxford university press."
  },
  {
    "objectID": "slides/08_multilevel_mod_ch8_o.html#comparing-ml-and-reml",
    "href": "slides/08_multilevel_mod_ch8_o.html#comparing-ml-and-reml",
    "title": "Multilevel Models",
    "section": "Comparing ML and REML",
    "text": "Comparing ML and REML\n\n\n\n\n\n\n\nMaximum Likelihood\n\n\nRestricted Maximum Likelihood\n\n\n\n\n\n\nTerm\n\n\nEstimate\n\n\nStd. Error\n\n\nEstimate\n\n\nStd. Error\n\n\n\n\n(Intercept)\n\n\n15.924\n\n\n0.623\n\n\n15.930\n\n\n0.641\n\n\n\n\norchestra1\n\n\n1.696\n\n\n0.919\n\n\n1.693\n\n\n0.945\n\n\n\n\nlarge_ensemble1\n\n\n-0.895\n\n\n0.827\n\n\n-0.911\n\n\n0.845\n\n\n\n\norchestra1:large_ensemble1\n\n\n-1.438\n\n\n1.074\n\n\n-1.424\n\n\n1.099\n\n\n\n\nsd__(Intercept)\n\n\n2.286\n\n\nNA\n\n\n2.378\n\n\nNA\n\n\n\n\ncor__(Intercept).large_ensemble1\n\n\n-1.00\n\n\nNA\n\n\n-0.635\n\n\nNA\n\n\n\n\nsd__large_ensemble1\n\n\n0.385\n\n\nNA\n\n\n0.672\n\n\nNA\n\n\n\n\nsd__Observation\n\n\n4.665\n\n\nNA\n\n\n4.670\n\n\nNA"
  },
  {
    "objectID": "slides/08_multilevel_mod_ch8_o.html#strategy-for-building-multilevel-models",
    "href": "slides/08_multilevel_mod_ch8_o.html#strategy-for-building-multilevel-models",
    "title": "Multilevel Models",
    "section": "Strategy for building multilevel models",
    "text": "Strategy for building multilevel models\n\nConduct exploratory data analysis for Level One and Level Two variables\nFit model with no covariates to assess variability at each level\nCreate Level One models. Start with a single term, then add terms as needed.\nCreate Level Two models. Start with a single term, then add terms as needed. Start with equation for intercept term.\nBegin with the full set of variance components, then remove variance terms as needed.\n\n. . .\nAlternate model building strategy in BMLR Section 8.6\n. . .\nSee BMLR Sections 8.6 - 8.11 for full step-by-step analysis."
  },
  {
    "objectID": "slides/08_multilevel_mod_ch8_o.html#acknowledgements",
    "href": "slides/08_multilevel_mod_ch8_o.html#acknowledgements",
    "title": "Multilevel Models",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThe content in the slides is from\n\nBMLR: Chapter 8 - Introduction to Multilevel Models\n\nInitial versions of the slides are by Dr. Maria Tackett, Duke University\n\nSinger, J. D. & Willett, J. B. (2003). Applied longitudinal data analysis: Modeling change and event occurrence. Oxford university press.\n\nSadler, Michael E., and Christopher J. Miller. 2010. “Performance Anxiety: A Longitudinal Study of the Roles of Personality and Experience in Musicians.” Social Psychological and Personality Science 1 (3): 280–87. http://dx.doi.org/10.1177/1948550610370492.\nInitial versions of the slides are by Dr. Maria Tackett, Duke University"
  },
  {
    "objectID": "slides/08_multilevel_mod_ch8.html#setup",
    "href": "slides/08_multilevel_mod_ch8.html#setup",
    "title": "Multilevel Models",
    "section": "Setup",
    "text": "Setup\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(GGally)\nlibrary(knitr)\nlibrary(patchwork)\nlibrary(viridis)\nlibrary(ggfortify)\nlibrary(kableExtra)\nlibrary(lme4)\nlibrary(broom.mixed)"
  },
  {
    "objectID": "slides/08_multilevel_mod_ch8.html#learning-goals-12",
    "href": "slides/08_multilevel_mod_ch8.html#learning-goals-12",
    "title": "Multilevel Models",
    "section": "Learning goals (1/2)",
    "text": "Learning goals (1/2)\n\nRecognize a potential for correlation in a data set\nIdentify observational units at varying levels\nUnderstand issues correlated data may cause in modeling\nUnderstand how random effects models can be used to take correlation into account\nUse EDA for multilevel data\nConduct univariate and bivariate EDA for multilevel models"
  },
  {
    "objectID": "slides/08_multilevel_mod_ch8.html#learning-goals-22",
    "href": "slides/08_multilevel_mod_ch8.html#learning-goals-22",
    "title": "Multilevel Models",
    "section": "Learning goals (2/2)",
    "text": "Learning goals (2/2)\n\nWrite multilevel model , including assumptions about variance components, in by-level and composite forms\nInterpret the model parameters, fixed effects, and variance components\nFit and interpret multilevel models\nCompare maximum likelihood (ML) and restricted maximum likelihood (REML) estimation approaches\nUnderstand general process for fitting and comparing multilevel models"
  },
  {
    "objectID": "slides/08_multilevel_mod_ch8.html#multilevel-data",
    "href": "slides/08_multilevel_mod_ch8.html#multilevel-data",
    "title": "Multilevel Models",
    "section": "Multilevel data",
    "text": "Multilevel data\n\nWe can think of correlated data as a multilevel structure\n\nPopulation elements are aggregated into groups\nThere are observational units and measurements at each level\n\n\n\n\nFor now we will focus on data with two levels:\n\nLevel one: Most basic level of observation\nLevel two: Groups formed from aggregated level-one observations"
  },
  {
    "objectID": "slides/08_multilevel_mod_ch8.html#two-types-of-effects",
    "href": "slides/08_multilevel_mod_ch8.html#two-types-of-effects",
    "title": "Multilevel Models",
    "section": "Two types of effects",
    "text": "Two types of effects\n\nFixed effects: Effects that are of interest in the study\n\nCan think of these as effects whose interpretations would be included in a write up of the study\n\n\n\n\nRandom effects: Effects we’re not interested in studying but whose variability we want to understand\n\nCan think of these as effects whose interpretations would not necessarily be included in a write up of the study"
  },
  {
    "objectID": "slides/08_multilevel_mod_ch8.html#practice-questions",
    "href": "slides/08_multilevel_mod_ch8.html#practice-questions",
    "title": "Multilevel Models",
    "section": "Practice Questions",
    "text": "Practice Questions\nRadon is a carcinogen – a naturally occurring radioactive gas whose decay products are also radioactive – known to cause lung cancer in high concentrations. The EPA sampled more than 80,000 homes across the U.S. Each house came from a randomly selected county and measurements were made on each level of each home. Uranium measurements at the county level were included to improve the radon estimates.\n\nWhat is the most basic level of observation (Level One)?\nWhat are the group units (Level Two, Level Three, etc…)\nWhat is the response variable?\nDescribe the within-group variation.\nWhat are the fixed effects? What are the random effects?\n\n\n\nEx. 1 from Section 7.10.1 in BMLR"
  },
  {
    "objectID": "slides/08_multilevel_mod_ch8.html#data-music-performance-anxiety",
    "href": "slides/08_multilevel_mod_ch8.html#data-music-performance-anxiety",
    "title": "Multilevel Models",
    "section": "Data: Music performance anxiety",
    "text": "Data: Music performance anxiety\nThe data musicdata.csv come from the Sadler and Miller (2010) study of the emotional state of musicians before performances. The dataset contains information collected from 37 undergraduate music majors who completed the Positive Affect Negative Affect Schedule (PANAS), an instrument produces a measure of anxiety (negative affect) and a measure of happiness (positive affect). This analysis will focus on negative affect as a measure of performance anxiety.\nThe primary variables we’ll use are\n\nna: negative affect score on PANAS (the response variable)\nperform_type: type of performance (Solo, Large Ensemble, Small Ensemble)\ninstrument: type of intstrument (Voice, Orchestral, Piano)"
  },
  {
    "objectID": "slides/08_multilevel_mod_ch8.html#look-at-data",
    "href": "slides/08_multilevel_mod_ch8.html#look-at-data",
    "title": "Multilevel Models",
    "section": "Look at data",
    "text": "Look at data\n\nData ViewCode\n\n\n\n\n\n\n\nid\ndiary\nperform_type\nna\ngender\ninstrument\n\n\n\n\n1\n1\nSolo\n11\nFemale\nvoice\n\n\n1\n2\nLarge Ensemble\n19\nFemale\nvoice\n\n\n1\n3\nLarge Ensemble\n14\nFemale\nvoice\n\n\n43\n1\nSolo\n19\nFemale\nvoice\n\n\n43\n2\nSolo\n13\nFemale\nvoice\n\n\n43\n3\nSmall Ensemble\n19\nFemale\nvoice\n\n\n\n\n\n\n\n\nmusic &lt;- read_csv(\"data/musicdata.csv\")\nmusic %&gt;%\n  filter(id %in% c(1, 43)) %&gt;%\n  group_by(id) %&gt;%\n  slice(1:3) %&gt;%\n  select(id, diary, perform_type, na, gender, instrument) %&gt;%\n  kable()\n\n\n\n\n\nWhat are the Level One observations? Level Two observations?\nWhat are the Level One variables? Level Two variables?"
  },
  {
    "objectID": "slides/08_multilevel_mod_ch8.html#univariate-exploratory-data-analysis",
    "href": "slides/08_multilevel_mod_ch8.html#univariate-exploratory-data-analysis",
    "title": "Multilevel Models",
    "section": "Univariate exploratory data analysis",
    "text": "Univariate exploratory data analysis\nLevel One variables\nTwo ways to approach univariate EDA (visualizations and summary statistics) for Level One variables:\n\nUse individual observations (i.e., treat observations as independent)\nUse aggregated values for each Level Two observation\n\n\nLevel Two variables\n\nUse a data set that contains one row per Level Two observation\n\n\n\nLet’s do some Univariate EDA together"
  },
  {
    "objectID": "slides/08_multilevel_mod_ch8.html#unviariate-eda",
    "href": "slides/08_multilevel_mod_ch8.html#unviariate-eda",
    "title": "Multilevel Models",
    "section": "Unviariate EDA",
    "text": "Unviariate EDA\n\nPlotsCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\np1 &lt;- ggplot(data = music, aes(x = na)) + \n  geom_histogram(fill = \"steelblue\", color = \"black\", binwidth = 2) + \n  labs(x = \"Individual negative affect\", \n       title = \"Negative affect scores\")\n\np2 &lt;- music %&gt;%\n  group_by(id) %&gt;%\n  summarise(mean_na = mean(na)) %&gt;%\n  ggplot(aes(x = mean_na)) + \n  geom_histogram(fill = \"steelblue\", color = \"black\", binwidth = 2) + \n  labs(x = \"Mean negative affect\", \n       title = \"Mean negative affect scores\")\n\np1 + p2"
  },
  {
    "objectID": "slides/08_multilevel_mod_ch8.html#bivariate-exploratory-data-analysis",
    "href": "slides/08_multilevel_mod_ch8.html#bivariate-exploratory-data-analysis",
    "title": "Multilevel Models",
    "section": "Bivariate exploratory data analysis",
    "text": "Bivariate exploratory data analysis\nGoals\n\nExplore general association between the predictor and response variable\nExplore whether subjects at a given level of the predictor tend to have similar mean responses\nExplore whether variation in response differs at different levels of a predictor\n\n\nThere are two ways to visualize these associations:\n\nOne plot of individual observations (i.e., treat observations as independent)\nSeparate plots of responses vs. predictor for each Level Two observation (lattice plots)\n\n\n\nNow lets do some Bivariate EDA."
  },
  {
    "objectID": "slides/08_multilevel_mod_ch8.html#questions-we-want-to-answer",
    "href": "slides/08_multilevel_mod_ch8.html#questions-we-want-to-answer",
    "title": "Multilevel Models",
    "section": "Questions we want to answer",
    "text": "Questions we want to answer\nThe goal is to understand variability in performance anxiety (na) based on performance-level and musician-level characteristics. Specifically:\n\nWhat is the association between performance type (large ensemble or not) and performance anxiety? Does the association differ based on instrument type (orchestral or not)?\n\n\nWhat is the problem with using the following model to draw conclusions?\n\nModel OutputCode\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n15.721\n0.359\n43.778\n0.000\n\n\norchestra\n1.789\n0.552\n3.243\n0.001\n\n\nlarge_ensemble\n-0.277\n0.791\n-0.350\n0.727\n\n\norchestra:large_ensemble\n-1.709\n1.062\n-1.609\n0.108\n\n\n\n\n\n\n\n\nmusic &lt;- music %&gt;%\n  mutate(orchestra = if_else(instrument == \"orchestral instrument\", 1, 0), \n         large_ensemble = if_else(perform_type == \"Large Ensemble\", 1,0))\n\nols &lt;- lm(na ~ orchestra + large_ensemble + orchestra * large_ensemble, \n          data = music)\ntidy(ols) %&gt;% kable(digits = 3)"
  },
  {
    "objectID": "slides/08_multilevel_mod_ch8.html#other-modeling-approaches",
    "href": "slides/08_multilevel_mod_ch8.html#other-modeling-approaches",
    "title": "Multilevel Models",
    "section": "Other modeling approaches",
    "text": "Other modeling approaches\n1️⃣ Condense each musician’s set of responses into a single outcome (e.g., mean max, last observation, etc.) and fit a linear model on these condensed observations\n\nLeaves few observations (37) to fit the model\nIgnoring a lot of information in the multiple observations for each musician\n\n\n2️⃣ Fit a separate model for each musician understand the association between performance type (Level One models). Then fit a system of Level Two models to predict the fitted coefficients in the Level One model for each subject based on instrument type (Level Two model).\n\n\nLet’s look at approach #2"
  },
  {
    "objectID": "slides/08_multilevel_mod_ch8.html#two-level-model",
    "href": "slides/08_multilevel_mod_ch8.html#two-level-model",
    "title": "Multilevel Models",
    "section": "Two-level model",
    "text": "Two-level model\nWe’ll start with the Level One model to understand the association between performance type and performance anxiety for the \\(i^{th}\\) musician.\n\\[na_{ij} = a_i + b_i ~ LargeEnsemble_{ij} + \\epsilon_i, \\hspace{5mm} \\epsilon_{ij} \\sim N(0,\\sigma^2)\\]\n\nWhy is it more meaningful to use performance type for the Level One model than instrument?\n\n\nFor now, estimate \\(a_i\\) and \\(b_i\\) using least-squares regression."
  },
  {
    "objectID": "slides/08_multilevel_mod_ch8.html#example-level-one-model",
    "href": "slides/08_multilevel_mod_ch8.html#example-level-one-model",
    "title": "Multilevel Models",
    "section": "Example Level One model",
    "text": "Example Level One model\nBelow is partial data for observation #22\n\nmusic %&gt;%\n  filter(id == 22) %&gt;%\n  select(id, diary, perform_type, instrument, na) %&gt;%\n  slice(1:3, 13:15) %&gt;%\n  kable()\n\n\n\n\nid\ndiary\nperform_type\ninstrument\nna\n\n\n\n\n22\n1\nSolo\norchestral instrument\n24\n\n\n22\n2\nLarge Ensemble\norchestral instrument\n21\n\n\n22\n3\nLarge Ensemble\norchestral instrument\n14\n\n\n22\n13\nLarge Ensemble\norchestral instrument\n12\n\n\n22\n14\nLarge Ensemble\norchestral instrument\n19\n\n\n22\n15\nSolo\norchestral instrument\n25"
  },
  {
    "objectID": "slides/08_multilevel_mod_ch8.html#level-one-model",
    "href": "slides/08_multilevel_mod_ch8.html#level-one-model",
    "title": "Multilevel Models",
    "section": "Level One model",
    "text": "Level One model\n\nModelCodeSlopes and Intercepts\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n24.500\n1.96\n12.503\n0.000\n\n\nlarge_ensemble\n-7.833\n2.53\n-3.097\n0.009\n\n\n\n\n\n\n\n\nmusic %&gt;%\n  filter(id == 22) %&gt;%\n  lm(na ~ large_ensemble, data = .) %&gt;%\n  tidy() %&gt;%\n  kable(digits = 3)\n\n. . .\nRepeat for all 37 musicians\n\n\n\n\n\n\n\nRecreated from BMLR Figure 8.9\n\n\n\n\n. . .\nNow let’s consider if there is an association between the estimated slopes, estimated intercepts, and the type of instrument"
  },
  {
    "objectID": "slides/08_multilevel_mod_ch8.html#level-two-model",
    "href": "slides/08_multilevel_mod_ch8.html#level-two-model",
    "title": "Multilevel Models",
    "section": "Level Two Model",
    "text": "Level Two Model\nThe slope and intercept for the \\(i^{th}\\) musician can be modeled as\n\\[\\begin{aligned}&a_i = \\alpha_0 + \\alpha_1 ~ Orchestra_i + u_i \\\\\n&b_i = \\beta_0 + \\beta_1 ~ Orchestra_i + v_i\\end{aligned}\\]\n\nNote the response variable in the Level Two models are not observed outcomes but the (fitted) slope and intercept from each musician\n\n\nLet’s fit a level 2 model"
  },
  {
    "objectID": "slides/08_multilevel_mod_ch8.html#estimated-coefficients-by-instrument",
    "href": "slides/08_multilevel_mod_ch8.html#estimated-coefficients-by-instrument",
    "title": "Multilevel Models",
    "section": "Estimated coefficients by instrument",
    "text": "Estimated coefficients by instrument\n\nOutputCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmusicians &lt;- music %&gt;%\n  distinct(id, orchestra) %&gt;%\n  bind_cols(model_stats)\n\np1 &lt;- ggplot(data = musicians, aes(x = intercepts, y = factor(orchestra))) + \n  geom_boxplot(fill = \"steelblue\", color = \"black\") + \n  labs(x = \"Fitted intercepts\", \n       y = \"Orchestra\")\n\np2 &lt;- ggplot(data = musicians, aes(x = slopes, y = factor(orchestra))) + \n  geom_boxplot(fill = \"steelblue\", color = \"black\") + \n  labs(x = \"Fitted slopes\", \n       y = \"Orchestra\")\n\np1 / p2"
  },
  {
    "objectID": "slides/08_multilevel_mod_ch8.html#level-two-model-1",
    "href": "slides/08_multilevel_mod_ch8.html#level-two-model-1",
    "title": "Multilevel Models",
    "section": "Level Two model",
    "text": "Level Two model\n\nModelsCode\n\n\nModel for intercepts\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n16.283\n0.671\n24.249\n0.000\n\n\norchestra\n1.411\n0.991\n1.424\n0.163\n\n\n\n\n\nModel for slopes\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-0.771\n0.851\n-0.906\n0.373\n\n\norchestra\n-1.406\n1.203\n-1.168\n0.253\n\n\n\n\n\n\n\nModel for intercepts\n\na &lt;- lm(intercepts ~ orchestra, data = musicians) \ntidy(a) %&gt;%\n  kable(digits = 3)\n\nModel for slopes\n\nb &lt;- lm(slopes ~ orchestra, data = musicians) \ntidy(b) %&gt;%\n  kable(digits = 3)"
  },
  {
    "objectID": "slides/08_multilevel_mod_ch8.html#writing-out-the-models",
    "href": "slides/08_multilevel_mod_ch8.html#writing-out-the-models",
    "title": "Multilevel Models",
    "section": "Writing out the models",
    "text": "Writing out the models\nLevel One\n\\[\\hat{na}_{ij}  = \\hat{a}_i + \\hat{b}_i ~ LargeEnsemble_{ij}\\]\nfor each musician.\nLevel Two\n\\[\\begin{aligned}&\\hat{a}_i = 16.283 + 1.441 ~ Orchestra_i \\\\\n&\\hat{b}_i = -0.771 - 1.406 ~ Orchestra_i\\end{aligned}\\]"
  },
  {
    "objectID": "slides/08_multilevel_mod_ch8.html#composite-model",
    "href": "slides/08_multilevel_mod_ch8.html#composite-model",
    "title": "Multilevel Models",
    "section": "Composite model",
    "text": "Composite model\n\\[\\begin{aligned}\\hat{na}_i &= 16.283 + 1.441 ~ Orchestra_i - 0.771 ~ LargeEnsemble_{ij} \\\\\n&- 1.406 ~ Orchestra:LargeEnsemble_{ij}\\end{aligned}\\]\n\n(Note that we also have the error terms \\(\\epsilon_{ij}, u_i, v_i\\) that we will discuss soon)\n\nWhat is the predicted average performance anxiety before solos and small ensemble performances for vocalists and keyboardists? For those who place orchestral instruments?\nWhat is the predicted average performance anxiety before large ensemble performances for those who play orchestral instruments?"
  },
  {
    "objectID": "slides/08_multilevel_mod_ch8.html#disadvantages-to-this-approach",
    "href": "slides/08_multilevel_mod_ch8.html#disadvantages-to-this-approach",
    "title": "Multilevel Models",
    "section": "Disadvantages to this approach",
    "text": "Disadvantages to this approach\n⚠️ Weighs each musician the same regardless of number of diary entries\n⚠️ Drops subjects who have missing values for slope (7 individuals who didn’t play a large ensemble performance)\n⚠️ Does not share strength effectively across individuals\n\nWe will use a unified approach that utilizes likelihood-based methods to address some of these drawbacks."
  },
  {
    "objectID": "slides/08_multilevel_mod_ch8.html#questions",
    "href": "slides/08_multilevel_mod_ch8.html#questions",
    "title": "Multilevel Models",
    "section": "Questions",
    "text": "Questions\n\nHow many Level One models are fit?\nHow many Level Two models are fit?"
  },
  {
    "objectID": "slides/08_multilevel_mod_ch8.html#framework",
    "href": "slides/08_multilevel_mod_ch8.html#framework",
    "title": "Multilevel Models",
    "section": "Framework",
    "text": "Framework\nLet \\(Y_{ij}\\) be the performance anxiety for the \\(i^{th}\\) musician before performance \\(j\\).\nLevel One\n\\[Y_{ij} = a_i + b_i ~ LargeEnsemble + \\epsilon_{ij}\\]\n\nLevel Two\n\\[\\begin{aligned}&a_i = \\alpha_0 + \\alpha_1 ~ Orchestra_i+ u_i\\\\\n&b_i = \\beta_0 + \\beta_1~Orchestra_i + v_i\\end{aligned}\\]\n\n\nThis approach uses likelihood-based methods (instead of least squares) to address the previously mentioned disadvantages"
  },
  {
    "objectID": "slides/08_multilevel_mod_ch8.html#creating-a-composite-model",
    "href": "slides/08_multilevel_mod_ch8.html#creating-a-composite-model",
    "title": "Multilevel Models",
    "section": "Creating a Composite Model",
    "text": "Creating a Composite Model\nPlug in the equations for \\(a_i\\) and \\(b_i\\) to get the composite model\n\n\n\\[Y_{ij} = a_i + b_i ~ LargeEnsemble + \\epsilon_{ij}\\]\n\n\\[\\begin{aligned}&a_i = \\alpha_0 + \\alpha_1 ~ Orchestra_i+ u_i\\\\\n&b_i = \\beta_0 + \\beta_1~Orchestra_i + v_i\\end{aligned}\\]\n\n\n\\[\\begin{aligned}Y_{ij} &= a_i \\hspace{1.8in}+ \\hspace{1.62in} b_i ~ LargeEnsemble  \\hspace{2.2in}+ \\epsilon_{ij}\\\\\n&=\n(\\alpha_0 + \\alpha_1 ~ Orchestra_i+ u_i) + (\\beta_0 + \\beta_1~Orchestra_i + v_i)LargeEnsemble\\hspace{2.17in}+\\epsilon_{ij}\\\\\n&= \\space \\alpha_0 + \\alpha_1 ~ Orchestra_i+ u_i \\hspace{.2in} + \\beta_0LargeEnsemble+\\beta_1Orchestra_iLargeEnsemble+v_iLargeEnsemble+\\epsilon_{ij}\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/08_multilevel_mod_ch8.html#lmer-syntax",
    "href": "slides/08_multilevel_mod_ch8.html#lmer-syntax",
    "title": "Multilevel Models",
    "section": "lmer Syntax",
    "text": "lmer Syntax\nlmer(response~fixed effect variables + (random effect variables|group variable),data = your_data)\n\nwe determine fixed and random effects based on our composite model\nterms with Greek letters will be fixed effects\nterms with random components, usually \\(u\\), \\(v\\), or \\(w\\) will be random effects"
  },
  {
    "objectID": "slides/08_multilevel_mod_ch8.html#fitting-our-model-with-lmer",
    "href": "slides/08_multilevel_mod_ch8.html#fitting-our-model-with-lmer",
    "title": "Multilevel Models",
    "section": "Fitting our Model with lmer",
    "text": "Fitting our Model with lmer\n\\[\\begin{aligned} Y_{ij}&= \\space \\alpha_0 + \\alpha_1 ~ Orchestra_i+ u_i +\\space\\space\\space \\beta_0LargeEnsemble+\\beta_1Orchestra_iLargeEnsemble+v_iLargeEnsemble+\\epsilon_{ij}\\\\\n&=\\alpha_0 + \\alpha_1 ~ Orchestra_i + \\beta_0LargeEnsemble+\\beta_1Orchestra_iLargeEnsemble+u_i + v_iLargeEnsemble+\\epsilon_{ij}\n\\end{aligned}\\]\n\nAll varibles with greek letters are fixed effects and all variables with \\(u\\), \\(v\\) are random effects.\n\n\nlmer(na ~ 1 + Orchastra + LargeEnsemble + Orchestra * LargeEnsemble + (1+LargeEnsemble|id),data = music)"
  },
  {
    "objectID": "slides/08_multilevel_mod_ch8.html#composite-model-1",
    "href": "slides/08_multilevel_mod_ch8.html#composite-model-1",
    "title": "Multilevel Models",
    "section": "Composite model",
    "text": "Composite model\nPlug in the equations for \\(a_i\\) and \\(b_i\\) to get the composite model\n\\[\\begin{aligned}Y_{ij} &= (\\alpha_0 + \\alpha_1 ~ Orchestra_i + \\beta_0 ~ LargeEnsemble_{ij} \\\\\n&+ \\beta_1 ~ Orchestra_i:LargeEnsemble_{ij})\\\\\n&+ (u_i + v_i ~ LargeEnsemble_{ij} + \\epsilon_{ij})\\end{aligned}\\]\n\nThe fixed effects to estimate are \\(\\alpha_0, \\alpha_1, \\beta_0, \\beta_1\\)\nThe error terms are \\(u_i, v_i, \\epsilon_{ij}\\)\n\n\nNote that we no longer need to estimate \\(a_i\\) and \\(b_i\\) directly as we did earlier. They conceptually connect the Level One and Level Two models."
  },
  {
    "objectID": "slides/08_multilevel_mod_ch8.html#notation",
    "href": "slides/08_multilevel_mod_ch8.html#notation",
    "title": "Multilevel Models",
    "section": "Notation",
    "text": "Notation\n\nGreek letters denote the fixed effect model parameters to be estimated\n\ne.g., \\(\\alpha_0, \\alpha_1, \\beta_0, \\beta_1\\)\n\nRoman letters denote the preliminary fixed effects at lower levels that will not be estimated directly.\n\ne.g. \\(a_i, b_i\\)\n\n\\(\\sigma\\) and \\(\\rho\\) denote variance components that will be estimated\n\\(\\epsilon_{ij}, u_i, v_i\\) denote error terms"
  },
  {
    "objectID": "slides/08_multilevel_mod_ch8.html#error-terms",
    "href": "slides/08_multilevel_mod_ch8.html#error-terms",
    "title": "Multilevel Models",
    "section": "Error terms",
    "text": "Error terms\n\nWe generally assume that the error terms are normally distributed, e.g. error associated with each performance of a given musician is \\(\\epsilon_{ij} \\sim N(0, \\sigma^2)\\)\nFor the Level Two models, the errors are\n\n\\(u_i\\): deviation of musician \\(i\\) from the mean performance anxiety before solos and small ensembles after accounting for the instrument\n\\(v_i\\): deviance of musician \\(i\\) from the mean difference in performance anxiety between large ensembles and other performance types after accounting for instrument\n\nNeed to account for fact that \\(u_i\\) and \\(v_i\\) are correlated for the \\(i^{th}\\) musician"
  },
  {
    "objectID": "slides/08_multilevel_mod_ch8.html#intercept-and-slope",
    "href": "slides/08_multilevel_mod_ch8.html#intercept-and-slope",
    "title": "Multilevel Models",
    "section": "Intercept and Slope",
    "text": "Intercept and Slope\n\nPlotCode\n\n\n\n\n\n\n\nRecreated from Figure 8.11\n\n\n\n\nQuestion: Describe what we learn about the association between the slopes and intercepts based on this plot.\n\n\n\nmusicians %&gt;%\n  filter(!is.na(slopes)) %&gt;%\nggplot(aes(x = intercepts, y = slopes)) + \n  geom_point() + \n  geom_smooth(method = \"lm\") + \n  labs(x = \"Fitted intercepts\",\n       y = \"Fitted slopes\", \n       title = \"Fitted slopes and intercepts\", \n       subtitle = paste0(\"r = \", round(cor(musicians %&gt;% filter(!is.na(slopes)) %&gt;% pull(intercepts), musicians %&gt;% filter(!is.na(slopes)) %&gt;% pull(slopes)),3)))"
  },
  {
    "objectID": "slides/08_multilevel_mod_ch8.html#distribution-of-level-two-errors",
    "href": "slides/08_multilevel_mod_ch8.html#distribution-of-level-two-errors",
    "title": "Multilevel Models",
    "section": "Distribution of Level Two errors",
    "text": "Distribution of Level Two errors\nUse a multivariate normal distribution for the Level Two error terms\n\\[\\left[ \\begin{array}{c}\n            u_{i} \\\\ v_{i}\n          \\end{array}  \\right] \\sim N \\left( \\left[\n          \\begin{array}{c}\n            0 \\\\ 0\n          \\end{array} \\right], \\left[\n          \\begin{array}{cc}\n            \\sigma_{u}^{2} & \\rho_{uv}\\sigma_{u}\\sigma_v \\\\\n            \\rho_{uv}\\sigma_{u}\\sigma_v & \\sigma_{v}^{2}\n          \\end{array} \\right] \\right)\\]\nwhere \\(\\sigma^2_u\\) and \\(\\sigma^2_v\\) are the variance of \\(u_i\\)’s and \\(v_i\\)’s respectively, and \\(\\sigma_{uv}  = \\rho_{uv}\\sigma_u\\sigma_v\\) is covariance between \\(u_i\\) and \\(v_i\\)\n\nWhat does it mean for \\(\\rho_{uv} &gt; 0\\)?\nWhat does it mean for \\(\\rho_{uv} &lt; 0\\)?\nBeing able to write out these mammoth variance-covariance matrices is less important than recognizing the number of variance components that must be estimated by our intended model."
  },
  {
    "objectID": "slides/08_multilevel_mod_ch8.html#visualizing-multivariate-normal-distribution",
    "href": "slides/08_multilevel_mod_ch8.html#visualizing-multivariate-normal-distribution",
    "title": "Multilevel Models",
    "section": "Visualizing multivariate normal distribution",
    "text": "Visualizing multivariate normal distribution\n\nRecreated from Figure 8.12"
  },
  {
    "objectID": "slides/08_multilevel_mod_ch8.html#fit-the-model-12",
    "href": "slides/08_multilevel_mod_ch8.html#fit-the-model-12",
    "title": "Multilevel Models",
    "section": "Fit the model (1/2)",
    "text": "Fit the model (1/2)\nFit multilevel model using the lmer function from the lme4 package. Display results using hte tidy() function from the broom.mixed package.\n\nModel OutputCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\neffect\ngroup\nterm\nestimate\nstd.error\nstatistic\n\n\n\n\nfixed\nNA\n(Intercept)\n15.930\n0.641\n24.833\n\n\nfixed\nNA\norchestra\n1.693\n0.945\n1.791\n\n\nfixed\nNA\nlarge_ensemble\n-0.911\n0.845\n-1.077\n\n\nfixed\nNA\norchestra:large_ensemble\n-1.424\n1.099\n-1.295\n\n\nran_pars\nid\nsd__(Intercept)\n2.378\nNA\nNA\n\n\nran_pars\nid\ncor__(Intercept).large_ensemble\n-0.635\nNA\nNA\n\n\nran_pars\nid\nsd__large_ensemble\n0.672\nNA\nNA\n\n\nran_pars\nResidual\nsd__Observation\n4.670\nNA\nNA\n\n\n\n\n\n\n\n\nlibrary(lme4)\nlibrary(broom.mixed)\nmusic_model &lt;- lmer(na ~ orchestra + large_ensemble + \n                      orchestra:large_ensemble + (large_ensemble|id), \n                    REML = TRUE, data = music)\n\ntidy(music_model) %&gt;% kable(digits = 3)"
  },
  {
    "objectID": "slides/08_multilevel_mod_ch8.html#fit-the-model-in-r-22",
    "href": "slides/08_multilevel_mod_ch8.html#fit-the-model-in-r-22",
    "title": "Multilevel Models",
    "section": "Fit the model in R (2/2)",
    "text": "Fit the model in R (2/2)\nna ~ orchestra + large_ensemble + orchestra:large_ensemble: Represents the fixed effects + intercept\n\n(large_ensemble|id): Represents the error terms and associated variance components - Specifies two error terms: \\(u_i\\) corresponding to the intercepts, \\(v_i\\) corresponding to effect of large ensemble"
  },
  {
    "objectID": "slides/08_multilevel_mod_ch8.html#tidy-output",
    "href": "slides/08_multilevel_mod_ch8.html#tidy-output",
    "title": "Multilevel Models",
    "section": "Tidy output",
    "text": "Tidy output\nDisplay results using the tidy function from the broom.mixed package.\n\nlibrary(broom.mixed)\ntidy(music_model) \n\n\nGet fixed effects only\n\n\ntidy(music_model) %&gt;% filter(effect == “fixed”)\n\nGet errors and variance components only\n\n\n\ntidy(music_model) %&gt;% filter(effect == “ran_pars”)"
  },
  {
    "objectID": "slides/08_multilevel_mod_ch8.html#model-output-fixed-effects",
    "href": "slides/08_multilevel_mod_ch8.html#model-output-fixed-effects",
    "title": "Multilevel Models",
    "section": "Model output: Fixed effects",
    "text": "Model output: Fixed effects\n\n\n\n\n\n\n\n\n\n\n\n\n\neffect\ngroup\nterm\nestimate\nstd.error\nstatistic\n\n\n\n\nfixed\nNA\n(Intercept)\n15.930\n0.641\n24.833\n\n\nfixed\nNA\norchestra\n1.693\n0.945\n1.791\n\n\nfixed\nNA\nlarge_ensemble\n-0.911\n0.845\n-1.077\n\n\nfixed\nNA\norchestra:large_ensemble\n-1.424\n1.099\n-1.295\n\n\n\n\n\n\nLabel the fixed effects"
  },
  {
    "objectID": "slides/08_multilevel_mod_ch8.html#model-output-random-effects",
    "href": "slides/08_multilevel_mod_ch8.html#model-output-random-effects",
    "title": "Multilevel Models",
    "section": "Model output: Random effects",
    "text": "Model output: Random effects\n\n\n\n\n\n\n\n\n\n\n\n\n\neffect\ngroup\nterm\nestimate\nstd.error\nstatistic\n\n\n\n\nran_pars\nid\nsd__(Intercept)\n2.378\nNA\nNA\n\n\nran_pars\nid\ncor__(Intercept).large_ensemble\n-0.635\nNA\nNA\n\n\nran_pars\nid\nsd__large_ensemble\n0.672\nNA\nNA\n\n\nran_pars\nResidual\nsd__Observation\n4.670\nNA\nNA\n\n\n\n\n\n\nLabel the error terms and variance components"
  },
  {
    "objectID": "slides/08_multilevel_mod_ch8.html#interpret-the-effects",
    "href": "slides/08_multilevel_mod_ch8.html#interpret-the-effects",
    "title": "Multilevel Models",
    "section": "Interpret the effects",
    "text": "Interpret the effects\n\nSplit into 3 groups.\nEach group will write the interpretation for one main effect and one variance component. The terms on each slide do not necessarily have a direct correspondence to each other.\nOne person write the group’s interpretations on the board."
  },
  {
    "objectID": "slides/08_multilevel_mod_ch8.html#comparing-fixed-effects-estimates",
    "href": "slides/08_multilevel_mod_ch8.html#comparing-fixed-effects-estimates",
    "title": "Multilevel Models",
    "section": "Comparing fixed effects estimates",
    "text": "Comparing fixed effects estimates\nBelow are the estimated coefficients and standard errors for four modeling approaches.\n\n\n\nfrom BMLR Table 8.3\n\n\nVariable\nIndependence\nTwo.Stage\nLVCF\nMultilevel\n\n\n\n\nIntercept\n15.72(0.36)\n16.28(0.67)\n15.20(1.25)\n15.93(0.64)\n\n\nOrch\n1.79(0.55)\n1.41(0.99)\n1.45(1.84)\n1.69(0.95)\n\n\nLarge\n-0.28(0.79)\n-0.77(0.85)\n-\n-0.91(0.85)\n\n\nOrch*Large\n-1.71(1.06)\n-1.41(1.20)\n-\n-1.42(1.10)"
  },
  {
    "objectID": "slides/08_multilevel_mod_ch8.html#questions-1",
    "href": "slides/08_multilevel_mod_ch8.html#questions-1",
    "title": "Multilevel Models",
    "section": "Questions",
    "text": "Questions\n\nHow do the coefficient estimates compare?\nHow do the standard errors compare?"
  },
  {
    "objectID": "slides/08_multilevel_mod_ch8.html#pseudo-r-squared-values",
    "href": "slides/08_multilevel_mod_ch8.html#pseudo-r-squared-values",
    "title": "Multilevel Models",
    "section": "Pseudo R-squared Values",
    "text": "Pseudo R-squared Values\n\\[\\textrm{Pseudo }R^2_{L1} = \\frac{\\hat{\\sigma}^{2}(\\textrm{Model A})-\\hat{\\sigma}^{2}(\\textrm{Model B})}{\\hat{\\sigma}^{2}(\\textrm{Model A})}\\] - Pseudo R-squared values are not universally reliable as measures of model performance. - Because of the complexity of estimating fixed effects and variance components at various levels of a multilevel model, it is not unusual to encounter situations in which covariates in a Level Two equation for, say, the intercept remain constant (while other aspects of the model change), yet the associated pseudo R-squared values differ or are negative. For this reason, pseudo R-squared values in multilevel models should be interpreted cautiously."
  },
  {
    "objectID": "slides/08_multilevel_mod_ch8.html#ml-and-reml-1",
    "href": "slides/08_multilevel_mod_ch8.html#ml-and-reml-1",
    "title": "Multilevel Models",
    "section": "ML and REML",
    "text": "ML and REML\nMaximum Likelihood (ML) and Restricted (Residual) Maximum Likelihood (REML) are the two most common methods for estimating the fixed effects and variance components\n\nMaximum Likelihood (ML)\n\nJointly estimate the fixed effects and variance components using all the sample data\nCan be used to draw conclusions about fixed and random effects\nIssue: Fixed effects are treated as known values when estimating variance components\n\nResults in biased estimates of variance components (especially when sample size is small)"
  },
  {
    "objectID": "slides/08_multilevel_mod_ch8.html#ml-and-reml-2",
    "href": "slides/08_multilevel_mod_ch8.html#ml-and-reml-2",
    "title": "Multilevel Models",
    "section": "ML and REML",
    "text": "ML and REML\nRestricted Maximum Likelihood (REML)\n\nEstimate the variance components using the sample residuals not the sample data\nIt is conditional on the fixed effects, so it accounts for uncertainty in fixed effects estimates. This results in unbiased estimates of variance components.\n\n\nExample using OLS\n\n\n\n[See the post Maximum Likelihood (ML) vs. REML for details and illustration of ML vs. REML."
  },
  {
    "objectID": "slides/08_multilevel_mod_ch8.html#ml-or-reml",
    "href": "slides/08_multilevel_mod_ch8.html#ml-or-reml",
    "title": "Multilevel Models",
    "section": "ML or REML?",
    "text": "ML or REML?\n\nResearch has not determined one method absolutely superior to the other\nREML (REML = TRUE; default in lmer) is preferable when\n\nthe number of parameters is large or primary, or\nprimary objective is to obtain estimates of the model parameters\n\nML (REML = FALSE) must be used if you want to compare nested fixed effects models using a likelihood ratio test (e.g., a drop-in-deviance test).\n\nFor REML, the goodness-of-fit and likelihood ratio tests can only be used to draw conclusions about variance components\n\n\n\n\n\nSource: Singer, J. D. & Willett, J. B. (2003). Applied longitudinal data analysis: Modeling change and event occurrence. Oxford university press."
  },
  {
    "objectID": "slides/08_multilevel_mod_ch8.html#comparing-ml-and-reml",
    "href": "slides/08_multilevel_mod_ch8.html#comparing-ml-and-reml",
    "title": "Multilevel Models",
    "section": "Comparing ML and REML",
    "text": "Comparing ML and REML\n\n\n\n\n\n\n\nMaximum Likelihood\n\n\nRestricted Maximum Likelihood\n\n\n\n\n\n\nTerm\n\n\nEstimate\n\n\nStd. Error\n\n\nEstimate\n\n\nStd. Error\n\n\n\n\n(Intercept)\n\n\n15.924\n\n\n0.623\n\n\n15.930\n\n\n0.641\n\n\n\n\norchestra1\n\n\n1.696\n\n\n0.919\n\n\n1.693\n\n\n0.945\n\n\n\n\nlarge_ensemble1\n\n\n-0.895\n\n\n0.827\n\n\n-0.911\n\n\n0.845\n\n\n\n\norchestra1:large_ensemble1\n\n\n-1.438\n\n\n1.074\n\n\n-1.424\n\n\n1.099\n\n\n\n\nsd__(Intercept)\n\n\n2.286\n\n\nNA\n\n\n2.378\n\n\nNA\n\n\n\n\ncor__(Intercept).large_ensemble1\n\n\n-1.00\n\n\nNA\n\n\n-0.635\n\n\nNA\n\n\n\n\nsd__large_ensemble1\n\n\n0.385\n\n\nNA\n\n\n0.672\n\n\nNA\n\n\n\n\nsd__Observation\n\n\n4.665\n\n\nNA\n\n\n4.670\n\n\nNA"
  },
  {
    "objectID": "slides/08_multilevel_mod_ch8.html#strategy-for-building-multilevel-models",
    "href": "slides/08_multilevel_mod_ch8.html#strategy-for-building-multilevel-models",
    "title": "Multilevel Models",
    "section": "Strategy for building multilevel models",
    "text": "Strategy for building multilevel models\n\nConduct exploratory data analysis for Level One and Level Two variables\nFit model with no covariates to assess variability at each level\nCreate Level One models. Start with a single term, then add terms as needed.\nCreate Level Two models. Start with a single term, then add terms as needed. Start with equation for intercept term.\nBegin with the full set of variance components, then remove variance terms as needed.\n\n\nAlternate model building strategy in BMLR Section 8.6\n\n\nSee BMLR Sections 8.6 - 8.11 for full step-by-step analysis."
  },
  {
    "objectID": "slides/08_multilevel_mod_ch8.html#acknowledgements",
    "href": "slides/08_multilevel_mod_ch8.html#acknowledgements",
    "title": "Multilevel Models",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThe content in the slides is from\n\nBMLR: Chapter 8 - Introduction to Multilevel Models\n\nInitial versions of the slides are by Dr. Maria Tackett, Duke University\n\nSinger, J. D. & Willett, J. B. (2003). Applied longitudinal data analysis: Modeling change and event occurrence. Oxford university press.\n\nSadler, Michael E., and Christopher J. Miller. 2010. “Performance Anxiety: A Longitudinal Study of the Roles of Personality and Experience in Musicians.” Social Psychological and Personality Science 1 (3): 280–87. http://dx.doi.org/10.1177/1948550610370492.\nInitial versions of the slides are by Dr. Maria Tackett, Duke University"
  },
  {
    "objectID": "slides/10_non_linear_ch7_o.html",
    "href": "slides/10_non_linear_ch7_o.html",
    "title": "Moving Beyond Linearity",
    "section": "",
    "text": "library(ISLR)\nlibrary(tidymodels)\nlibrary(DALEXtra)\nco &lt;- read.table(\"data/co.dat\", col.names = c(\"age\", \"pop\"), header = FALSE)"
  },
  {
    "objectID": "slides/10_non_linear_ch7_o.html#setup",
    "href": "slides/10_non_linear_ch7_o.html#setup",
    "title": "Moving Beyond Linearity",
    "section": "",
    "text": "library(ISLR)\nlibrary(tidymodels)\nlibrary(DALEXtra)\nco &lt;- read.table(\"data/co.dat\", col.names = c(\"age\", \"pop\"), header = FALSE)"
  },
  {
    "objectID": "slides/10_non_linear_ch7_o.html#non-linear-relationships",
    "href": "slides/10_non_linear_ch7_o.html#non-linear-relationships",
    "title": "Moving Beyond Linearity",
    "section": "Non-linear relationships",
    "text": "Non-linear relationships\n\nWhat have we used so far to deal with non-linear relationships?\n\n. . .\nPolynomials!"
  },
  {
    "objectID": "slides/10_non_linear_ch7_o.html#polynomials",
    "href": "slides/10_non_linear_ch7_o.html#polynomials",
    "title": "Moving Beyond Linearity",
    "section": "Polynomials",
    "text": "Polynomials\n\\[y_i = \\beta_0 + \\beta_1x_i + \\beta_2x_i^2+\\beta_3x_i^3 \\dots + \\beta_dx_i^d+\\epsilon_i\\]\n. . .\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(co, aes(age, pop)) + \n  geom_point() + \n  geom_smooth(formula = y ~ poly(x, 4), method = \"lm\")\n\nThis is data from the Columbia World Fertility Survey (1975-76) to examine household compositions"
  },
  {
    "objectID": "slides/10_non_linear_ch7_o.html#polynomials-1",
    "href": "slides/10_non_linear_ch7_o.html#polynomials-1",
    "title": "Moving Beyond Linearity",
    "section": "Polynomials",
    "text": "Polynomials\n\\[y_i = \\beta_0 + \\beta_1x_i + \\beta_2x_i^2+\\beta_3x_i^3 \\dots + \\beta_dx_i^d+\\epsilon_i\\]\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\nFit here with a 4th degree polynomial\n\n\n\n\nggplot(co, aes(age, pop)) + \n  geom_point() + \n  geom_smooth(formula = y ~ poly(x, 4), method = \"lm\")"
  },
  {
    "objectID": "slides/10_non_linear_ch7_o.html#how-is-it-done",
    "href": "slides/10_non_linear_ch7_o.html#how-is-it-done",
    "title": "Moving Beyond Linearity",
    "section": "How is it done?",
    "text": "How is it done?\n\nNew variables are created ( \\(X_1 = X\\), \\(X_2 = X^2\\), \\(X_3 = X^3\\), etc) and treated as multiple linear regression\nWe are not interested in the individual coefficients, we are interested in how a specific \\(x\\) value behaves\n\\(\\hat{f}(x_0) = \\hat\\beta_0 + \\hat\\beta_1x_0 + \\hat\\beta_2x_0^2 + \\hat\\beta_3x_0^3 + \\hat\\beta_4x_0^4\\)\nor more often a change between two values, \\(a\\) and \\(b\\)\n\\(\\hat{f}(b) -\\hat{f}(a) = \\hat\\beta_1b + \\hat\\beta_2b^2 + \\hat\\beta_3b^3 + \\hat\\beta_4b^4 - \\hat\\beta_1a - \\hat\\beta_2a^2 - \\hat\\beta_3a^3 -\\hat\\beta_4a^4\\)\n\\(\\hat{f}(b) -\\hat{f}(a) =\\hat\\beta_1(b-a) + \\hat\\beta_2(b^2-a^2)+\\hat\\beta_3(b^3-a^3)+\\hat\\beta_4(b^4-a^4)\\)"
  },
  {
    "objectID": "slides/10_non_linear_ch7_o.html#polynomial-regression",
    "href": "slides/10_non_linear_ch7_o.html#polynomial-regression",
    "title": "Moving Beyond Linearity",
    "section": "Polynomial Regression",
    "text": "Polynomial Regression\n\\[\\hat{f}(b) -\\hat{f}(a) =\\hat\\beta_1(b-a) + \\hat\\beta_2(b^2-a^2)+\\hat\\beta_3(b^3-a^3)+\\hat\\beta_4(b^4-a^4)\\]\n\nHow do you pick \\(a\\) and \\(b\\)?\n\n\nIf given no other information, a sensible choice may be the 25th and 75th percentiles of \\(x\\)"
  },
  {
    "objectID": "slides/10_non_linear_ch7_o.html#polynomial-regression-1",
    "href": "slides/10_non_linear_ch7_o.html#polynomial-regression-1",
    "title": "Moving Beyond Linearity",
    "section": "Polynomial Regression",
    "text": "Polynomial Regression\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(co, aes(x = age, y = pop)) + \n  geom_point() + \n  geom_smooth(formula = y ~ poly(x, 4), method = \"lm\") + \n  geom_vline(xintercept = c(24.5, 73.5), lty = 2)"
  },
  {
    "objectID": "slides/10_non_linear_ch7_o.html#application-exercise",
    "href": "slides/10_non_linear_ch7_o.html#application-exercise",
    "title": "Moving Beyond Linearity",
    "section": " Application Exercise",
    "text": "Application Exercise\n\\[pop = \\beta_0 + \\beta_1age + \\beta_2age^2 + \\beta_3age^3 +\\beta_4age^4+ \\epsilon\\]\n. . .\n\nUsing the information below, write out the equation to predicted change in population from a change in age from the 25th percentile (24.5) to a 75th percentile (73.5).\n\n\nModel ResultCode\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n1672.0854\n64.5606\n25.8995\n0.0000\n\n\nage\n-10.6429\n9.2268\n-1.1535\n0.2516\n\n\nI(age^2)\n-1.1427\n0.3857\n-2.9627\n0.0039\n\n\nI(age^3)\n0.0216\n0.0059\n3.6498\n0.0004\n\n\nI(age^4)\n-0.0001\n0.0000\n-3.6540\n0.0004\n\n\n\n\n\n\n\n\nlm(pop ~ age + I(age^2) + I(age^3) + I(age^4), data = co) %&gt;%\n  tidy()"
  },
  {
    "objectID": "slides/10_non_linear_ch7_o.html#choosing-d",
    "href": "slides/10_non_linear_ch7_o.html#choosing-d",
    "title": "Moving Beyond Linearity",
    "section": "Choosing \\(d\\)",
    "text": "Choosing \\(d\\)\n\\[y_i = \\beta_0 + \\beta_1x_i + \\beta_2x_i^2+\\beta_3x_i^3 \\dots + \\beta_dx_i^d+\\epsilon_i\\]\n\nEither:\n\n\nPre-specify \\(d\\) (before looking 👀 at your data!)\nUse cross-validation to pick \\(d\\) (take statistical learning!)\n\n\n\nWhy before looking?"
  },
  {
    "objectID": "slides/10_non_linear_ch7_o.html#polynomial-regression-2",
    "href": "slides/10_non_linear_ch7_o.html#polynomial-regression-2",
    "title": "Moving Beyond Linearity",
    "section": "Polynomial Regression",
    "text": "Polynomial Regression\nPolynomials have notoriously bad tail behavior (so they can be bad for extrapolation)\n\nWhat does this mean?"
  },
  {
    "objectID": "slides/10_non_linear_ch7_o.html#step-functions",
    "href": "slides/10_non_linear_ch7_o.html#step-functions",
    "title": "Moving Beyond Linearity",
    "section": "Step functions",
    "text": "Step functions\nAnother way to create a transformation is to cut the variable into distinct regions\n\\[C_1(X) = I(X &lt; 35), C_2(X) = I(35\\leq X&lt;65), C_3(X) = I(X \\geq 65)\\]\n. . .\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmod &lt;- lm(pop ~ I(age &lt; 35) + I(age &gt;=35 & age &lt; 65) + I(age &gt;= 65), data = co)\np &lt;- predict(mod)\nggplot(co, aes(x = age, y = pop)) +\n  geom_point() +\n  geom_line(aes(x = age, y = p), color = \"blue\")"
  },
  {
    "objectID": "slides/10_non_linear_ch7_o.html#step-functions-1",
    "href": "slides/10_non_linear_ch7_o.html#step-functions-1",
    "title": "Moving Beyond Linearity",
    "section": "Step functions",
    "text": "Step functions\n\nCreate dummy variables for each group\nInclude each of these variables in multiple regression\nThe choice of cutpoints or knots can be problematic (and make a big difference!)"
  },
  {
    "objectID": "slides/10_non_linear_ch7_o.html#step-functions-2",
    "href": "slides/10_non_linear_ch7_o.html#step-functions-2",
    "title": "Moving Beyond Linearity",
    "section": "Step functions",
    "text": "Step functions\n\\[C_1(X) = I(X &lt; 35), C_2(X) = I(35\\leq X&lt;65), C_3(X) = I(X \\geq 65)\\]\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmod &lt;- lm(pop ~ I(age &lt; 35) + I(age &gt;=35 & age &lt; 65) + I(age &gt;= 65), data = co)\np &lt;- predict(mod)\nggplot(co, aes(x = age, y = pop)) +\n  geom_point() +\n  geom_line(aes(x = age, y = p), color = \"blue\")\n\n\n\n\n\nWhat is the predicted value when \\(age = 25\\)?"
  },
  {
    "objectID": "slides/10_non_linear_ch7_o.html#step-functions-3",
    "href": "slides/10_non_linear_ch7_o.html#step-functions-3",
    "title": "Moving Beyond Linearity",
    "section": "Step functions",
    "text": "Step functions\n\\[C_1(X) = I(X &lt; 15), C_2(X) = I(15\\leq X&lt;65), C_3(X) = I(X \\geq 65)\\]\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmod &lt;- lm(pop ~ I(age &lt; 15) + I(age &gt;=15 & age &lt; 65) + I(age &gt;= 65), data = co)\np &lt;- predict(mod)\nggplot(co, aes(x = age, y = pop)) +\n  geom_point() +\n  geom_line(aes(x = age, y = p), color = \"blue\")\n\n\n\n\n. . .\n\nWhat is the predicted value when \\(age = 25\\)?"
  },
  {
    "objectID": "slides/10_non_linear_ch7_o.html#piecewise-polynomials",
    "href": "slides/10_non_linear_ch7_o.html#piecewise-polynomials",
    "title": "Moving Beyond Linearity",
    "section": "Piecewise polynomials",
    "text": "Piecewise polynomials\nInstead of a single polynomial in \\(X\\) over it’s whole domain, we can use different polynomials in regions defined by knots\n\\[y_i = \\begin{cases}\\beta_{01}+\\beta_{11}x_i + \\beta_{21}x^2_i+\\beta_{31}x^3_i+\\epsilon_i& \\textrm{if } x_i &lt; c\\\\ \\beta_{02}+\\beta_{12}x_i + \\beta_{22}x_i^2 + \\beta_{32}x_{i}^3+\\epsilon_i&\\textrm{if }x_i\\geq c\\end{cases}\\]\n\nWhat could go wrong here?\n\n\nIt would be nice to have constraints (like continuity!)\nInsert splines!"
  },
  {
    "objectID": "slides/10_non_linear_ch7_o.html#examples",
    "href": "slides/10_non_linear_ch7_o.html#examples",
    "title": "Moving Beyond Linearity",
    "section": "Examples",
    "text": "Examples"
  },
  {
    "objectID": "slides/10_non_linear_ch7_o.html#linear-splines",
    "href": "slides/10_non_linear_ch7_o.html#linear-splines",
    "title": "Moving Beyond Linearity",
    "section": "Linear Splines",
    "text": "Linear Splines\nA linear spline with knots at \\(\\xi_k\\), \\(k = 1,\\dots, K\\) is a piecewise linear polynomial continuous at each knot\n\\[y_i = \\beta_0 + \\beta_1b_1(x_i)+\\beta_2b_2(x_i)+\\dots+\\beta_{K+1}b_{K+1}(x_i)+\\epsilon_i\\]\n\n\\(b_k\\) are basis functions\n\\(\\begin{align}b_1(x_i)&=x_i\\\\ b_{k+1}(x_i)&=(x_i-\\xi_k)_+,k=1,\\dots,K\\end{align}\\)\nHere \\(()_+\\) means the positive part\n\\((x_i-\\xi_k)_+=\\begin{cases}x_i-\\xi_k & \\textrm{if } x_i&gt;\\xi_k\\\\0&\\textrm{otherwise}\\end{cases}\\)"
  },
  {
    "objectID": "slides/10_non_linear_ch7_o.html#application-exercise-1",
    "href": "slides/10_non_linear_ch7_o.html#application-exercise-1",
    "title": "Moving Beyond Linearity",
    "section": " Application Exercise",
    "text": "Application Exercise\nLet’s create data set to fit a linear spline with 2 knots: 35 and 65.\n\n\n\n\n\nx\n\n\n\n\n4\n\n\n15\n\n\n25\n\n\n37\n\n\n49\n\n\n66\n\n\n70\n\n\n80\n\n\n\n\n\n\nUsing the data to the left create a new dataset with three variables: \\(b_1(x), b_2(x), b_3(x)\\)\nWrite out the equation you would be fitting to estimate the effect on some outcome \\(y\\) using this linear spline"
  },
  {
    "objectID": "slides/10_non_linear_ch7_o.html#linear-spline",
    "href": "slides/10_non_linear_ch7_o.html#linear-spline",
    "title": "Moving Beyond Linearity",
    "section": "Linear Spline",
    "text": "Linear Spline\n\n\n\n\n\nx\n\n\n\n\n4\n\n\n15\n\n\n25\n\n\n37\n\n\n49\n\n\n66\n\n\n70\n\n\n80\n\n\n\n\n\n➡️\n\n\n\n\n\n\\(b_1(x)\\)\n\\(b_2(x)\\)\n\\(b_3(x)\\)\n\n\n\n\n4\n0\n0\n\n\n15\n0\n0\n\n\n25\n0\n0\n\n\n37\n2\n0\n\n\n49\n14\n0\n\n\n66\n31\n1\n\n\n70\n35\n5\n\n\n80\n45\n15"
  },
  {
    "objectID": "slides/10_non_linear_ch7_o.html#application-exercise-2",
    "href": "slides/10_non_linear_ch7_o.html#application-exercise-2",
    "title": "Moving Beyond Linearity",
    "section": " Application Exercise",
    "text": "Application Exercise\nBelow is a linear regression model fit to include the 3 bases you just created with 2 knots: 35 and 65. Use the information here to draw the relationship between \\(x\\) and \\(y\\).\n\nResultCode\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-0.3\n0.2\n-1.3\n0.3\n\n\nb1\n2.0\n0.0\n231.3\n0.0\n\n\nb2\n-2.0\n0.0\n-130.0\n0.0\n\n\nb3\n-3.0\n0.0\n-116.5\n0.0\n\n\n\n\n\n\n\n\nset.seed(1)\nd &lt;- tibble(\n  b1 = c(4, 15, 25, 37, 49, 66, 70, 80),\n  b2 = ifelse(b1 &lt; 35, 0, b1 - 35),\n  b3 = ifelse(b1 &lt; 65, 0, b1 - 65),\n  y = 2 * b1 + -2 * b2 -3 * b3 + rnorm(8, sd = 0.25)\n)\nlm(y ~ b1 + b2 + b3, data = d) |&gt;\n  tidy() |&gt;\n  knitr::kable(digits = 1)"
  },
  {
    "objectID": "slides/10_non_linear_ch7_o.html#linear-splines-1",
    "href": "slides/10_non_linear_ch7_o.html#linear-splines-1",
    "title": "Moving Beyond Linearity",
    "section": "Linear Splines",
    "text": "Linear Splines\n\n\n\n\n\\(b_1(x)\\)\n\\(b_2(x)\\)\n\\(b_3(x)\\)\n\n\n\n\n4\n0\n0\n\n\n15\n0\n0\n\n\n25\n0\n0\n\n\n37\n2\n0\n\n\n49\n14\n0\n\n\n66\n31\n1\n\n\n70\n35\n5\n\n\n80\n45\n15"
  },
  {
    "objectID": "slides/10_non_linear_ch7_o.html#linear-splines-2",
    "href": "slides/10_non_linear_ch7_o.html#linear-splines-2",
    "title": "Moving Beyond Linearity",
    "section": "Linear Splines",
    "text": "Linear Splines\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\np_ &lt;- predict(lm(y ~ b1 + b2 + b3, data = d))\n\nggplot(d, aes(x = b1, y = p_)) +\n  geom_point() +\n  labs(x = \"X\",\n       y = expression(hat(y)))"
  },
  {
    "objectID": "slides/10_non_linear_ch7_o.html#linear-splines-3",
    "href": "slides/10_non_linear_ch7_o.html#linear-splines-3",
    "title": "Moving Beyond Linearity",
    "section": "Linear Splines",
    "text": "Linear Splines\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nnewdat &lt;- tibble(\n  b1 = 4:80,\n  b2 = ifelse(b1 &gt; 35, b1 - 35, 0),\n  b3 = ifelse(b1 &gt; 65, b1 - 65, 0)\n)\np &lt;- predict(lm(y ~ b1 + b2 + b3, data = d),\n             newdata = newdat)\n\nggplot(newdat, aes(x = b1, y = p)) +\n  geom_point() + \n  labs(x = \"X\",\n       y = expression(hat(y)))"
  },
  {
    "objectID": "slides/10_non_linear_ch7_o.html#cubic-splines",
    "href": "slides/10_non_linear_ch7_o.html#cubic-splines",
    "title": "Moving Beyond Linearity",
    "section": "Cubic Splines",
    "text": "Cubic Splines\nA cubic splines with knots at \\(\\xi_i, k = 1, \\dots, K\\) is a piecewise cubic polynomial with continuous derivatives up to order 2 at each knot.\nAgain we can represent this model with truncated power functions\n\\[y_i = \\beta_0 + \\beta_1b_1(x_i)+\\beta_2b_2(x_i)+\\dots+\\beta_{K+3}b_{K+3}(x_i) + \\epsilon_i\\]\n\\[\\begin{align}b_1(x_i)&=x_i\\\\b_2(x_i)&=x_i^2\\\\b_3(x_i)&=x_i^3\\\\b_{k+3}(x_i)&=(x_i-\\xi_k)^3_+, k = 1,\\dots,K\\end{align}\\]\nwhere\n\\[(x_i-\\xi_k)^{3}_+=\\begin{cases}(x_i-\\xi_k)^3&\\textrm{if }x_i&gt;\\xi_k\\\\0&\\textrm{otherwise}\\end{cases}\\]"
  },
  {
    "objectID": "slides/10_non_linear_ch7_o.html#application-exercise-3",
    "href": "slides/10_non_linear_ch7_o.html#application-exercise-3",
    "title": "Moving Beyond Linearity",
    "section": " Application Exercise",
    "text": "Application Exercise\nLet’s create data set to fit a cubic spline with 2 knots: 35 and 65.\n\n\n\n\n\nx\n\n\n\n\n4\n\n\n15\n\n\n25\n\n\n37\n\n\n49\n\n\n66\n\n\n70\n\n\n80\n\n\n\n\n\n\nUsing the data to the left create a new dataset with five variables: \\(b_1(x), b_2(x), b_3(x), b_4(x), b_5(x)\\)\nWrite out the equation you would be fitting to estimate the effect on some outcome y using this cubic spline"
  },
  {
    "objectID": "slides/10_non_linear_ch7_o.html#cubic-spline",
    "href": "slides/10_non_linear_ch7_o.html#cubic-spline",
    "title": "Moving Beyond Linearity",
    "section": "Cubic Spline",
    "text": "Cubic Spline\n\n\n\n\n\nx\n\n\n\n\n4\n\n\n15\n\n\n25\n\n\n37\n\n\n49\n\n\n66\n\n\n70\n\n\n80\n\n\n\n\n\n➡️\n\n\n\nExampleCode\n\n\n\n\n\n\n\nb1\nb2\nb3\nb4\nb5\n\n\n\n\n4\n16\n64\n0\n0\n\n\n15\n225\n3375\n0\n0\n\n\n25\n625\n15625\n0\n0\n\n\n37\n1369\n50653\n8\n0\n\n\n49\n2401\n117649\n2744\n0\n\n\n66\n4356\n287496\n29791\n1\n\n\n70\n4900\n343000\n42875\n125\n\n\n80\n6400\n512000\n91125\n3375\n\n\n\n\n\n\n\n\nd |&gt;\n  mutate(b2 = b1^2,\n         b3 = b1^3,\n         b4 = ifelse(b1 &gt; 35, (b1 - 35)^3, 0),\n         b5 = ifelse(b1 &gt; 65, (b1 - 65)^3, 0)\n  ) -&gt; d\nd |&gt;\n  select(-y) |&gt;\n  knitr::kable()"
  },
  {
    "objectID": "slides/10_non_linear_ch7_o.html#cubic-spline-1",
    "href": "slides/10_non_linear_ch7_o.html#cubic-spline-1",
    "title": "Moving Beyond Linearity",
    "section": "Cubic Spline",
    "text": "Cubic Spline\n\nFit ResultCode\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n1.172\n8.282\n0.141\n0.900\n\n\nb1\n1.520\n1.565\n0.971\n0.434\n\n\nb2\n0.040\n0.075\n0.528\n0.650\n\n\nb3\n-0.001\n0.001\n-0.855\n0.483\n\n\nb4\n0.001\n0.002\n0.635\n0.590\n\n\nb5\n-0.006\n0.007\n-0.860\n0.480\n\n\n\n\n\n\n\n\nlm(y ~ b1 + b2 + b3 + b4 + b5, data = d) |&gt;\n  tidy() |&gt;\n  knitr::kable(digits = 3)"
  },
  {
    "objectID": "slides/10_non_linear_ch7_o.html#cubic-splines-1",
    "href": "slides/10_non_linear_ch7_o.html#cubic-splines-1",
    "title": "Moving Beyond Linearity",
    "section": "Cubic Splines",
    "text": "Cubic Splines\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\np_ &lt;- predict(lm(y ~ b1 + b2 + b3 + b4 + b5, data = d))\n\nggplot(d, aes(x = b1, y = p_)) +\n  geom_point() +\n  labs(x = \"X\",\n       y = expression(hat(y)))"
  },
  {
    "objectID": "slides/10_non_linear_ch7_o.html#cubic-splines-2",
    "href": "slides/10_non_linear_ch7_o.html#cubic-splines-2",
    "title": "Moving Beyond Linearity",
    "section": "Cubic Splines",
    "text": "Cubic Splines\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nnewdat &lt;- tibble(\n  b1 = 4:80,\n  b2 = b1^2,\n  b3 = b1^3,\n  b4 = ifelse(b1 &gt; 35, (b1 - 35)^3, 0),\n  b5 = ifelse(b1 &gt; 65, (b1 - 65)^3, 0)\n)\np &lt;- predict(lm(y ~ b1 + b2 + b3 + b4 + b5, data = d),\n             newdata = newdat)\n\nggplot(newdat, aes(x = b1, y = p)) +\n  geom_point() + \n  labs(x = \"X\",\n       y = expression(hat(y)))"
  },
  {
    "objectID": "slides/10_non_linear_ch7_o.html#cubic-splines-3",
    "href": "slides/10_non_linear_ch7_o.html#cubic-splines-3",
    "title": "Moving Beyond Linearity",
    "section": "Cubic Splines",
    "text": "Cubic Splines\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nnewdat &lt;- tibble(\n  b1 = -100:100,\n  b2 = b1^2,\n  b3 = b1^3,\n  b4 = ifelse(b1 &gt; 35, (b1 - 35)^3, 0),\n  b5 = ifelse(b1 &gt; 65, (b1 - 65)^3, 0)\n)\np &lt;- predict(lm(y ~ b1 + b2 + b3 + b4 + b5, data = d),\n             newdata = newdat)\n\nggplot(newdat, aes(x = b1, y = p)) +\n  geom_point() + \n  geom_vline(xintercept = c(4, 80), lty = 2) +\n  labs(x = \"X\",\n       y = expression(hat(y)))"
  },
  {
    "objectID": "slides/10_non_linear_ch7_o.html#natural-cubic-splines",
    "href": "slides/10_non_linear_ch7_o.html#natural-cubic-splines",
    "title": "Moving Beyond Linearity",
    "section": "Natural cubic splines",
    "text": "Natural cubic splines\nA natural cubic spline extrapolates linearly beyond the boundary knots\nThis adds 4 extra constraints and allows us to put more internal knots for the same degrees of freedom as a regular cubic spline"
  },
  {
    "objectID": "slides/10_non_linear_ch7_o.html#natural-cubic-splines-1",
    "href": "slides/10_non_linear_ch7_o.html#natural-cubic-splines-1",
    "title": "Moving Beyond Linearity",
    "section": "Natural Cubic Splines",
    "text": "Natural Cubic Splines\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\np &lt;- predict(lm(y ~ splines::ns(b1, knots = c(35, 65)), data = d),\n             newdata = newdat)\n\nggplot(newdat, aes(x = b1, y = p)) +\n  geom_point() + \n  geom_vline(xintercept = c(4, 80), lty = 2) +\n  labs(x = \"X\",\n       y = expression(hat(y)))"
  },
  {
    "objectID": "slides/10_non_linear_ch7_o.html#natural-cubic-splines-2",
    "href": "slides/10_non_linear_ch7_o.html#natural-cubic-splines-2",
    "title": "Moving Beyond Linearity",
    "section": "Natural Cubic Splines",
    "text": "Natural Cubic Splines\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nda &lt;- tibble(\n  x = newdat$b1,\n  ns = predict(lm(y ~ splines::ns(b1, knots = c(35, 65)), data = d),\n             newdata = newdat),\n  cubic = predict(lm(y ~ b1 + b2 + b3 + b4 + b5, data = d), \n                  newdata = newdat),\n  linear = predict(lm(y ~ b1 + ifelse(b1&gt;35, b1 - 35, 0) + ifelse(b1&gt;65, b1 - 65, 0), data = d),\n                   newdata = newdat)\n) |&gt;\n  pivot_longer(ns:linear)\n\nda |&gt;\n  filter(name != \"linear\") |&gt;\nggplot(aes(x = x, y = value, color = name)) +\n  geom_point(alpha = 0.5) + \n  geom_vline(xintercept = c(4, 80), lty = 2) +\n  labs(x = \"X\",\n       y = expression(hat(y)),\n       color = \"Spline\")"
  },
  {
    "objectID": "slides/10_non_linear_ch7_o.html#natural-splines",
    "href": "slides/10_non_linear_ch7_o.html#natural-splines",
    "title": "Moving Beyond Linearity",
    "section": "Natural Splines",
    "text": "Natural Splines\n\nggplot(da, aes(x = x, y = value, color = name)) +\n  geom_point(alpha = 0.5) + \n  geom_vline(xintercept = c(4, 80), lty = 2) +\n  labs(x = \"X\",\n       y = expression(hat(y)),\n       color = \"Spline\")"
  },
  {
    "objectID": "slides/10_non_linear_ch7_o.html#knot-placement",
    "href": "slides/10_non_linear_ch7_o.html#knot-placement",
    "title": "Moving Beyond Linearity",
    "section": "Knot placement",
    "text": "Knot placement\n\nOne strategy is to decide \\(K\\) (the number of knots) in advance and then place them at appropriate quantiles of the observed \\(X\\)\nA cubic spline with \\(K\\) knots has \\(K+3\\) parameters (or degrees of freedom!)\nA natural spline with \\(K\\) knots has \\(K-1\\) degrees of freedom"
  },
  {
    "objectID": "slides/10_non_linear_ch7_o.html#knot-placement-1",
    "href": "slides/10_non_linear_ch7_o.html#knot-placement-1",
    "title": "Moving Beyond Linearity",
    "section": "Knot placement",
    "text": "Knot placement\nHere is a comparison of a degree-14 polynomial and natural cubic spline (both have 15 degrees of freedom)"
  },
  {
    "objectID": "slides/10_non_linear_ch7_o.html#acknowledgements",
    "href": "slides/10_non_linear_ch7_o.html#acknowledgements",
    "title": "Moving Beyond Linearity",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThe content in the slides is from\n\nChapter 7 of Introduction to Statistical Learning, 2nd Ed by James, Witten, Hastie, and Tibshirani\nInitial versions of the slides are by Dr. Lucy D’Agostino McGowan"
  },
  {
    "objectID": "slides/10_non_linear_ch7.html#setup",
    "href": "slides/10_non_linear_ch7.html#setup",
    "title": "Moving Beyond Linearity",
    "section": "Setup",
    "text": "Setup\n\nlibrary(ISLR)\nlibrary(tidymodels)\nlibrary(DALEXtra)\nco &lt;- read.table(\"data/co.dat\", col.names = c(\"age\", \"pop\"), header = FALSE)"
  },
  {
    "objectID": "slides/10_non_linear_ch7.html#non-linear-relationships",
    "href": "slides/10_non_linear_ch7.html#non-linear-relationships",
    "title": "Moving Beyond Linearity",
    "section": "Non-linear relationships",
    "text": "Non-linear relationships\n\nWhat have we used so far to deal with non-linear relationships?\n\n\nPolynomials!"
  },
  {
    "objectID": "slides/10_non_linear_ch7.html#polynomials",
    "href": "slides/10_non_linear_ch7.html#polynomials",
    "title": "Moving Beyond Linearity",
    "section": "Polynomials",
    "text": "Polynomials\n\\[y_i = \\beta_0 + \\beta_1x_i + \\beta_2x_i^2+\\beta_3x_i^3 \\dots + \\beta_dx_i^d+\\epsilon_i\\]\n\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(co, aes(age, pop)) + \n  geom_point() + \n  geom_smooth(formula = y ~ poly(x, 4), method = \"lm\")\n\nThis is data from the Columbia World Fertility Survey (1975-76) to examine household compositions"
  },
  {
    "objectID": "slides/10_non_linear_ch7.html#polynomials-1",
    "href": "slides/10_non_linear_ch7.html#polynomials-1",
    "title": "Moving Beyond Linearity",
    "section": "Polynomials",
    "text": "Polynomials\n\\[y_i = \\beta_0 + \\beta_1x_i + \\beta_2x_i^2+\\beta_3x_i^3 \\dots + \\beta_dx_i^d+\\epsilon_i\\]\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\nFit here with a 4th degree polynomial\n\n\n\n\nggplot(co, aes(age, pop)) + \n  geom_point() + \n  geom_smooth(formula = y ~ poly(x, 4), method = \"lm\")"
  },
  {
    "objectID": "slides/10_non_linear_ch7.html#how-is-it-done",
    "href": "slides/10_non_linear_ch7.html#how-is-it-done",
    "title": "Moving Beyond Linearity",
    "section": "How is it done?",
    "text": "How is it done?\n\nNew variables are created ( \\(X_1 = X\\), \\(X_2 = X^2\\), \\(X_3 = X^3\\), etc) and treated as multiple linear regression\nWe are not interested in the individual coefficients, we are interested in how a specific \\(x\\) value behaves\n\\(\\hat{f}(x_0) = \\hat\\beta_0 + \\hat\\beta_1x_0 + \\hat\\beta_2x_0^2 + \\hat\\beta_3x_0^3 + \\hat\\beta_4x_0^4\\)\nor more often a change between two values, \\(a\\) and \\(b\\)\n\\(\\hat{f}(b) -\\hat{f}(a) = \\hat\\beta_1b + \\hat\\beta_2b^2 + \\hat\\beta_3b^3 + \\hat\\beta_4b^4 - \\hat\\beta_1a - \\hat\\beta_2a^2 - \\hat\\beta_3a^3 -\\hat\\beta_4a^4\\)\n\\(\\hat{f}(b) -\\hat{f}(a) =\\hat\\beta_1(b-a) + \\hat\\beta_2(b^2-a^2)+\\hat\\beta_3(b^3-a^3)+\\hat\\beta_4(b^4-a^4)\\)"
  },
  {
    "objectID": "slides/10_non_linear_ch7.html#polynomial-regression",
    "href": "slides/10_non_linear_ch7.html#polynomial-regression",
    "title": "Moving Beyond Linearity",
    "section": "Polynomial Regression",
    "text": "Polynomial Regression\n\\[\\hat{f}(b) -\\hat{f}(a) =\\hat\\beta_1(b-a) + \\hat\\beta_2(b^2-a^2)+\\hat\\beta_3(b^3-a^3)+\\hat\\beta_4(b^4-a^4)\\]\n\nHow do you pick \\(a\\) and \\(b\\)?\n\n\nIf given no other information, a sensible choice may be the 25th and 75th percentiles of \\(x\\)"
  },
  {
    "objectID": "slides/10_non_linear_ch7.html#polynomial-regression-1",
    "href": "slides/10_non_linear_ch7.html#polynomial-regression-1",
    "title": "Moving Beyond Linearity",
    "section": "Polynomial Regression",
    "text": "Polynomial Regression\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(co, aes(x = age, y = pop)) + \n  geom_point() + \n  geom_smooth(formula = y ~ poly(x, 4), method = \"lm\") + \n  geom_vline(xintercept = c(24.5, 73.5), lty = 2)"
  },
  {
    "objectID": "slides/10_non_linear_ch7.html#application-exercise",
    "href": "slides/10_non_linear_ch7.html#application-exercise",
    "title": "Moving Beyond Linearity",
    "section": " Application Exercise",
    "text": "Application Exercise\n\\[pop = \\beta_0 + \\beta_1age + \\beta_2age^2 + \\beta_3age^3 +\\beta_4age^4+ \\epsilon\\]\n\n\nUsing the information below, write out the equation to predicted change in population from a change in age from the 25th percentile (24.5) to a 75th percentile (73.5).\n\n\nModel ResultCode\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n1672.0854\n64.5606\n25.8995\n0.0000\n\n\nage\n-10.6429\n9.2268\n-1.1535\n0.2516\n\n\nI(age^2)\n-1.1427\n0.3857\n-2.9627\n0.0039\n\n\nI(age^3)\n0.0216\n0.0059\n3.6498\n0.0004\n\n\nI(age^4)\n-0.0001\n0.0000\n-3.6540\n0.0004\n\n\n\n\n\n\n\n\nlm(pop ~ age + I(age^2) + I(age^3) + I(age^4), data = co) %&gt;%\n  tidy()"
  },
  {
    "objectID": "slides/10_non_linear_ch7.html#choosing-d",
    "href": "slides/10_non_linear_ch7.html#choosing-d",
    "title": "Moving Beyond Linearity",
    "section": "Choosing \\(d\\)",
    "text": "Choosing \\(d\\)\n\\[y_i = \\beta_0 + \\beta_1x_i + \\beta_2x_i^2+\\beta_3x_i^3 \\dots + \\beta_dx_i^d+\\epsilon_i\\]\nEither:\n\n\nPre-specify \\(d\\) (before looking 👀 at your data!)\nUse cross-validation to pick \\(d\\) (take statistical learning!)\n\n\n\nWhy before looking?"
  },
  {
    "objectID": "slides/10_non_linear_ch7.html#polynomial-regression-2",
    "href": "slides/10_non_linear_ch7.html#polynomial-regression-2",
    "title": "Moving Beyond Linearity",
    "section": "Polynomial Regression",
    "text": "Polynomial Regression\nPolynomials have notoriously bad tail behavior (so they can be bad for extrapolation)\n\nWhat does this mean?"
  },
  {
    "objectID": "slides/10_non_linear_ch7.html#step-functions",
    "href": "slides/10_non_linear_ch7.html#step-functions",
    "title": "Moving Beyond Linearity",
    "section": "Step functions",
    "text": "Step functions\nAnother way to create a transformation is to cut the variable into distinct regions\n\\[C_1(X) = I(X &lt; 35), C_2(X) = I(35\\leq X&lt;65), C_3(X) = I(X \\geq 65)\\]\n\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmod &lt;- lm(pop ~ I(age &lt; 35) + I(age &gt;=35 & age &lt; 65) + I(age &gt;= 65), data = co)\np &lt;- predict(mod)\nggplot(co, aes(x = age, y = pop)) +\n  geom_point() +\n  geom_line(aes(x = age, y = p), color = \"blue\")"
  },
  {
    "objectID": "slides/10_non_linear_ch7.html#step-functions-1",
    "href": "slides/10_non_linear_ch7.html#step-functions-1",
    "title": "Moving Beyond Linearity",
    "section": "Step functions",
    "text": "Step functions\n\nCreate dummy variables for each group\nInclude each of these variables in multiple regression\nThe choice of cutpoints or knots can be problematic (and make a big difference!)"
  },
  {
    "objectID": "slides/10_non_linear_ch7.html#step-functions-2",
    "href": "slides/10_non_linear_ch7.html#step-functions-2",
    "title": "Moving Beyond Linearity",
    "section": "Step functions",
    "text": "Step functions\n\\[C_1(X) = I(X &lt; 35), C_2(X) = I(35\\leq X&lt;65), C_3(X) = I(X \\geq 65)\\]\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmod &lt;- lm(pop ~ I(age &lt; 35) + I(age &gt;=35 & age &lt; 65) + I(age &gt;= 65), data = co)\np &lt;- predict(mod)\nggplot(co, aes(x = age, y = pop)) +\n  geom_point() +\n  geom_line(aes(x = age, y = p), color = \"blue\")\n\n\n\n\n\nWhat is the predicted value when \\(age = 25\\)?"
  },
  {
    "objectID": "slides/10_non_linear_ch7.html#step-functions-3",
    "href": "slides/10_non_linear_ch7.html#step-functions-3",
    "title": "Moving Beyond Linearity",
    "section": "Step functions",
    "text": "Step functions\n\\[C_1(X) = I(X &lt; 15), C_2(X) = I(15\\leq X&lt;65), C_3(X) = I(X \\geq 65)\\]\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmod &lt;- lm(pop ~ I(age &lt; 15) + I(age &gt;=15 & age &lt; 65) + I(age &gt;= 65), data = co)\np &lt;- predict(mod)\nggplot(co, aes(x = age, y = pop)) +\n  geom_point() +\n  geom_line(aes(x = age, y = p), color = \"blue\")\n\n\n\n\n\n\nWhat is the predicted value when \\(age = 25\\)?"
  },
  {
    "objectID": "slides/10_non_linear_ch7.html#piecewise-polynomials",
    "href": "slides/10_non_linear_ch7.html#piecewise-polynomials",
    "title": "Moving Beyond Linearity",
    "section": "Piecewise polynomials",
    "text": "Piecewise polynomials\nInstead of a single polynomial in \\(X\\) over it’s whole domain, we can use different polynomials in regions defined by knots\n\\[y_i = \\begin{cases}\\beta_{01}+\\beta_{11}x_i + \\beta_{21}x^2_i+\\beta_{31}x^3_i+\\epsilon_i& \\textrm{if } x_i &lt; c\\\\ \\beta_{02}+\\beta_{12}x_i + \\beta_{22}x_i^2 + \\beta_{32}x_{i}^3+\\epsilon_i&\\textrm{if }x_i\\geq c\\end{cases}\\]\n\nWhat could go wrong here?\n\n\nIt would be nice to have constraints (like continuity!)\nInsert splines!"
  },
  {
    "objectID": "slides/10_non_linear_ch7.html#examples",
    "href": "slides/10_non_linear_ch7.html#examples",
    "title": "Moving Beyond Linearity",
    "section": "Examples",
    "text": "Examples"
  },
  {
    "objectID": "slides/10_non_linear_ch7.html#linear-splines",
    "href": "slides/10_non_linear_ch7.html#linear-splines",
    "title": "Moving Beyond Linearity",
    "section": "Linear Splines",
    "text": "Linear Splines\nA linear spline with knots at \\(\\xi_k\\), \\(k = 1,\\dots, K\\) is a piecewise linear polynomial continuous at each knot\n\\[y_i = \\beta_0 + \\beta_1b_1(x_i)+\\beta_2b_2(x_i)+\\dots+\\beta_{K+1}b_{K+1}(x_i)+\\epsilon_i\\]\n\n\\(b_k\\) are basis functions\n\\(\\begin{align}b_1(x_i)&=x_i\\\\ b_{k+1}(x_i)&=(x_i-\\xi_k)_+,k=1,\\dots,K\\end{align}\\)\nHere \\(()_+\\) means the positive part\n\\((x_i-\\xi_k)_+=\\begin{cases}x_i-\\xi_k & \\textrm{if } x_i&gt;\\xi_k\\\\0&\\textrm{otherwise}\\end{cases}\\)"
  },
  {
    "objectID": "slides/10_non_linear_ch7.html#application-exercise-1",
    "href": "slides/10_non_linear_ch7.html#application-exercise-1",
    "title": "Moving Beyond Linearity",
    "section": " Application Exercise",
    "text": "Application Exercise\nLet’s create data set to fit a linear spline with 2 knots: 35 and 65.\n\n\n\n\n\nx\n\n\n\n\n4\n\n\n15\n\n\n25\n\n\n37\n\n\n49\n\n\n66\n\n\n70\n\n\n80\n\n\n\n\n\n\nUsing the data to the left create a new dataset with three variables: \\(b_1(x), b_2(x), b_3(x)\\)\nWrite out the equation you would be fitting to estimate the effect on some outcome \\(y\\) using this linear spline"
  },
  {
    "objectID": "slides/10_non_linear_ch7.html#linear-spline",
    "href": "slides/10_non_linear_ch7.html#linear-spline",
    "title": "Moving Beyond Linearity",
    "section": "Linear Spline",
    "text": "Linear Spline\n\n\n\n\n\nx\n\n\n\n\n4\n\n\n15\n\n\n25\n\n\n37\n\n\n49\n\n\n66\n\n\n70\n\n\n80\n\n\n\n\n➡️\n\n\n\n\n\\(b_1(x)\\)\n\\(b_2(x)\\)\n\\(b_3(x)\\)\n\n\n\n\n4\n0\n0\n\n\n15\n0\n0\n\n\n25\n0\n0\n\n\n37\n2\n0\n\n\n49\n14\n0\n\n\n66\n31\n1\n\n\n70\n35\n5\n\n\n80\n45\n15"
  },
  {
    "objectID": "slides/10_non_linear_ch7.html#application-exercise-2",
    "href": "slides/10_non_linear_ch7.html#application-exercise-2",
    "title": "Moving Beyond Linearity",
    "section": " Application Exercise",
    "text": "Application Exercise\nBelow is a linear regression model fit to include the 3 bases you just created with 2 knots: 35 and 65. Use the information here to draw the relationship between \\(x\\) and \\(y\\).\n\nResultCode\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-0.3\n0.2\n-1.3\n0.3\n\n\nb1\n2.0\n0.0\n231.3\n0.0\n\n\nb2\n-2.0\n0.0\n-130.0\n0.0\n\n\nb3\n-3.0\n0.0\n-116.5\n0.0\n\n\n\n\n\n\n\n\nset.seed(1)\nd &lt;- tibble(\n  b1 = c(4, 15, 25, 37, 49, 66, 70, 80),\n  b2 = ifelse(b1 &lt; 35, 0, b1 - 35),\n  b3 = ifelse(b1 &lt; 65, 0, b1 - 65),\n  y = 2 * b1 + -2 * b2 -3 * b3 + rnorm(8, sd = 0.25)\n)\nlm(y ~ b1 + b2 + b3, data = d) |&gt;\n  tidy() |&gt;\n  knitr::kable(digits = 1)"
  },
  {
    "objectID": "slides/10_non_linear_ch7.html#linear-splines-1",
    "href": "slides/10_non_linear_ch7.html#linear-splines-1",
    "title": "Moving Beyond Linearity",
    "section": "Linear Splines",
    "text": "Linear Splines\n\n\n\n\n\\(b_1(x)\\)\n\\(b_2(x)\\)\n\\(b_3(x)\\)\n\n\n\n\n4\n0\n0\n\n\n15\n0\n0\n\n\n25\n0\n0\n\n\n37\n2\n0\n\n\n49\n14\n0\n\n\n66\n31\n1\n\n\n70\n35\n5\n\n\n80\n45\n15"
  },
  {
    "objectID": "slides/10_non_linear_ch7.html#linear-splines-2",
    "href": "slides/10_non_linear_ch7.html#linear-splines-2",
    "title": "Moving Beyond Linearity",
    "section": "Linear Splines",
    "text": "Linear Splines\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\np_ &lt;- predict(lm(y ~ b1 + b2 + b3, data = d))\n\nggplot(d, aes(x = b1, y = p_)) +\n  geom_point() +\n  labs(x = \"X\",\n       y = expression(hat(y)))"
  },
  {
    "objectID": "slides/10_non_linear_ch7.html#linear-splines-3",
    "href": "slides/10_non_linear_ch7.html#linear-splines-3",
    "title": "Moving Beyond Linearity",
    "section": "Linear Splines",
    "text": "Linear Splines\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nnewdat &lt;- tibble(\n  b1 = 4:80,\n  b2 = ifelse(b1 &gt; 35, b1 - 35, 0),\n  b3 = ifelse(b1 &gt; 65, b1 - 65, 0)\n)\np &lt;- predict(lm(y ~ b1 + b2 + b3, data = d),\n             newdata = newdat)\n\nggplot(newdat, aes(x = b1, y = p)) +\n  geom_point() + \n  labs(x = \"X\",\n       y = expression(hat(y)))"
  },
  {
    "objectID": "slides/10_non_linear_ch7.html#cubic-splines",
    "href": "slides/10_non_linear_ch7.html#cubic-splines",
    "title": "Moving Beyond Linearity",
    "section": "Cubic Splines",
    "text": "Cubic Splines\nA cubic splines with knots at \\(\\xi_i, k = 1, \\dots, K\\) is a piecewise cubic polynomial with continuous derivatives up to order 2 at each knot.\nAgain we can represent this model with truncated power functions\n\\[y_i = \\beta_0 + \\beta_1b_1(x_i)+\\beta_2b_2(x_i)+\\dots+\\beta_{K+3}b_{K+3}(x_i) + \\epsilon_i\\]\n\\[\\begin{align}b_1(x_i)&=x_i\\\\b_2(x_i)&=x_i^2\\\\b_3(x_i)&=x_i^3\\\\b_{k+3}(x_i)&=(x_i-\\xi_k)^3_+, k = 1,\\dots,K\\end{align}\\]\nwhere\n\\[(x_i-\\xi_k)^{3}_+=\\begin{cases}(x_i-\\xi_k)^3&\\textrm{if }x_i&gt;\\xi_k\\\\0&\\textrm{otherwise}\\end{cases}\\]"
  },
  {
    "objectID": "slides/10_non_linear_ch7.html#application-exercise-3",
    "href": "slides/10_non_linear_ch7.html#application-exercise-3",
    "title": "Moving Beyond Linearity",
    "section": " Application Exercise",
    "text": "Application Exercise\nLet’s create data set to fit a cubic spline with 2 knots: 35 and 65.\n\n\n\n\n\nx\n\n\n\n\n4\n\n\n15\n\n\n25\n\n\n37\n\n\n49\n\n\n66\n\n\n70\n\n\n80\n\n\n\n\n\n\nUsing the data to the left create a new dataset with five variables: \\(b_1(x), b_2(x), b_3(x), b_4(x), b_5(x)\\)\nWrite out the equation you would be fitting to estimate the effect on some outcome y using this cubic spline"
  },
  {
    "objectID": "slides/10_non_linear_ch7.html#cubic-spline",
    "href": "slides/10_non_linear_ch7.html#cubic-spline",
    "title": "Moving Beyond Linearity",
    "section": "Cubic Spline",
    "text": "Cubic Spline\n\n\n\n\n\nx\n\n\n\n\n4\n\n\n15\n\n\n25\n\n\n37\n\n\n49\n\n\n66\n\n\n70\n\n\n80\n\n\n\n\n➡️\n\n\nExampleCode\n\n\n\n\n\n\n\nb1\nb2\nb3\nb4\nb5\n\n\n\n\n4\n16\n64\n0\n0\n\n\n15\n225\n3375\n0\n0\n\n\n25\n625\n15625\n0\n0\n\n\n37\n1369\n50653\n8\n0\n\n\n49\n2401\n117649\n2744\n0\n\n\n66\n4356\n287496\n29791\n1\n\n\n70\n4900\n343000\n42875\n125\n\n\n80\n6400\n512000\n91125\n3375\n\n\n\n\n\n\n\n\nd |&gt;\n  mutate(b2 = b1^2,\n         b3 = b1^3,\n         b4 = ifelse(b1 &gt; 35, (b1 - 35)^3, 0),\n         b5 = ifelse(b1 &gt; 65, (b1 - 65)^3, 0)\n  ) -&gt; d\nd |&gt;\n  select(-y) |&gt;\n  knitr::kable()"
  },
  {
    "objectID": "slides/10_non_linear_ch7.html#cubic-spline-1",
    "href": "slides/10_non_linear_ch7.html#cubic-spline-1",
    "title": "Moving Beyond Linearity",
    "section": "Cubic Spline",
    "text": "Cubic Spline\n\nFit ResultCode\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n1.172\n8.282\n0.141\n0.900\n\n\nb1\n1.520\n1.565\n0.971\n0.434\n\n\nb2\n0.040\n0.075\n0.528\n0.650\n\n\nb3\n-0.001\n0.001\n-0.855\n0.483\n\n\nb4\n0.001\n0.002\n0.635\n0.590\n\n\nb5\n-0.006\n0.007\n-0.860\n0.480\n\n\n\n\n\n\n\n\nlm(y ~ b1 + b2 + b3 + b4 + b5, data = d) |&gt;\n  tidy() |&gt;\n  knitr::kable(digits = 3)"
  },
  {
    "objectID": "slides/10_non_linear_ch7.html#cubic-splines-1",
    "href": "slides/10_non_linear_ch7.html#cubic-splines-1",
    "title": "Moving Beyond Linearity",
    "section": "Cubic Splines",
    "text": "Cubic Splines\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\np_ &lt;- predict(lm(y ~ b1 + b2 + b3 + b4 + b5, data = d))\n\nggplot(d, aes(x = b1, y = p_)) +\n  geom_point() +\n  labs(x = \"X\",\n       y = expression(hat(y)))"
  },
  {
    "objectID": "slides/10_non_linear_ch7.html#cubic-splines-2",
    "href": "slides/10_non_linear_ch7.html#cubic-splines-2",
    "title": "Moving Beyond Linearity",
    "section": "Cubic Splines",
    "text": "Cubic Splines\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nnewdat &lt;- tibble(\n  b1 = 4:80,\n  b2 = b1^2,\n  b3 = b1^3,\n  b4 = ifelse(b1 &gt; 35, (b1 - 35)^3, 0),\n  b5 = ifelse(b1 &gt; 65, (b1 - 65)^3, 0)\n)\np &lt;- predict(lm(y ~ b1 + b2 + b3 + b4 + b5, data = d),\n             newdata = newdat)\n\nggplot(newdat, aes(x = b1, y = p)) +\n  geom_point() + \n  labs(x = \"X\",\n       y = expression(hat(y)))"
  },
  {
    "objectID": "slides/10_non_linear_ch7.html#cubic-splines-3",
    "href": "slides/10_non_linear_ch7.html#cubic-splines-3",
    "title": "Moving Beyond Linearity",
    "section": "Cubic Splines",
    "text": "Cubic Splines\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nnewdat &lt;- tibble(\n  b1 = -100:100,\n  b2 = b1^2,\n  b3 = b1^3,\n  b4 = ifelse(b1 &gt; 35, (b1 - 35)^3, 0),\n  b5 = ifelse(b1 &gt; 65, (b1 - 65)^3, 0)\n)\np &lt;- predict(lm(y ~ b1 + b2 + b3 + b4 + b5, data = d),\n             newdata = newdat)\n\nggplot(newdat, aes(x = b1, y = p)) +\n  geom_point() + \n  geom_vline(xintercept = c(4, 80), lty = 2) +\n  labs(x = \"X\",\n       y = expression(hat(y)))"
  },
  {
    "objectID": "slides/10_non_linear_ch7.html#natural-cubic-splines",
    "href": "slides/10_non_linear_ch7.html#natural-cubic-splines",
    "title": "Moving Beyond Linearity",
    "section": "Natural cubic splines",
    "text": "Natural cubic splines\nA natural cubic spline extrapolates linearly beyond the boundary knots\nThis adds 4 extra constraints and allows us to put more internal knots for the same degrees of freedom as a regular cubic spline"
  },
  {
    "objectID": "slides/10_non_linear_ch7.html#natural-cubic-splines-1",
    "href": "slides/10_non_linear_ch7.html#natural-cubic-splines-1",
    "title": "Moving Beyond Linearity",
    "section": "Natural Cubic Splines",
    "text": "Natural Cubic Splines\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\np &lt;- predict(lm(y ~ splines::ns(b1, knots = c(35, 65)), data = d),\n             newdata = newdat)\n\nggplot(newdat, aes(x = b1, y = p)) +\n  geom_point() + \n  geom_vline(xintercept = c(4, 80), lty = 2) +\n  labs(x = \"X\",\n       y = expression(hat(y)))"
  },
  {
    "objectID": "slides/10_non_linear_ch7.html#natural-cubic-splines-2",
    "href": "slides/10_non_linear_ch7.html#natural-cubic-splines-2",
    "title": "Moving Beyond Linearity",
    "section": "Natural Cubic Splines",
    "text": "Natural Cubic Splines\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nda &lt;- tibble(\n  x = newdat$b1,\n  ns = predict(lm(y ~ splines::ns(b1, knots = c(35, 65)), data = d),\n             newdata = newdat),\n  cubic = predict(lm(y ~ b1 + b2 + b3 + b4 + b5, data = d), \n                  newdata = newdat),\n  linear = predict(lm(y ~ b1 + ifelse(b1&gt;35, b1 - 35, 0) + ifelse(b1&gt;65, b1 - 65, 0), data = d),\n                   newdata = newdat)\n) |&gt;\n  pivot_longer(ns:linear)\n\nda |&gt;\n  filter(name != \"linear\") |&gt;\nggplot(aes(x = x, y = value, color = name)) +\n  geom_point(alpha = 0.5) + \n  geom_vline(xintercept = c(4, 80), lty = 2) +\n  labs(x = \"X\",\n       y = expression(hat(y)),\n       color = \"Spline\")"
  },
  {
    "objectID": "slides/10_non_linear_ch7.html#natural-splines",
    "href": "slides/10_non_linear_ch7.html#natural-splines",
    "title": "Moving Beyond Linearity",
    "section": "Natural Splines",
    "text": "Natural Splines\n\nggplot(da, aes(x = x, y = value, color = name)) +\n  geom_point(alpha = 0.5) + \n  geom_vline(xintercept = c(4, 80), lty = 2) +\n  labs(x = \"X\",\n       y = expression(hat(y)),\n       color = \"Spline\")"
  },
  {
    "objectID": "slides/10_non_linear_ch7.html#knot-placement",
    "href": "slides/10_non_linear_ch7.html#knot-placement",
    "title": "Moving Beyond Linearity",
    "section": "Knot placement",
    "text": "Knot placement\n\nOne strategy is to decide \\(K\\) (the number of knots) in advance and then place them at appropriate quantiles of the observed \\(X\\)\nA cubic spline with \\(K\\) knots has \\(K+3\\) parameters (or degrees of freedom!)\nA natural spline with \\(K\\) knots has \\(K-1\\) degrees of freedom"
  },
  {
    "objectID": "slides/10_non_linear_ch7.html#knot-placement-1",
    "href": "slides/10_non_linear_ch7.html#knot-placement-1",
    "title": "Moving Beyond Linearity",
    "section": "Knot placement",
    "text": "Knot placement\nHere is a comparison of a degree-14 polynomial and natural cubic spline (both have 15 degrees of freedom)"
  },
  {
    "objectID": "slides/10_non_linear_ch7.html#acknowledgements",
    "href": "slides/10_non_linear_ch7.html#acknowledgements",
    "title": "Moving Beyond Linearity",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThe content in the slides is from\n\nChapter 7 of Introduction to Statistical Learning, 2nd Ed by James, Witten, Hastie, and Tibshirani\nInitial versions of the slides are by Dr. Lucy D’Agostino McGowan"
  }
]